=> merge config from ../code/configs/swin_tiny_patch4_window7_224_lite.yaml
=> merge config from ../code/configs/swin_tiny_patch4_window7_224_lite.yaml
True
Namespace(root_path='../../dataset/Dermofit', exp='Dermofit/Cross_Teaching_Between_CNN_Transformer', model='unet', max_iterations=10000, batch_size=16, deterministic=1, base_lr=0.01, patch_size=[480, 480], seed=1234, num_classes=2, cfg='../code/configs/swin_tiny_patch4_window7_224_lite.yaml', opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False, labeled_bs=8, labeled_num=7, ema_decay=0.99, consistency_type='mse', consistency=0.1, consistency_rampup=200.0)
/ext3/miniconda3/envs/ssl/lib/python3.9/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1640811803361/work/aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/ext3/miniconda3/envs/ssl/lib/python3.9/site-packages/torch/utils/data/dataloader.py:478: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
AMP_OPT_LEVEL: O1
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: None
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE: ['']
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: 
  IMG_SIZE: 480
  INTERPOLATION: bicubic
  NUM_WORKERS: 8
  PIN_MEMORY: True
  ZIP_MODE: False
EVAL_MODE: False
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 1000
  PRETRAIN_CKPT: ../code/pretrained_ckpt/swin_tiny_patch4_window7_224.pth
  RESUME: 
  SWIN:
    APE: False
    DECODER_DEPTHS: [2, 2, 2, 1]
    DEPTHS: [2, 2, 2, 2]
    EMBED_DIM: 96
    FINAL_UPSAMPLE: expand_first
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 4
    QKV_BIAS: True
    QK_SCALE: None
    WINDOW_SIZE: 5
  TYPE: swin
OUTPUT: 
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: True
THROUGHPUT_MODE: False
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: True
  BASE_LR: 0.0005
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 5e-06
  OPTIMIZER:
    BETAS: (0.9, 0.999)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: False
  WARMUP_EPOCHS: 20
  WARMUP_LR: 5e-07
  WEIGHT_DECAY: 0.05
SwinTransformerSys expand initial----depths:[2, 2, 2, 2];depths_decoder:[1, 2, 2, 2];drop_path_rate:0.2;num_classes:2
---final upsample expand_first---
pretrained_path:../code/pretrained_ckpt/swin_tiny_patch4_window7_224.pth
---start load pretrained modle of swin encoder---
delete:layers.0.blocks.0.attn.relative_position_index;shape pretrain:torch.Size([768, 1536]);shape model:torch.Size([25, 25])
delete:layers.0.blocks.1.attn.relative_position_index;shape pretrain:torch.Size([768, 1536]);shape model:torch.Size([25, 25])
delete:layers.1.blocks.0.attn.relative_position_index;shape pretrain:torch.Size([768, 1536]);shape model:torch.Size([25, 25])
delete:layers.1.blocks.1.attn.relative_position_index;shape pretrain:torch.Size([768, 1536]);shape model:torch.Size([25, 25])
delete:layers.2.blocks.0.attn.relative_position_index;shape pretrain:torch.Size([768, 1536]);shape model:torch.Size([25, 25])
delete:layers.2.blocks.1.attn.relative_position_index;shape pretrain:torch.Size([768, 1536]);shape model:torch.Size([25, 25])
delete:layers.3.blocks.0.attn.relative_position_index;shape pretrain:torch.Size([768, 1536]);shape model:torch.Size([25, 25])
delete:layers.3.blocks.1.attn.relative_position_index;shape pretrain:torch.Size([768, 1536]);shape model:torch.Size([25, 25])
delete:layers.0.blocks.1.attn_mask;shape pretrain:torch.Size([768, 1536]);shape model:torch.Size([576, 25, 25])
delete:layers.1.blocks.1.attn_mask;shape pretrain:torch.Size([768, 1536]);shape model:torch.Size([144, 25, 25])
delete:layers.2.blocks.1.attn_mask;shape pretrain:torch.Size([768, 1536]);shape model:torch.Size([36, 25, 25])
delete:layers.0.blocks.0.attn.relative_position_bias_table;shape pretrain:torch.Size([768, 1536]);shape model:torch.Size([81, 3])
delete:layers.0.blocks.1.attn.relative_position_bias_table;shape pretrain:torch.Size([768, 1536]);shape model:torch.Size([81, 3])
delete:layers.1.blocks.0.attn.relative_position_bias_table;shape pretrain:torch.Size([768, 1536]);shape model:torch.Size([81, 6])
delete:layers.1.blocks.1.attn.relative_position_bias_table;shape pretrain:torch.Size([768, 1536]);shape model:torch.Size([81, 6])
delete:layers.2.blocks.0.attn.relative_position_bias_table;shape pretrain:torch.Size([768, 1536]);shape model:torch.Size([81, 12])
delete:layers.2.blocks.1.attn.relative_position_bias_table;shape pretrain:torch.Size([768, 1536]);shape model:torch.Size([81, 12])
delete:layers.3.blocks.0.attn.relative_position_bias_table;shape pretrain:torch.Size([768, 1536]);shape model:torch.Size([81, 24])
delete:layers.3.blocks.1.attn.relative_position_bias_table;shape pretrain:torch.Size([768, 1536]);shape model:torch.Size([81, 24])
delete:layers_up.3.blocks.0.attn.relative_position_index;shape pretrain:torch.Size([768, 1536]);shape model:torch.Size([25, 25])
delete:layers_up.3.blocks.1.attn.relative_position_index;shape pretrain:torch.Size([768, 1536]);shape model:torch.Size([25, 25])
delete:layers_up.2.blocks.0.attn.relative_position_index;shape pretrain:torch.Size([768, 1536]);shape model:torch.Size([25, 25])
delete:layers_up.2.blocks.1.attn.relative_position_index;shape pretrain:torch.Size([768, 1536]);shape model:torch.Size([25, 25])
delete:layers_up.1.blocks.0.attn.relative_position_index;shape pretrain:torch.Size([768, 1536]);shape model:torch.Size([25, 25])
delete:layers_up.1.blocks.1.attn.relative_position_index;shape pretrain:torch.Size([768, 1536]);shape model:torch.Size([25, 25])
delete:layers_up.3.blocks.1.attn_mask;shape pretrain:torch.Size([768, 1536]);shape model:torch.Size([576, 25, 25])
delete:layers_up.2.blocks.1.attn_mask;shape pretrain:torch.Size([768, 1536]);shape model:torch.Size([144, 25, 25])
delete:layers_up.1.blocks.1.attn_mask;shape pretrain:torch.Size([768, 1536]);shape model:torch.Size([36, 25, 25])
delete:layers_up.3.blocks.0.attn.relative_position_bias_table;shape pretrain:torch.Size([768, 1536]);shape model:torch.Size([81, 3])
delete:layers_up.3.blocks.1.attn.relative_position_bias_table;shape pretrain:torch.Size([768, 1536]);shape model:torch.Size([81, 3])
delete:layers_up.2.blocks.0.attn.relative_position_bias_table;shape pretrain:torch.Size([768, 1536]);shape model:torch.Size([81, 6])
delete:layers_up.2.blocks.1.attn.relative_position_bias_table;shape pretrain:torch.Size([768, 1536]);shape model:torch.Size([81, 6])
delete:layers_up.1.blocks.0.attn.relative_position_bias_table;shape pretrain:torch.Size([768, 1536]);shape model:torch.Size([81, 12])
delete:layers_up.1.blocks.1.attn.relative_position_bias_table;shape pretrain:torch.Size([768, 1536]);shape model:torch.Size([81, 12])
one_type = AK, total count of instances = 45
one_type = ALLSCC, total count of instances = 88
one_type = ALLMEL, total count of instances = 76
one_type = ALLBCC, total count of instances = 239
one_type = ALLSK, total count of instances = 257
one_type = ALLIEC, total count of instances = 78
one_type = ALLML, total count of instances = 331
one_type = ALLDF, total count of instances = 65
one_type = PYO, total count of instances = 24
one_type = ALLVASC, total count of instances = 97

train data: 1036
validation data: 130
test data: 134
Total silices is: 1036, labeled slices is: 136
17 iterations per epoch
  0%|                                         | 0/589 [00:00<?, ?it/s]iteration 1 : model1 loss : 0.580714 model2 loss : 0.551545
iteration 2 : model1 loss : 0.569094 model2 loss : 0.542229
iteration 3 : model1 loss : 0.574205 model2 loss : 0.524114
iteration 4 : model1 loss : 0.524610 model2 loss : 0.528072
iteration 5 : model1 loss : 0.529639 model2 loss : 0.532001
iteration 6 : model1 loss : 0.516390 model2 loss : 0.508483
iteration 7 : model1 loss : 0.499881 model2 loss : 0.512432
iteration 8 : model1 loss : 0.472972 model2 loss : 0.509709
iteration 9 : model1 loss : 0.481726 model2 loss : 0.512502
iteration 10 : model1 loss : 0.481181 model2 loss : 0.511144
iteration 11 : model1 loss : 0.471554 model2 loss : 0.505965
iteration 12 : model1 loss : 0.487916 model2 loss : 0.516703
iteration 13 : model1 loss : 0.519593 model2 loss : 0.531129
iteration 14 : model1 loss : 0.453266 model2 loss : 0.497417
iteration 15 : model1 loss : 0.465986 model2 loss : 0.513852
iteration 16 : model1 loss : 0.455083 model2 loss : 0.518287
iteration 17 : model1 loss : 0.485796 model2 loss : 0.511769
  0%|                               | 1/589 [00:28<4:40:25, 28.61s/it]iteration 18 : model1 loss : 0.509256 model2 loss : 0.515704
iteration 19 : model1 loss : 0.449324 model2 loss : 0.514470
iteration 20 : model1 loss : 0.381503 model2 loss : 0.509778
iteration 21 : model1 loss : 0.445469 model2 loss : 0.511061
iteration 22 : model1 loss : 0.401515 model2 loss : 0.512567
iteration 23 : model1 loss : 0.390494 model2 loss : 0.507843
iteration 24 : model1 loss : 0.419411 model2 loss : 0.507200
iteration 25 : model1 loss : 0.415650 model2 loss : 0.500702
iteration 26 : model1 loss : 0.340932 model2 loss : 0.500200
iteration 27 : model1 loss : 0.467395 model2 loss : 0.516468
iteration 28 : model1 loss : 0.393079 model2 loss : 0.514752
iteration 29 : model1 loss : 0.382387 model2 loss : 0.499780
iteration 30 : model1 loss : 0.447263 model2 loss : 0.499375
iteration 31 : model1 loss : 0.331603 model2 loss : 0.511340
iteration 32 : model1 loss : 0.449704 model2 loss : 0.500726
iteration 33 : model1 loss : 0.326384 model2 loss : 0.509421
iteration 34 : model1 loss : 0.344588 model2 loss : 0.508154
  0%|                               | 2/589 [00:51<4:05:52, 25.13s/it]iteration 35 : model1 loss : 0.345833 model2 loss : 0.504164
iteration 36 : model1 loss : 0.334155 model2 loss : 0.505627
iteration 37 : model1 loss : 0.276487 model2 loss : 0.511448
iteration 38 : model1 loss : 0.385406 model2 loss : 0.513027
iteration 39 : model1 loss : 0.372509 model2 loss : 0.503273
iteration 40 : model1 loss : 0.329690 model2 loss : 0.505672
iteration 41 : model1 loss : 0.262279 model2 loss : 0.502446
iteration 42 : model1 loss : 0.371809 model2 loss : 0.502551
iteration 43 : model1 loss : 0.339861 model2 loss : 0.495913
iteration 44 : model1 loss : 0.363042 model2 loss : 0.498292
iteration 45 : model1 loss : 0.337017 model2 loss : 0.500333
iteration 46 : model1 loss : 0.446365 model2 loss : 0.490518
iteration 47 : model1 loss : 0.318209 model2 loss : 0.497540
iteration 48 : model1 loss : 0.401434 model2 loss : 0.503359
iteration 49 : model1 loss : 0.301445 model2 loss : 0.488726
iteration 50 : model1 loss : 0.281261 model2 loss : 0.477991
iteration 51 : model1 loss : 0.283822 model2 loss : 0.472757
  1%|▏                              | 3/589 [01:13<3:53:57, 23.95s/it]iteration 52 : model1 loss : 0.343337 model2 loss : 0.481699
iteration 53 : model1 loss : 0.313213 model2 loss : 0.474062
iteration 54 : model1 loss : 0.281441 model2 loss : 0.440017
iteration 55 : model1 loss : 0.270069 model2 loss : 0.426021
iteration 56 : model1 loss : 0.279829 model2 loss : 0.476331
iteration 57 : model1 loss : 0.312284 model2 loss : 0.582599
iteration 58 : model1 loss : 0.371795 model2 loss : 0.522464
iteration 59 : model1 loss : 0.217188 model2 loss : 0.509656
iteration 60 : model1 loss : 0.385428 model2 loss : 0.485755
iteration 61 : model1 loss : 0.339074 model2 loss : 0.496143
iteration 62 : model1 loss : 0.313066 model2 loss : 0.511981
iteration 63 : model1 loss : 0.278764 model2 loss : 0.496603
iteration 64 : model1 loss : 0.271240 model2 loss : 0.502181
iteration 65 : model1 loss : 0.283777 model2 loss : 0.483266
iteration 66 : model1 loss : 0.259249 model2 loss : 0.513325
iteration 67 : model1 loss : 0.281379 model2 loss : 0.479479
iteration 68 : model1 loss : 0.279848 model2 loss : 0.475068
  1%|▏                              | 4/589 [01:36<3:48:04, 23.39s/it]iteration 69 : model1 loss : 0.275000 model2 loss : 0.478269
iteration 70 : model1 loss : 0.299457 model2 loss : 0.490327
iteration 71 : model1 loss : 0.266019 model2 loss : 0.468949
iteration 72 : model1 loss : 0.268867 model2 loss : 0.468106
iteration 73 : model1 loss : 0.315393 model2 loss : 0.491864
iteration 74 : model1 loss : 0.288182 model2 loss : 0.447209
iteration 75 : model1 loss : 0.266669 model2 loss : 0.461347
iteration 76 : model1 loss : 0.316682 model2 loss : 0.473830
iteration 77 : model1 loss : 0.257371 model2 loss : 0.471327
iteration 78 : model1 loss : 0.291629 model2 loss : 0.514527
iteration 79 : model1 loss : 0.264755 model2 loss : 0.420697
iteration 80 : model1 loss : 0.247420 model2 loss : 0.497696
iteration 81 : model1 loss : 0.242065 model2 loss : 0.420239
iteration 82 : model1 loss : 0.272666 model2 loss : 0.439077
iteration 83 : model1 loss : 0.262254 model2 loss : 0.455304
iteration 84 : model1 loss : 0.279628 model2 loss : 0.486443
iteration 85 : model1 loss : 0.258230 model2 loss : 0.446126
  1%|▎                              | 5/589 [01:58<3:44:31, 23.07s/it]iteration 86 : model1 loss : 0.249754 model2 loss : 0.379995
iteration 87 : model1 loss : 0.266267 model2 loss : 0.463924
iteration 88 : model1 loss : 0.229363 model2 loss : 0.382493
iteration 89 : model1 loss : 0.310149 model2 loss : 0.565809
iteration 90 : model1 loss : 0.327244 model2 loss : 0.586127
iteration 91 : model1 loss : 0.333369 model2 loss : 0.508590
iteration 92 : model1 loss : 0.247293 model2 loss : 0.461183
iteration 93 : model1 loss : 0.287650 model2 loss : 0.443079
iteration 94 : model1 loss : 0.252067 model2 loss : 0.435226
iteration 95 : model1 loss : 0.231377 model2 loss : 0.422995
iteration 96 : model1 loss : 0.266820 model2 loss : 0.439075
iteration 97 : model1 loss : 0.229627 model2 loss : 0.443294
iteration 98 : model1 loss : 0.309248 model2 loss : 0.512592
iteration 99 : model1 loss : 0.290685 model2 loss : 0.487074
iteration 100 : model1 loss : 0.262766 model2 loss : 0.436766
iteration 101 : model1 loss : 0.243403 model2 loss : 0.465813
iteration 102 : model1 loss : 0.219451 model2 loss : 0.461186
  1%|▎                              | 6/589 [02:21<3:42:54, 22.94s/it]iteration 103 : model1 loss : 0.309681 model2 loss : 0.499513
iteration 104 : model1 loss : 0.272074 model2 loss : 0.521077
iteration 105 : model1 loss : 0.250880 model2 loss : 0.503374
iteration 106 : model1 loss : 0.237080 model2 loss : 0.473133
iteration 107 : model1 loss : 0.299178 model2 loss : 0.443381
iteration 108 : model1 loss : 0.236491 model2 loss : 0.408025
iteration 109 : model1 loss : 0.232339 model2 loss : 0.465531
iteration 110 : model1 loss : 0.234342 model2 loss : 0.438674
iteration 111 : model1 loss : 0.282367 model2 loss : 0.428392
iteration 112 : model1 loss : 0.224124 model2 loss : 0.425220
iteration 113 : model1 loss : 0.281737 model2 loss : 0.474928
iteration 114 : model1 loss : 0.216780 model2 loss : 0.430989
iteration 115 : model1 loss : 0.267110 model2 loss : 0.430733
iteration 116 : model1 loss : 0.198122 model2 loss : 0.373067
iteration 117 : model1 loss : 0.210118 model2 loss : 0.414003
iteration 118 : model1 loss : 0.274548 model2 loss : 0.506637
iteration 119 : model1 loss : 0.267944 model2 loss : 0.453012
  1%|▎                              | 7/589 [02:44<3:42:00, 22.89s/it]iteration 120 : model1 loss : 0.212634 model2 loss : 0.439107
iteration 121 : model1 loss : 0.290192 model2 loss : 0.480734
iteration 122 : model1 loss : 0.260011 model2 loss : 0.429540
iteration 123 : model1 loss : 0.267493 model2 loss : 0.498623
iteration 124 : model1 loss : 0.197407 model2 loss : 0.471118
iteration 125 : model1 loss : 0.269393 model2 loss : 0.414457
iteration 126 : model1 loss : 0.243514 model2 loss : 0.410009
iteration 127 : model1 loss : 0.277438 model2 loss : 0.441792
iteration 128 : model1 loss : 0.200335 model2 loss : 0.404422
iteration 129 : model1 loss : 0.267717 model2 loss : 0.437596
iteration 130 : model1 loss : 0.205387 model2 loss : 0.396275
iteration 131 : model1 loss : 0.261250 model2 loss : 0.433799
iteration 132 : model1 loss : 0.204113 model2 loss : 0.443737
iteration 133 : model1 loss : 0.259069 model2 loss : 0.365208
iteration 134 : model1 loss : 0.303551 model2 loss : 0.402792
iteration 135 : model1 loss : 0.258899 model2 loss : 0.424759
iteration 136 : model1 loss : 0.253534 model2 loss : 0.524870
  1%|▍                              | 8/589 [03:06<3:40:42, 22.79s/it]iteration 137 : model1 loss : 0.236290 model2 loss : 0.368264
iteration 138 : model1 loss : 0.247907 model2 loss : 0.435678
iteration 139 : model1 loss : 0.253258 model2 loss : 0.493724
iteration 140 : model1 loss : 0.250965 model2 loss : 0.405396
iteration 141 : model1 loss : 0.234257 model2 loss : 0.432295
iteration 142 : model1 loss : 0.212683 model2 loss : 0.455462
iteration 143 : model1 loss : 0.222094 model2 loss : 0.391814
iteration 144 : model1 loss : 0.194491 model2 loss : 0.410066
iteration 145 : model1 loss : 0.224174 model2 loss : 0.423498
iteration 146 : model1 loss : 0.202396 model2 loss : 0.415141
iteration 147 : model1 loss : 0.275101 model2 loss : 0.497565
iteration 148 : model1 loss : 0.265607 model2 loss : 0.471409
iteration 149 : model1 loss : 0.230102 model2 loss : 0.479669
iteration 150 : model1 loss : 0.305586 model2 loss : 0.448207
iteration 151 : model1 loss : 0.259091 model2 loss : 0.465513
iteration 152 : model1 loss : 0.263131 model2 loss : 0.478313
iteration 153 : model1 loss : 0.242586 model2 loss : 0.412104
  2%|▍                              | 9/589 [03:29<3:40:06, 22.77s/it]iteration 154 : model1 loss : 0.249596 model2 loss : 0.414023
iteration 155 : model1 loss : 0.232677 model2 loss : 0.430432
iteration 156 : model1 loss : 0.272663 model2 loss : 0.488230
iteration 157 : model1 loss : 0.257689 model2 loss : 0.441974
iteration 158 : model1 loss : 0.219728 model2 loss : 0.463548
iteration 159 : model1 loss : 0.231454 model2 loss : 0.398667
iteration 160 : model1 loss : 0.198827 model2 loss : 0.439028
iteration 161 : model1 loss : 0.230108 model2 loss : 0.425737
iteration 162 : model1 loss : 0.240984 model2 loss : 0.413560
iteration 163 : model1 loss : 0.247566 model2 loss : 0.411998
iteration 164 : model1 loss : 0.215799 model2 loss : 0.457325
iteration 165 : model1 loss : 0.167297 model2 loss : 0.429755
iteration 166 : model1 loss : 0.211873 model2 loss : 0.389502
iteration 167 : model1 loss : 0.281238 model2 loss : 0.480638
iteration 168 : model1 loss : 0.239894 model2 loss : 0.455608
iteration 169 : model1 loss : 0.262530 model2 loss : 0.467077
iteration 170 : model1 loss : 0.229507 model2 loss : 0.435309
  2%|▌                             | 10/589 [03:52<3:39:17, 22.72s/it]iteration 171 : model1 loss : 0.213935 model2 loss : 0.384006
iteration 172 : model1 loss : 0.226379 model2 loss : 0.370734
iteration 173 : model1 loss : 0.246106 model2 loss : 0.436516
iteration 174 : model1 loss : 0.239925 model2 loss : 0.418752
iteration 175 : model1 loss : 0.237226 model2 loss : 0.428823
iteration 176 : model1 loss : 0.214108 model2 loss : 0.426655
iteration 177 : model1 loss : 0.229880 model2 loss : 0.405908
iteration 178 : model1 loss : 0.220832 model2 loss : 0.472788
iteration 179 : model1 loss : 0.223107 model2 loss : 0.397867
iteration 180 : model1 loss : 0.246656 model2 loss : 0.422045
iteration 181 : model1 loss : 0.239614 model2 loss : 0.383149
iteration 182 : model1 loss : 0.303041 model2 loss : 0.401466
iteration 183 : model1 loss : 0.232101 model2 loss : 0.393759
iteration 184 : model1 loss : 0.234578 model2 loss : 0.411680
iteration 185 : model1 loss : 0.276037 model2 loss : 0.499964
iteration 186 : model1 loss : 0.256345 model2 loss : 0.338695
iteration 187 : model1 loss : 0.212007 model2 loss : 0.389634
  2%|▌                             | 11/589 [04:14<3:38:36, 22.69s/it]iteration 188 : model1 loss : 0.255794 model2 loss : 0.359754
iteration 189 : model1 loss : 0.322616 model2 loss : 0.459753
iteration 190 : model1 loss : 0.231024 model2 loss : 0.353928
iteration 191 : model1 loss : 0.280743 model2 loss : 0.452674
iteration 192 : model1 loss : 0.234339 model2 loss : 0.375559
iteration 193 : model1 loss : 0.222614 model2 loss : 0.401118
iteration 194 : model1 loss : 0.214038 model2 loss : 0.407089
iteration 195 : model1 loss : 0.211431 model2 loss : 0.513742
iteration 196 : model1 loss : 0.251890 model2 loss : 0.453286
iteration 197 : model1 loss : 0.210900 model2 loss : 0.410138
iteration 198 : model1 loss : 0.199127 model2 loss : 0.399830
iteration 199 : model1 loss : 0.238420 model2 loss : 0.421567
iteration 200 : model1 loss : 0.168600 model2 loss : 0.395376
iteration 200 : model1_mean_dice : 0.679596 model1_mean_hd95 : 82.787855 model1_mean_iou : 0.553301
iteration 200 : model2_mean_dice : 0.643831 model2_mean_hd95 : 107.398529 model2_mean_iou : 0.516387
iteration 201 : model1 loss : 0.238746 model2 loss : 0.463630
iteration 202 : model1 loss : 0.221460 model2 loss : 0.384177
iteration 203 : model1 loss : 0.192338 model2 loss : 0.422196
iteration 204 : model1 loss : 0.247909 model2 loss : 0.352155
  2%|▌                             | 12/589 [04:59<4:43:06, 29.44s/it]iteration 205 : model1 loss : 0.281382 model2 loss : 0.502448
iteration 206 : model1 loss : 0.249582 model2 loss : 0.510489
iteration 207 : model1 loss : 0.193301 model2 loss : 0.410148
iteration 208 : model1 loss : 0.219538 model2 loss : 0.517019
iteration 209 : model1 loss : 0.205160 model2 loss : 0.511409
iteration 210 : model1 loss : 0.201860 model2 loss : 0.437174
iteration 211 : model1 loss : 0.176190 model2 loss : 0.354140
iteration 212 : model1 loss : 0.232638 model2 loss : 0.447797
iteration 213 : model1 loss : 0.214983 model2 loss : 0.431071
iteration 214 : model1 loss : 0.229891 model2 loss : 0.416075
iteration 215 : model1 loss : 0.208799 model2 loss : 0.382756
iteration 216 : model1 loss : 0.195210 model2 loss : 0.442365
iteration 217 : model1 loss : 0.286461 model2 loss : 0.506791
iteration 218 : model1 loss : 0.240302 model2 loss : 0.401489
iteration 219 : model1 loss : 0.234310 model2 loss : 0.433784
iteration 220 : model1 loss : 0.218307 model2 loss : 0.411960
iteration 221 : model1 loss : 0.288811 model2 loss : 0.441405
  2%|▋                             | 13/589 [05:22<4:23:00, 27.40s/it]iteration 222 : model1 loss : 0.184784 model2 loss : 0.383210
iteration 223 : model1 loss : 0.258707 model2 loss : 0.352486
iteration 224 : model1 loss : 0.202918 model2 loss : 0.388510
iteration 225 : model1 loss : 0.236417 model2 loss : 0.370575
iteration 226 : model1 loss : 0.198491 model2 loss : 0.422197
iteration 227 : model1 loss : 0.252692 model2 loss : 0.419667
iteration 228 : model1 loss : 0.217263 model2 loss : 0.383337
iteration 229 : model1 loss : 0.241363 model2 loss : 0.402088
iteration 230 : model1 loss : 0.230714 model2 loss : 0.424580
iteration 231 : model1 loss : 0.240969 model2 loss : 0.404088
iteration 232 : model1 loss : 0.211006 model2 loss : 0.402874
iteration 233 : model1 loss : 0.258215 model2 loss : 0.456729
iteration 234 : model1 loss : 0.215847 model2 loss : 0.422633
iteration 235 : model1 loss : 0.200223 model2 loss : 0.401072
iteration 236 : model1 loss : 0.241352 model2 loss : 0.435773
iteration 237 : model1 loss : 0.196450 model2 loss : 0.363668
iteration 238 : model1 loss : 0.226429 model2 loss : 0.359053
  2%|▋                             | 14/589 [05:45<4:08:33, 25.94s/it]iteration 239 : model1 loss : 0.249627 model2 loss : 0.440050
iteration 240 : model1 loss : 0.247839 model2 loss : 0.376565
iteration 241 : model1 loss : 0.246698 model2 loss : 0.360251
iteration 242 : model1 loss : 0.245144 model2 loss : 0.452004
iteration 243 : model1 loss : 0.222604 model2 loss : 0.391811
iteration 244 : model1 loss : 0.177376 model2 loss : 0.301943
iteration 245 : model1 loss : 0.295706 model2 loss : 0.395283
iteration 246 : model1 loss : 0.212501 model2 loss : 0.429798
iteration 247 : model1 loss : 0.187412 model2 loss : 0.260247
iteration 248 : model1 loss : 0.224761 model2 loss : 0.423029
iteration 249 : model1 loss : 0.255522 model2 loss : 0.401165
iteration 250 : model1 loss : 0.200227 model2 loss : 0.297163
iteration 251 : model1 loss : 0.224087 model2 loss : 0.408515
iteration 252 : model1 loss : 0.212340 model2 loss : 0.477376
iteration 253 : model1 loss : 0.278880 model2 loss : 0.464254
iteration 254 : model1 loss : 0.288420 model2 loss : 0.460865
iteration 255 : model1 loss : 0.184569 model2 loss : 0.450411
  3%|▊                             | 15/589 [06:07<3:58:48, 24.96s/it]iteration 256 : model1 loss : 0.214380 model2 loss : 0.572732
iteration 257 : model1 loss : 0.214830 model2 loss : 0.470643
iteration 258 : model1 loss : 0.195319 model2 loss : 0.397491
iteration 259 : model1 loss : 0.202313 model2 loss : 0.418549
iteration 260 : model1 loss : 0.170941 model2 loss : 0.452492
iteration 261 : model1 loss : 0.247441 model2 loss : 0.473643
iteration 262 : model1 loss : 0.206317 model2 loss : 0.403983
iteration 263 : model1 loss : 0.233388 model2 loss : 0.439993
iteration 264 : model1 loss : 0.224117 model2 loss : 0.441040
iteration 265 : model1 loss : 0.216258 model2 loss : 0.389752
iteration 266 : model1 loss : 0.254162 model2 loss : 0.414652
iteration 267 : model1 loss : 0.217669 model2 loss : 0.403844
iteration 268 : model1 loss : 0.247501 model2 loss : 0.360552
iteration 269 : model1 loss : 0.222618 model2 loss : 0.510600
iteration 270 : model1 loss : 0.197874 model2 loss : 0.365957
iteration 271 : model1 loss : 0.281417 model2 loss : 0.478042
iteration 272 : model1 loss : 0.239851 model2 loss : 0.425255
  3%|▊                             | 16/589 [06:30<3:51:59, 24.29s/it]iteration 273 : model1 loss : 0.199620 model2 loss : 0.378410
iteration 274 : model1 loss : 0.240349 model2 loss : 0.431109
iteration 275 : model1 loss : 0.196592 model2 loss : 0.444719
iteration 276 : model1 loss : 0.231061 model2 loss : 0.374007
iteration 277 : model1 loss : 0.236464 model2 loss : 0.332042
iteration 278 : model1 loss : 0.203392 model2 loss : 0.333263
iteration 279 : model1 loss : 0.217879 model2 loss : 0.353085
iteration 280 : model1 loss : 0.205454 model2 loss : 0.532755
iteration 281 : model1 loss : 0.206169 model2 loss : 0.334531
iteration 282 : model1 loss : 0.231990 model2 loss : 0.402107
iteration 283 : model1 loss : 0.202887 model2 loss : 0.368513
iteration 284 : model1 loss : 0.205691 model2 loss : 0.347301
iteration 285 : model1 loss : 0.208453 model2 loss : 0.455985
iteration 286 : model1 loss : 0.272264 model2 loss : 0.430094
iteration 287 : model1 loss : 0.210745 model2 loss : 0.424906
iteration 288 : model1 loss : 0.201887 model2 loss : 0.426418
iteration 289 : model1 loss : 0.225359 model2 loss : 0.472154
  3%|▊                             | 17/589 [06:53<3:47:01, 23.81s/it]iteration 290 : model1 loss : 0.196250 model2 loss : 0.383637
iteration 291 : model1 loss : 0.259422 model2 loss : 0.409822
iteration 292 : model1 loss : 0.172205 model2 loss : 0.365588
iteration 293 : model1 loss : 0.214536 model2 loss : 0.400529
iteration 294 : model1 loss : 0.210930 model2 loss : 0.475327
iteration 295 : model1 loss : 0.259996 model2 loss : 0.441775
iteration 296 : model1 loss : 0.216079 model2 loss : 0.339223
iteration 297 : model1 loss : 0.238561 model2 loss : 0.373908
iteration 298 : model1 loss : 0.258673 model2 loss : 0.483720
iteration 299 : model1 loss : 0.217663 model2 loss : 0.446362
iteration 300 : model1 loss : 0.219323 model2 loss : 0.388622
iteration 301 : model1 loss : 0.226590 model2 loss : 0.374113
iteration 302 : model1 loss : 0.230604 model2 loss : 0.357119
iteration 303 : model1 loss : 0.240798 model2 loss : 0.420251
iteration 304 : model1 loss : 0.177318 model2 loss : 0.488465
iteration 305 : model1 loss : 0.215009 model2 loss : 0.290997
iteration 306 : model1 loss : 0.241855 model2 loss : 0.380806
  3%|▉                             | 18/589 [07:15<3:43:22, 23.47s/it]iteration 307 : model1 loss : 0.221157 model2 loss : 0.387208
iteration 308 : model1 loss : 0.200780 model2 loss : 0.445217
iteration 309 : model1 loss : 0.218562 model2 loss : 0.451779
iteration 310 : model1 loss : 0.194191 model2 loss : 0.421258
iteration 311 : model1 loss : 0.217902 model2 loss : 0.386314
iteration 312 : model1 loss : 0.244858 model2 loss : 0.370647
iteration 313 : model1 loss : 0.200903 model2 loss : 0.335028
iteration 314 : model1 loss : 0.268190 model2 loss : 0.379037
iteration 315 : model1 loss : 0.237193 model2 loss : 0.392484
iteration 316 : model1 loss : 0.194108 model2 loss : 0.295397
iteration 317 : model1 loss : 0.232587 model2 loss : 0.358649
iteration 318 : model1 loss : 0.193138 model2 loss : 0.382395
iteration 319 : model1 loss : 0.225456 model2 loss : 0.371880
iteration 320 : model1 loss : 0.239913 model2 loss : 0.355775
iteration 321 : model1 loss : 0.186981 model2 loss : 0.319362
iteration 322 : model1 loss : 0.209485 model2 loss : 0.412746
iteration 323 : model1 loss : 0.217040 model2 loss : 0.492204
  3%|▉                             | 19/589 [07:38<3:40:55, 23.25s/it]iteration 324 : model1 loss : 0.164808 model2 loss : 0.347974
iteration 325 : model1 loss : 0.200453 model2 loss : 0.317344
iteration 326 : model1 loss : 0.237513 model2 loss : 0.371025
iteration 327 : model1 loss : 0.228938 model2 loss : 0.450096
iteration 328 : model1 loss : 0.231853 model2 loss : 0.287676
iteration 329 : model1 loss : 0.235837 model2 loss : 0.456008
iteration 330 : model1 loss : 0.242059 model2 loss : 0.415029
iteration 331 : model1 loss : 0.207848 model2 loss : 0.397003
iteration 332 : model1 loss : 0.196803 model2 loss : 0.349831
iteration 333 : model1 loss : 0.207407 model2 loss : 0.367632
iteration 334 : model1 loss : 0.254902 model2 loss : 0.371240
iteration 335 : model1 loss : 0.235619 model2 loss : 0.370064
iteration 336 : model1 loss : 0.227251 model2 loss : 0.369459
iteration 337 : model1 loss : 0.168088 model2 loss : 0.341953
iteration 338 : model1 loss : 0.228913 model2 loss : 0.303083
iteration 339 : model1 loss : 0.198534 model2 loss : 0.313494
iteration 340 : model1 loss : 0.246201 model2 loss : 0.374754
  3%|█                             | 20/589 [08:01<3:38:59, 23.09s/it]iteration 341 : model1 loss : 0.188524 model2 loss : 0.330776
iteration 342 : model1 loss : 0.162288 model2 loss : 0.272080
iteration 343 : model1 loss : 0.207181 model2 loss : 0.423495
iteration 344 : model1 loss : 0.213772 model2 loss : 0.303479
iteration 345 : model1 loss : 0.200814 model2 loss : 0.378398
iteration 346 : model1 loss : 0.236276 model2 loss : 0.439868
iteration 347 : model1 loss : 0.181718 model2 loss : 0.483737
iteration 348 : model1 loss : 0.267930 model2 loss : 0.484297
iteration 349 : model1 loss : 0.243085 model2 loss : 0.362068
iteration 350 : model1 loss : 0.166762 model2 loss : 0.362153
iteration 351 : model1 loss : 0.206092 model2 loss : 0.408157
iteration 352 : model1 loss : 0.202125 model2 loss : 0.357973
iteration 353 : model1 loss : 0.253137 model2 loss : 0.371597
iteration 354 : model1 loss : 0.218430 model2 loss : 0.301827
iteration 355 : model1 loss : 0.217628 model2 loss : 0.391167
iteration 356 : model1 loss : 0.227482 model2 loss : 0.362996
iteration 357 : model1 loss : 0.245322 model2 loss : 0.368539
  4%|█                             | 21/589 [08:24<3:37:44, 23.00s/it]iteration 358 : model1 loss : 0.210184 model2 loss : 0.305246
iteration 359 : model1 loss : 0.174342 model2 loss : 0.343451
iteration 360 : model1 loss : 0.223757 model2 loss : 0.395359
iteration 361 : model1 loss : 0.222662 model2 loss : 0.368196
iteration 362 : model1 loss : 0.240027 model2 loss : 0.358638
iteration 363 : model1 loss : 0.193079 model2 loss : 0.365379
iteration 364 : model1 loss : 0.199998 model2 loss : 0.349931
iteration 365 : model1 loss : 0.178155 model2 loss : 0.376134
iteration 366 : model1 loss : 0.206717 model2 loss : 0.383027
iteration 367 : model1 loss : 0.215101 model2 loss : 0.302737
iteration 368 : model1 loss : 0.272199 model2 loss : 0.299051
iteration 369 : model1 loss : 0.184664 model2 loss : 0.311010
iteration 370 : model1 loss : 0.239390 model2 loss : 0.301415
iteration 371 : model1 loss : 0.206641 model2 loss : 0.421343
iteration 372 : model1 loss : 0.208694 model2 loss : 0.387624
iteration 373 : model1 loss : 0.249303 model2 loss : 0.349349
iteration 374 : model1 loss : 0.252934 model2 loss : 0.385988
  4%|█                             | 22/589 [08:46<3:36:23, 22.90s/it]iteration 375 : model1 loss : 0.229689 model2 loss : 0.411647
iteration 376 : model1 loss : 0.223965 model2 loss : 0.359098
iteration 377 : model1 loss : 0.167858 model2 loss : 0.257165
iteration 378 : model1 loss : 0.188898 model2 loss : 0.306663
iteration 379 : model1 loss : 0.191378 model2 loss : 0.305641
iteration 380 : model1 loss : 0.232072 model2 loss : 0.338806
iteration 381 : model1 loss : 0.217616 model2 loss : 0.325987
iteration 382 : model1 loss : 0.205905 model2 loss : 0.367129
iteration 383 : model1 loss : 0.213623 model2 loss : 0.354186
iteration 384 : model1 loss : 0.207402 model2 loss : 0.320121
iteration 385 : model1 loss : 0.201791 model2 loss : 0.351781
iteration 386 : model1 loss : 0.262568 model2 loss : 0.370871
iteration 387 : model1 loss : 0.230060 model2 loss : 0.335998
iteration 388 : model1 loss : 0.272884 model2 loss : 0.407754
iteration 389 : model1 loss : 0.216033 model2 loss : 0.419445
iteration 390 : model1 loss : 0.190833 model2 loss : 0.422288
iteration 391 : model1 loss : 0.215205 model2 loss : 0.369204
  4%|█▏                            | 23/589 [09:09<3:35:21, 22.83s/it]iteration 392 : model1 loss : 0.189532 model2 loss : 0.276561
iteration 393 : model1 loss : 0.204817 model2 loss : 0.314048
iteration 394 : model1 loss : 0.209255 model2 loss : 0.279451
iteration 395 : model1 loss : 0.207013 model2 loss : 0.303942
iteration 396 : model1 loss : 0.167394 model2 loss : 0.323584
iteration 397 : model1 loss : 0.245047 model2 loss : 0.442875
iteration 398 : model1 loss : 0.194902 model2 loss : 0.294078
iteration 399 : model1 loss : 0.203771 model2 loss : 0.363005
iteration 400 : model1 loss : 0.206232 model2 loss : 0.345263
iteration 400 : model1_mean_dice : 0.637554 model1_mean_hd95 : 87.128819 model1_mean_iou : 0.511359
iteration 400 : model2_mean_dice : 0.651415 model2_mean_hd95 : 89.058511 model2_mean_iou : 0.531689
iteration 401 : model1 loss : 0.243763 model2 loss : 0.441485
iteration 402 : model1 loss : 0.188089 model2 loss : 0.439026
iteration 403 : model1 loss : 0.192285 model2 loss : 0.360659
iteration 404 : model1 loss : 0.231624 model2 loss : 0.434321
iteration 405 : model1 loss : 0.233730 model2 loss : 0.423702
iteration 406 : model1 loss : 0.255830 model2 loss : 0.347032
iteration 407 : model1 loss : 0.178037 model2 loss : 0.303817
iteration 408 : model1 loss : 0.242625 model2 loss : 0.324761
  4%|█▏                            | 24/589 [09:52<4:32:01, 28.89s/it]iteration 409 : model1 loss : 0.225815 model2 loss : 0.366902
iteration 410 : model1 loss : 0.192949 model2 loss : 0.411855
iteration 411 : model1 loss : 0.216398 model2 loss : 0.373467
iteration 412 : model1 loss : 0.181354 model2 loss : 0.345148
iteration 413 : model1 loss : 0.226987 model2 loss : 0.314360
iteration 414 : model1 loss : 0.178020 model2 loss : 0.273736
iteration 415 : model1 loss : 0.212223 model2 loss : 0.370235
iteration 416 : model1 loss : 0.179353 model2 loss : 0.381235
iteration 417 : model1 loss : 0.164419 model2 loss : 0.277461
iteration 418 : model1 loss : 0.169372 model2 loss : 0.244571
iteration 419 : model1 loss : 0.223131 model2 loss : 0.310980
iteration 420 : model1 loss : 0.237087 model2 loss : 0.452849
iteration 421 : model1 loss : 0.260963 model2 loss : 0.344667
iteration 422 : model1 loss : 0.198547 model2 loss : 0.282335
iteration 423 : model1 loss : 0.196492 model2 loss : 0.310729
iteration 424 : model1 loss : 0.206118 model2 loss : 0.307610
iteration 425 : model1 loss : 0.266182 model2 loss : 0.342094
  4%|█▎                            | 25/589 [10:15<4:13:58, 27.02s/it]iteration 426 : model1 loss : 0.191848 model2 loss : 0.349852
iteration 427 : model1 loss : 0.228765 model2 loss : 0.382864
iteration 428 : model1 loss : 0.208929 model2 loss : 0.398518
iteration 429 : model1 loss : 0.176744 model2 loss : 0.348434
iteration 430 : model1 loss : 0.195943 model2 loss : 0.256350
iteration 431 : model1 loss : 0.202067 model2 loss : 0.422116
iteration 432 : model1 loss : 0.187763 model2 loss : 0.308239
iteration 433 : model1 loss : 0.213045 model2 loss : 0.387076
iteration 434 : model1 loss : 0.226069 model2 loss : 0.323399
iteration 435 : model1 loss : 0.189781 model2 loss : 0.340258
iteration 436 : model1 loss : 0.200905 model2 loss : 0.389124
iteration 437 : model1 loss : 0.188475 model2 loss : 0.449104
iteration 438 : model1 loss : 0.218786 model2 loss : 0.374173
iteration 439 : model1 loss : 0.205699 model2 loss : 0.428806
iteration 440 : model1 loss : 0.179970 model2 loss : 0.322021
iteration 441 : model1 loss : 0.353715 model2 loss : 0.477144
iteration 442 : model1 loss : 0.208212 model2 loss : 0.292048
  4%|█▎                            | 26/589 [10:37<4:01:28, 25.73s/it]iteration 443 : model1 loss : 0.210557 model2 loss : 0.321860
iteration 444 : model1 loss : 0.203476 model2 loss : 0.341835
iteration 445 : model1 loss : 0.194842 model2 loss : 0.350184
iteration 446 : model1 loss : 0.197576 model2 loss : 0.345266
iteration 447 : model1 loss : 0.189596 model2 loss : 0.380614
iteration 448 : model1 loss : 0.228982 model2 loss : 0.354664
iteration 449 : model1 loss : 0.209843 model2 loss : 0.322883
iteration 450 : model1 loss : 0.217256 model2 loss : 0.343846
iteration 451 : model1 loss : 0.246040 model2 loss : 0.371479
iteration 452 : model1 loss : 0.201651 model2 loss : 0.348593
iteration 453 : model1 loss : 0.216289 model2 loss : 0.372316
iteration 454 : model1 loss : 0.226086 model2 loss : 0.307183
iteration 455 : model1 loss : 0.201932 model2 loss : 0.353099
iteration 456 : model1 loss : 0.181639 model2 loss : 0.291520
iteration 457 : model1 loss : 0.228827 model2 loss : 0.337426
iteration 458 : model1 loss : 0.256864 model2 loss : 0.334523
iteration 459 : model1 loss : 0.210466 model2 loss : 0.325771
  5%|█▍                            | 27/589 [11:00<3:52:42, 24.84s/it]iteration 460 : model1 loss : 0.224397 model2 loss : 0.324264
iteration 461 : model1 loss : 0.240491 model2 loss : 0.343959
iteration 462 : model1 loss : 0.201489 model2 loss : 0.390154
iteration 463 : model1 loss : 0.203279 model2 loss : 0.310531
iteration 464 : model1 loss : 0.178793 model2 loss : 0.330019
iteration 465 : model1 loss : 0.211330 model2 loss : 0.336846
iteration 466 : model1 loss : 0.154978 model2 loss : 0.308245
iteration 467 : model1 loss : 0.250250 model2 loss : 0.400514
iteration 468 : model1 loss : 0.219139 model2 loss : 0.304533
iteration 469 : model1 loss : 0.280372 model2 loss : 0.374197
iteration 470 : model1 loss : 0.214319 model2 loss : 0.378407
iteration 471 : model1 loss : 0.214264 model2 loss : 0.293702
iteration 472 : model1 loss : 0.234358 model2 loss : 0.292674
iteration 473 : model1 loss : 0.199271 model2 loss : 0.312087
iteration 474 : model1 loss : 0.230128 model2 loss : 0.478252
iteration 475 : model1 loss : 0.202882 model2 loss : 0.287383
iteration 476 : model1 loss : 0.173116 model2 loss : 0.285855
  5%|█▍                            | 28/589 [11:23<3:46:11, 24.19s/it]iteration 477 : model1 loss : 0.202658 model2 loss : 0.287779
iteration 478 : model1 loss : 0.220788 model2 loss : 0.265646
iteration 479 : model1 loss : 0.237697 model2 loss : 0.282299
iteration 480 : model1 loss : 0.181440 model2 loss : 0.256198
iteration 481 : model1 loss : 0.215363 model2 loss : 0.292634
iteration 482 : model1 loss : 0.167746 model2 loss : 0.352443
iteration 483 : model1 loss : 0.234345 model2 loss : 0.397472
iteration 484 : model1 loss : 0.188409 model2 loss : 0.280623
iteration 485 : model1 loss : 0.210607 model2 loss : 0.299271
iteration 486 : model1 loss : 0.186365 model2 loss : 0.468216
iteration 487 : model1 loss : 0.205975 model2 loss : 0.253231
iteration 488 : model1 loss : 0.212995 model2 loss : 0.322438
iteration 489 : model1 loss : 0.191104 model2 loss : 0.339151
iteration 490 : model1 loss : 0.210377 model2 loss : 0.242340
iteration 491 : model1 loss : 0.229966 model2 loss : 0.423431
iteration 492 : model1 loss : 0.233386 model2 loss : 0.438720
iteration 493 : model1 loss : 0.303948 model2 loss : 0.467427
  5%|█▍                            | 29/589 [11:45<3:41:36, 23.74s/it]iteration 494 : model1 loss : 0.189410 model2 loss : 0.425061
iteration 495 : model1 loss : 0.208541 model2 loss : 0.388643
iteration 496 : model1 loss : 0.263971 model2 loss : 0.404882
iteration 497 : model1 loss : 0.194164 model2 loss : 0.353471
iteration 498 : model1 loss : 0.205452 model2 loss : 0.319505
iteration 499 : model1 loss : 0.231047 model2 loss : 0.389219
iteration 500 : model1 loss : 0.186572 model2 loss : 0.341742
iteration 501 : model1 loss : 0.220645 model2 loss : 0.352303
iteration 502 : model1 loss : 0.258320 model2 loss : 0.311909
iteration 503 : model1 loss : 0.226692 model2 loss : 0.323766
iteration 504 : model1 loss : 0.171079 model2 loss : 0.333481
iteration 505 : model1 loss : 0.199329 model2 loss : 0.252180
iteration 506 : model1 loss : 0.216915 model2 loss : 0.358452
iteration 507 : model1 loss : 0.198400 model2 loss : 0.321618
iteration 508 : model1 loss : 0.206384 model2 loss : 0.416329
iteration 509 : model1 loss : 0.245135 model2 loss : 0.287355
iteration 510 : model1 loss : 0.233932 model2 loss : 0.340145
  5%|█▌                            | 30/589 [12:08<3:38:39, 23.47s/it]iteration 511 : model1 loss : 0.208331 model2 loss : 0.386435
iteration 512 : model1 loss : 0.221218 model2 loss : 0.434836
iteration 513 : model1 loss : 0.208038 model2 loss : 0.413313
iteration 514 : model1 loss : 0.252521 model2 loss : 0.426729
iteration 515 : model1 loss : 0.173680 model2 loss : 0.293666
iteration 516 : model1 loss : 0.198597 model2 loss : 0.293635
iteration 517 : model1 loss : 0.220562 model2 loss : 0.299943
iteration 518 : model1 loss : 0.226789 model2 loss : 0.403625
iteration 519 : model1 loss : 0.255871 model2 loss : 0.327381
iteration 520 : model1 loss : 0.199227 model2 loss : 0.329755
iteration 521 : model1 loss : 0.188993 model2 loss : 0.417196
iteration 522 : model1 loss : 0.184318 model2 loss : 0.363531
iteration 523 : model1 loss : 0.236952 model2 loss : 0.418754
iteration 524 : model1 loss : 0.193216 model2 loss : 0.407018
iteration 525 : model1 loss : 0.233623 model2 loss : 0.394468
iteration 526 : model1 loss : 0.207531 model2 loss : 0.354201
iteration 527 : model1 loss : 0.215758 model2 loss : 0.348396
  5%|█▌                            | 31/589 [12:31<3:36:11, 23.25s/it]iteration 528 : model1 loss : 0.210046 model2 loss : 0.320136
iteration 529 : model1 loss : 0.181778 model2 loss : 0.367373
iteration 530 : model1 loss : 0.205578 model2 loss : 0.326449
iteration 531 : model1 loss : 0.243874 model2 loss : 0.416466
iteration 532 : model1 loss : 0.250123 model2 loss : 0.346089
iteration 533 : model1 loss : 0.166243 model2 loss : 0.325563
iteration 534 : model1 loss : 0.194028 model2 loss : 0.308734
iteration 535 : model1 loss : 0.238150 model2 loss : 0.408473
iteration 536 : model1 loss : 0.236478 model2 loss : 0.396701
iteration 537 : model1 loss : 0.225409 model2 loss : 0.296459
iteration 538 : model1 loss : 0.217870 model2 loss : 0.384900
iteration 539 : model1 loss : 0.211665 model2 loss : 0.347030
iteration 540 : model1 loss : 0.192580 model2 loss : 0.304962
iteration 541 : model1 loss : 0.167431 model2 loss : 0.273118
iteration 542 : model1 loss : 0.218759 model2 loss : 0.261220
iteration 543 : model1 loss : 0.209180 model2 loss : 0.350466
iteration 544 : model1 loss : 0.242870 model2 loss : 0.274971
  5%|█▋                            | 32/589 [12:54<3:34:17, 23.08s/it]iteration 545 : model1 loss : 0.193770 model2 loss : 0.380888
iteration 546 : model1 loss : 0.168779 model2 loss : 0.230708
iteration 547 : model1 loss : 0.205095 model2 loss : 0.342388
iteration 548 : model1 loss : 0.187228 model2 loss : 0.317743
iteration 549 : model1 loss : 0.221694 model2 loss : 0.409922
iteration 550 : model1 loss : 0.208030 model2 loss : 0.383460
iteration 551 : model1 loss : 0.201006 model2 loss : 0.271130
iteration 552 : model1 loss : 0.183523 model2 loss : 0.311175
iteration 553 : model1 loss : 0.187428 model2 loss : 0.298379
iteration 554 : model1 loss : 0.177790 model2 loss : 0.304278
iteration 555 : model1 loss : 0.235160 model2 loss : 0.340173
iteration 556 : model1 loss : 0.205758 model2 loss : 0.337610
iteration 557 : model1 loss : 0.257912 model2 loss : 0.359001
iteration 558 : model1 loss : 0.199699 model2 loss : 0.319772
iteration 559 : model1 loss : 0.234373 model2 loss : 0.325563
iteration 560 : model1 loss : 0.225340 model2 loss : 0.271684
iteration 561 : model1 loss : 0.282340 model2 loss : 0.382340
  6%|█▋                            | 33/589 [13:17<3:33:15, 23.01s/it]iteration 562 : model1 loss : 0.227803 model2 loss : 0.295386
iteration 563 : model1 loss : 0.205213 model2 loss : 0.341507
iteration 564 : model1 loss : 0.182621 model2 loss : 0.323954
iteration 565 : model1 loss : 0.250112 model2 loss : 0.312511
iteration 566 : model1 loss : 0.222048 model2 loss : 0.342524
iteration 567 : model1 loss : 0.221830 model2 loss : 0.324128
iteration 568 : model1 loss : 0.239516 model2 loss : 0.360191
iteration 569 : model1 loss : 0.197128 model2 loss : 0.271256
iteration 570 : model1 loss : 0.258558 model2 loss : 0.411243
iteration 571 : model1 loss : 0.187767 model2 loss : 0.289215
iteration 572 : model1 loss : 0.202785 model2 loss : 0.330460
iteration 573 : model1 loss : 0.200651 model2 loss : 0.322402
iteration 574 : model1 loss : 0.178710 model2 loss : 0.393558
iteration 575 : model1 loss : 0.176741 model2 loss : 0.303306
iteration 576 : model1 loss : 0.210299 model2 loss : 0.343142
iteration 577 : model1 loss : 0.247323 model2 loss : 0.266016
iteration 578 : model1 loss : 0.239056 model2 loss : 0.302923
  6%|█▋                            | 34/589 [13:39<3:31:55, 22.91s/it]iteration 579 : model1 loss : 0.179398 model2 loss : 0.325843
iteration 580 : model1 loss : 0.195298 model2 loss : 0.272790
iteration 581 : model1 loss : 0.173156 model2 loss : 0.372603
iteration 582 : model1 loss : 0.214555 model2 loss : 0.315415
iteration 583 : model1 loss : 0.239053 model2 loss : 0.319947
iteration 584 : model1 loss : 0.195481 model2 loss : 0.258940
iteration 585 : model1 loss : 0.196045 model2 loss : 0.312488
iteration 586 : model1 loss : 0.207027 model2 loss : 0.261428
iteration 587 : model1 loss : 0.200216 model2 loss : 0.310356
iteration 588 : model1 loss : 0.240406 model2 loss : 0.323399
iteration 589 : model1 loss : 0.265363 model2 loss : 0.337129
iteration 590 : model1 loss : 0.217659 model2 loss : 0.264191
iteration 591 : model1 loss : 0.208621 model2 loss : 0.321900
iteration 592 : model1 loss : 0.171294 model2 loss : 0.326258
iteration 593 : model1 loss : 0.198107 model2 loss : 0.237284
iteration 594 : model1 loss : 0.243866 model2 loss : 0.403437
iteration 595 : model1 loss : 0.237931 model2 loss : 0.297659
  6%|█▊                            | 35/589 [14:02<3:30:58, 22.85s/it]iteration 596 : model1 loss : 0.243477 model2 loss : 0.290774
iteration 597 : model1 loss : 0.190765 model2 loss : 0.341385
iteration 598 : model1 loss : 0.212571 model2 loss : 0.350762
iteration 599 : model1 loss : 0.209262 model2 loss : 0.305167
iteration 600 : model1 loss : 0.211436 model2 loss : 0.280713
iteration 600 : model1_mean_dice : 0.676763 model1_mean_hd95 : 78.896328 model1_mean_iou : 0.558423
iteration 600 : model2_mean_dice : 0.698628 model2_mean_hd95 : 90.093677 model2_mean_iou : 0.578563
iteration 601 : model1 loss : 0.180486 model2 loss : 0.230089
iteration 602 : model1 loss : 0.230491 model2 loss : 0.322600
iteration 603 : model1 loss : 0.203137 model2 loss : 0.218910
iteration 604 : model1 loss : 0.183008 model2 loss : 0.280820
iteration 605 : model1 loss : 0.176126 model2 loss : 0.290373
iteration 606 : model1 loss : 0.224967 model2 loss : 0.297657
iteration 607 : model1 loss : 0.215524 model2 loss : 0.208657
iteration 608 : model1 loss : 0.184462 model2 loss : 0.342676
iteration 609 : model1 loss : 0.230186 model2 loss : 0.312888
iteration 610 : model1 loss : 0.277190 model2 loss : 0.434441
iteration 611 : model1 loss : 0.259019 model2 loss : 0.304738
iteration 612 : model1 loss : 0.207811 model2 loss : 0.323334
  6%|█▊                            | 36/589 [14:45<4:26:55, 28.96s/it]iteration 613 : model1 loss : 0.256380 model2 loss : 0.365917
iteration 614 : model1 loss : 0.190215 model2 loss : 0.326009
iteration 615 : model1 loss : 0.234203 model2 loss : 0.345917
iteration 616 : model1 loss : 0.219734 model2 loss : 0.291854
iteration 617 : model1 loss : 0.259180 model2 loss : 0.326355
iteration 618 : model1 loss : 0.193946 model2 loss : 0.298669
iteration 619 : model1 loss : 0.235302 model2 loss : 0.345880
iteration 620 : model1 loss : 0.197414 model2 loss : 0.199437
iteration 621 : model1 loss : 0.202981 model2 loss : 0.395716
iteration 622 : model1 loss : 0.217127 model2 loss : 0.263819
iteration 623 : model1 loss : 0.212413 model2 loss : 0.299112
iteration 624 : model1 loss : 0.220764 model2 loss : 0.293017
iteration 625 : model1 loss : 0.196217 model2 loss : 0.275810
iteration 626 : model1 loss : 0.216799 model2 loss : 0.290615
iteration 627 : model1 loss : 0.198481 model2 loss : 0.346609
iteration 628 : model1 loss : 0.178131 model2 loss : 0.283853
iteration 629 : model1 loss : 0.182252 model2 loss : 0.397056
  6%|█▉                            | 37/589 [15:08<4:08:58, 27.06s/it]iteration 630 : model1 loss : 0.213352 model2 loss : 0.336592
iteration 631 : model1 loss : 0.206064 model2 loss : 0.276451
iteration 632 : model1 loss : 0.244046 model2 loss : 0.349385
iteration 633 : model1 loss : 0.187582 model2 loss : 0.286322
iteration 634 : model1 loss : 0.185421 model2 loss : 0.250611
iteration 635 : model1 loss : 0.197030 model2 loss : 0.292202
iteration 636 : model1 loss : 0.183250 model2 loss : 0.306942
iteration 637 : model1 loss : 0.220153 model2 loss : 0.314907
iteration 638 : model1 loss : 0.233768 model2 loss : 0.363791
iteration 639 : model1 loss : 0.161765 model2 loss : 0.220880
iteration 640 : model1 loss : 0.233976 model2 loss : 0.313068
iteration 641 : model1 loss : 0.241398 model2 loss : 0.382472
iteration 642 : model1 loss : 0.241163 model2 loss : 0.405152
iteration 643 : model1 loss : 0.212039 model2 loss : 0.246641
iteration 644 : model1 loss : 0.181327 model2 loss : 0.298682
iteration 645 : model1 loss : 0.199590 model2 loss : 0.296235
iteration 646 : model1 loss : 0.220257 model2 loss : 0.273875
  6%|█▉                            | 38/589 [15:30<3:56:21, 25.74s/it]iteration 647 : model1 loss : 0.201739 model2 loss : 0.325328
iteration 648 : model1 loss : 0.205915 model2 loss : 0.283964
iteration 649 : model1 loss : 0.184751 model2 loss : 0.332750
iteration 650 : model1 loss : 0.214445 model2 loss : 0.332983
iteration 651 : model1 loss : 0.211944 model2 loss : 0.309470
iteration 652 : model1 loss : 0.187939 model2 loss : 0.333232
iteration 653 : model1 loss : 0.266480 model2 loss : 0.364470
iteration 654 : model1 loss : 0.203217 model2 loss : 0.281996
iteration 655 : model1 loss : 0.237250 model2 loss : 0.348622
iteration 656 : model1 loss : 0.237453 model2 loss : 0.324877
iteration 657 : model1 loss : 0.197980 model2 loss : 0.283959
iteration 658 : model1 loss : 0.190555 model2 loss : 0.283490
iteration 659 : model1 loss : 0.231786 model2 loss : 0.305521
iteration 660 : model1 loss : 0.228313 model2 loss : 0.438605
iteration 661 : model1 loss : 0.174331 model2 loss : 0.306882
iteration 662 : model1 loss : 0.186644 model2 loss : 0.261406
iteration 663 : model1 loss : 0.200715 model2 loss : 0.233706
  7%|█▉                            | 39/589 [15:53<3:47:48, 24.85s/it]iteration 664 : model1 loss : 0.164704 model2 loss : 0.211003
iteration 665 : model1 loss : 0.207025 model2 loss : 0.509236
iteration 666 : model1 loss : 0.176184 model2 loss : 0.364846
iteration 667 : model1 loss : 0.223245 model2 loss : 0.421428
iteration 668 : model1 loss : 0.230598 model2 loss : 0.467137
iteration 669 : model1 loss : 0.205124 model2 loss : 0.372162
iteration 670 : model1 loss : 0.189214 model2 loss : 0.379957
iteration 671 : model1 loss : 0.256609 model2 loss : 0.343071
iteration 672 : model1 loss : 0.174168 model2 loss : 0.306550
iteration 673 : model1 loss : 0.185941 model2 loss : 0.385010
iteration 674 : model1 loss : 0.283137 model2 loss : 0.556810
iteration 675 : model1 loss : 0.217785 model2 loss : 0.389269
iteration 676 : model1 loss : 0.202075 model2 loss : 0.340685
iteration 677 : model1 loss : 0.192569 model2 loss : 0.415019
iteration 678 : model1 loss : 0.210042 model2 loss : 0.383475
iteration 679 : model1 loss : 0.185331 model2 loss : 0.345424
iteration 680 : model1 loss : 0.224106 model2 loss : 0.394742
  7%|██                            | 40/589 [16:16<3:41:37, 24.22s/it]iteration 681 : model1 loss : 0.195565 model2 loss : 0.354143
iteration 682 : model1 loss : 0.232254 model2 loss : 0.387091
iteration 683 : model1 loss : 0.169603 model2 loss : 0.347534
iteration 684 : model1 loss : 0.237726 model2 loss : 0.402354
iteration 685 : model1 loss : 0.199450 model2 loss : 0.466176
iteration 686 : model1 loss : 0.205597 model2 loss : 0.296470
iteration 687 : model1 loss : 0.256270 model2 loss : 0.425942
iteration 688 : model1 loss : 0.240343 model2 loss : 0.311861
iteration 689 : model1 loss : 0.266097 model2 loss : 0.390779
iteration 690 : model1 loss : 0.199923 model2 loss : 0.299240
iteration 691 : model1 loss : 0.192244 model2 loss : 0.331063
iteration 692 : model1 loss : 0.229053 model2 loss : 0.329779
iteration 693 : model1 loss : 0.182430 model2 loss : 0.245204
iteration 694 : model1 loss : 0.210642 model2 loss : 0.375751
iteration 695 : model1 loss : 0.164339 model2 loss : 0.327801
iteration 696 : model1 loss : 0.188256 model2 loss : 0.308488
iteration 697 : model1 loss : 0.199759 model2 loss : 0.327097
  7%|██                            | 41/589 [16:39<3:37:10, 23.78s/it]iteration 698 : model1 loss : 0.203339 model2 loss : 0.382146
iteration 699 : model1 loss : 0.175612 model2 loss : 0.259771
iteration 700 : model1 loss : 0.228490 model2 loss : 0.491532
iteration 701 : model1 loss : 0.174276 model2 loss : 0.214890
iteration 702 : model1 loss : 0.197622 model2 loss : 0.278385
iteration 703 : model1 loss : 0.247130 model2 loss : 0.374847
iteration 704 : model1 loss : 0.324939 model2 loss : 0.388693
iteration 705 : model1 loss : 0.195976 model2 loss : 0.350785
iteration 706 : model1 loss : 0.186792 model2 loss : 0.335624
iteration 707 : model1 loss : 0.177501 model2 loss : 0.332485
iteration 708 : model1 loss : 0.157394 model2 loss : 0.282776
iteration 709 : model1 loss : 0.261566 model2 loss : 0.340319
iteration 710 : model1 loss : 0.221453 model2 loss : 0.403191
iteration 711 : model1 loss : 0.219579 model2 loss : 0.307502
iteration 712 : model1 loss : 0.184264 model2 loss : 0.248817
iteration 713 : model1 loss : 0.200641 model2 loss : 0.236243
iteration 714 : model1 loss : 0.204573 model2 loss : 0.319001
  7%|██▏                           | 42/589 [17:02<3:34:03, 23.48s/it]iteration 715 : model1 loss : 0.190964 model2 loss : 0.245024
iteration 716 : model1 loss : 0.231572 model2 loss : 0.394650
iteration 717 : model1 loss : 0.201659 model2 loss : 0.329183
iteration 718 : model1 loss : 0.239530 model2 loss : 0.345992
iteration 719 : model1 loss : 0.169946 model2 loss : 0.288202
iteration 720 : model1 loss : 0.236775 model2 loss : 0.365006
iteration 721 : model1 loss : 0.231292 model2 loss : 0.325099
iteration 722 : model1 loss : 0.217041 model2 loss : 0.359345
iteration 723 : model1 loss : 0.195927 model2 loss : 0.308746
iteration 724 : model1 loss : 0.185810 model2 loss : 0.274791
iteration 725 : model1 loss : 0.272576 model2 loss : 0.356730
iteration 726 : model1 loss : 0.162966 model2 loss : 0.281743
iteration 727 : model1 loss : 0.219247 model2 loss : 0.312422
iteration 728 : model1 loss : 0.236441 model2 loss : 0.371259
iteration 729 : model1 loss : 0.200464 model2 loss : 0.353588
iteration 730 : model1 loss : 0.196270 model2 loss : 0.236422
iteration 731 : model1 loss : 0.212494 model2 loss : 0.347689
  7%|██▏                           | 43/589 [17:24<3:31:21, 23.23s/it]iteration 732 : model1 loss : 0.216768 model2 loss : 0.323939
iteration 733 : model1 loss : 0.277665 model2 loss : 0.403687
iteration 734 : model1 loss : 0.244517 model2 loss : 0.410831
iteration 735 : model1 loss : 0.206403 model2 loss : 0.369289
iteration 736 : model1 loss : 0.189644 model2 loss : 0.278271
iteration 737 : model1 loss : 0.181826 model2 loss : 0.233696
iteration 738 : model1 loss : 0.217411 model2 loss : 0.250680
iteration 739 : model1 loss : 0.195584 model2 loss : 0.364787
iteration 740 : model1 loss : 0.216242 model2 loss : 0.300362
iteration 741 : model1 loss : 0.160428 model2 loss : 0.313447
iteration 742 : model1 loss : 0.208243 model2 loss : 0.304155
iteration 743 : model1 loss : 0.171673 model2 loss : 0.215256
iteration 744 : model1 loss : 0.206505 model2 loss : 0.284789
iteration 745 : model1 loss : 0.176941 model2 loss : 0.281108
iteration 746 : model1 loss : 0.254278 model2 loss : 0.437928
iteration 747 : model1 loss : 0.168172 model2 loss : 0.239295
iteration 748 : model1 loss : 0.210101 model2 loss : 0.417267
  7%|██▏                           | 44/589 [17:47<3:29:30, 23.07s/it]iteration 749 : model1 loss : 0.228885 model2 loss : 0.381288
iteration 750 : model1 loss : 0.209193 model2 loss : 0.345897
iteration 751 : model1 loss : 0.199631 model2 loss : 0.224075
iteration 752 : model1 loss : 0.191116 model2 loss : 0.350805
iteration 753 : model1 loss : 0.193208 model2 loss : 0.281591
iteration 754 : model1 loss : 0.210787 model2 loss : 0.260416
iteration 755 : model1 loss : 0.182141 model2 loss : 0.352199
iteration 756 : model1 loss : 0.210760 model2 loss : 0.302024
iteration 757 : model1 loss : 0.180848 model2 loss : 0.276568
iteration 758 : model1 loss : 0.184027 model2 loss : 0.287177
iteration 759 : model1 loss : 0.231727 model2 loss : 0.344432
iteration 760 : model1 loss : 0.204784 model2 loss : 0.287853
iteration 761 : model1 loss : 0.205761 model2 loss : 0.309133
iteration 762 : model1 loss : 0.186610 model2 loss : 0.338071
iteration 763 : model1 loss : 0.173210 model2 loss : 0.270309
iteration 764 : model1 loss : 0.192288 model2 loss : 0.272730
iteration 765 : model1 loss : 0.225257 model2 loss : 0.338389
  8%|██▎                           | 45/589 [18:10<3:28:37, 23.01s/it]iteration 766 : model1 loss : 0.193809 model2 loss : 0.250028
iteration 767 : model1 loss : 0.188725 model2 loss : 0.263708
iteration 768 : model1 loss : 0.212519 model2 loss : 0.329781
iteration 769 : model1 loss : 0.181190 model2 loss : 0.340258
iteration 770 : model1 loss : 0.185501 model2 loss : 0.318448
iteration 771 : model1 loss : 0.190553 model2 loss : 0.283482
iteration 772 : model1 loss : 0.200528 model2 loss : 0.363414
iteration 773 : model1 loss : 0.224355 model2 loss : 0.342487
iteration 774 : model1 loss : 0.187587 model2 loss : 0.231275
iteration 775 : model1 loss : 0.203016 model2 loss : 0.390065
iteration 776 : model1 loss : 0.210666 model2 loss : 0.246425
iteration 777 : model1 loss : 0.176556 model2 loss : 0.411535
iteration 778 : model1 loss : 0.178580 model2 loss : 0.287092
iteration 779 : model1 loss : 0.226639 model2 loss : 0.391904
iteration 780 : model1 loss : 0.233797 model2 loss : 0.409481
iteration 781 : model1 loss : 0.195027 model2 loss : 0.318921
iteration 782 : model1 loss : 0.241431 model2 loss : 0.313528
  8%|██▎                           | 46/589 [18:32<3:27:31, 22.93s/it]iteration 783 : model1 loss : 0.209080 model2 loss : 0.280685
iteration 784 : model1 loss : 0.236418 model2 loss : 0.429371
iteration 785 : model1 loss : 0.184776 model2 loss : 0.298570
iteration 786 : model1 loss : 0.179976 model2 loss : 0.287170
iteration 787 : model1 loss : 0.170752 model2 loss : 0.258286
iteration 788 : model1 loss : 0.213360 model2 loss : 0.275247
iteration 789 : model1 loss : 0.186427 model2 loss : 0.283318
iteration 790 : model1 loss : 0.217351 model2 loss : 0.304903
iteration 791 : model1 loss : 0.202152 model2 loss : 0.289817
iteration 792 : model1 loss : 0.183341 model2 loss : 0.300820
iteration 793 : model1 loss : 0.200863 model2 loss : 0.258993
iteration 794 : model1 loss : 0.193165 model2 loss : 0.311338
iteration 795 : model1 loss : 0.178720 model2 loss : 0.239890
iteration 796 : model1 loss : 0.202112 model2 loss : 0.306763
iteration 797 : model1 loss : 0.222114 model2 loss : 0.390976
iteration 798 : model1 loss : 0.238457 model2 loss : 0.442420
iteration 799 : model1 loss : 0.202484 model2 loss : 0.316307
  8%|██▍                           | 47/589 [18:55<3:26:28, 22.86s/it]iteration 800 : model1 loss : 0.242257 model2 loss : 0.277450
iteration 800 : model1_mean_dice : 0.473600 model1_mean_hd95 : 107.766574 model1_mean_iou : 0.362643
iteration 800 : model2_mean_dice : 0.700979 model2_mean_hd95 : 85.593342 model2_mean_iou : 0.583749
iteration 801 : model1 loss : 0.214571 model2 loss : 0.318971
iteration 802 : model1 loss : 0.209808 model2 loss : 0.346460
iteration 803 : model1 loss : 0.177964 model2 loss : 0.424176
iteration 804 : model1 loss : 0.203575 model2 loss : 0.284598
iteration 805 : model1 loss : 0.217827 model2 loss : 0.256923
iteration 806 : model1 loss : 0.217571 model2 loss : 0.358689
iteration 807 : model1 loss : 0.214660 model2 loss : 0.411430
iteration 808 : model1 loss : 0.185190 model2 loss : 0.270202
iteration 809 : model1 loss : 0.197926 model2 loss : 0.307621
iteration 810 : model1 loss : 0.175344 model2 loss : 0.268815
iteration 811 : model1 loss : 0.198291 model2 loss : 0.266985
iteration 812 : model1 loss : 0.256449 model2 loss : 0.283048
iteration 813 : model1 loss : 0.201523 model2 loss : 0.262039
iteration 814 : model1 loss : 0.225462 model2 loss : 0.327836
iteration 815 : model1 loss : 0.221291 model2 loss : 0.285782
iteration 816 : model1 loss : 0.192309 model2 loss : 0.287187
  8%|██▍                           | 48/589 [19:38<4:20:49, 28.93s/it]iteration 817 : model1 loss : 0.227378 model2 loss : 0.333282
iteration 818 : model1 loss : 0.200430 model2 loss : 0.284211
iteration 819 : model1 loss : 0.209318 model2 loss : 0.250919
iteration 820 : model1 loss : 0.220807 model2 loss : 0.397037
iteration 821 : model1 loss : 0.275103 model2 loss : 0.326486
iteration 822 : model1 loss : 0.153823 model2 loss : 0.182556
iteration 823 : model1 loss : 0.263123 model2 loss : 0.285957
iteration 824 : model1 loss : 0.208175 model2 loss : 0.265605
iteration 825 : model1 loss : 0.184405 model2 loss : 0.211266
iteration 826 : model1 loss : 0.192302 model2 loss : 0.256721
iteration 827 : model1 loss : 0.178442 model2 loss : 0.365147
iteration 828 : model1 loss : 0.207274 model2 loss : 0.287544
iteration 829 : model1 loss : 0.199818 model2 loss : 0.245931
iteration 830 : model1 loss : 0.188649 model2 loss : 0.250302
iteration 831 : model1 loss : 0.194042 model2 loss : 0.246421
iteration 832 : model1 loss : 0.221304 model2 loss : 0.355139
iteration 833 : model1 loss : 0.212942 model2 loss : 0.351145
  8%|██▍                           | 49/589 [20:01<4:03:25, 27.05s/it]iteration 834 : model1 loss : 0.200944 model2 loss : 0.307204
iteration 835 : model1 loss : 0.207768 model2 loss : 0.259066
iteration 836 : model1 loss : 0.205175 model2 loss : 0.359284
iteration 837 : model1 loss : 0.165981 model2 loss : 0.204484
iteration 838 : model1 loss : 0.211986 model2 loss : 0.248386
iteration 839 : model1 loss : 0.194021 model2 loss : 0.243882
iteration 840 : model1 loss : 0.201712 model2 loss : 0.223137
iteration 841 : model1 loss : 0.216886 model2 loss : 0.255205
iteration 842 : model1 loss : 0.181235 model2 loss : 0.287705
iteration 843 : model1 loss : 0.207464 model2 loss : 0.310876
iteration 844 : model1 loss : 0.233249 model2 loss : 0.243836
iteration 845 : model1 loss : 0.179942 model2 loss : 0.316795
iteration 846 : model1 loss : 0.266926 model2 loss : 0.421214
iteration 847 : model1 loss : 0.267930 model2 loss : 0.529330
iteration 848 : model1 loss : 0.173617 model2 loss : 0.274232
iteration 849 : model1 loss : 0.206602 model2 loss : 0.331473
iteration 850 : model1 loss : 0.189992 model2 loss : 0.280071
  8%|██▌                           | 50/589 [20:24<3:51:18, 25.75s/it]iteration 851 : model1 loss : 0.191402 model2 loss : 0.273129
iteration 852 : model1 loss : 0.196200 model2 loss : 0.305567
iteration 853 : model1 loss : 0.194185 model2 loss : 0.301305
iteration 854 : model1 loss : 0.203490 model2 loss : 0.313219
iteration 855 : model1 loss : 0.187880 model2 loss : 0.278334
iteration 856 : model1 loss : 0.216935 model2 loss : 0.331891
iteration 857 : model1 loss : 0.247956 model2 loss : 0.471487
iteration 858 : model1 loss : 0.227987 model2 loss : 0.341705
iteration 859 : model1 loss : 0.225478 model2 loss : 0.336358
iteration 860 : model1 loss : 0.185913 model2 loss : 0.360315
iteration 861 : model1 loss : 0.192395 model2 loss : 0.289538
iteration 862 : model1 loss : 0.218383 model2 loss : 0.348871
iteration 863 : model1 loss : 0.174608 model2 loss : 0.281862
iteration 864 : model1 loss : 0.215864 model2 loss : 0.381459
iteration 865 : model1 loss : 0.227789 model2 loss : 0.289512
iteration 866 : model1 loss : 0.186641 model2 loss : 0.284601
iteration 867 : model1 loss : 0.201139 model2 loss : 0.286911
  9%|██▌                           | 51/589 [20:46<3:42:37, 24.83s/it]iteration 868 : model1 loss : 0.210770 model2 loss : 0.401246
iteration 869 : model1 loss : 0.177347 model2 loss : 0.328362
iteration 870 : model1 loss : 0.245793 model2 loss : 0.420528
iteration 871 : model1 loss : 0.196538 model2 loss : 0.333302
iteration 872 : model1 loss : 0.208429 model2 loss : 0.318227
iteration 873 : model1 loss : 0.165651 model2 loss : 0.304226
iteration 874 : model1 loss : 0.198957 model2 loss : 0.310411
iteration 875 : model1 loss : 0.204900 model2 loss : 0.242636
iteration 876 : model1 loss : 0.196781 model2 loss : 0.289162
iteration 877 : model1 loss : 0.218495 model2 loss : 0.270294
iteration 878 : model1 loss : 0.225709 model2 loss : 0.294651
iteration 879 : model1 loss : 0.209945 model2 loss : 0.250700
iteration 880 : model1 loss : 0.182704 model2 loss : 0.265230
iteration 881 : model1 loss : 0.233846 model2 loss : 0.318487
iteration 882 : model1 loss : 0.174470 model2 loss : 0.293183
iteration 883 : model1 loss : 0.178380 model2 loss : 0.214848
iteration 884 : model1 loss : 0.208734 model2 loss : 0.262599
  9%|██▋                           | 52/589 [21:09<3:36:31, 24.19s/it]iteration 885 : model1 loss : 0.230091 model2 loss : 0.304355
iteration 886 : model1 loss : 0.205111 model2 loss : 0.289219
iteration 887 : model1 loss : 0.204247 model2 loss : 0.395998
iteration 888 : model1 loss : 0.177437 model2 loss : 0.312557
iteration 889 : model1 loss : 0.252110 model2 loss : 0.269486
iteration 890 : model1 loss : 0.168868 model2 loss : 0.235319
iteration 891 : model1 loss : 0.172529 model2 loss : 0.339059
iteration 892 : model1 loss : 0.181073 model2 loss : 0.242941
iteration 893 : model1 loss : 0.193023 model2 loss : 0.207332
iteration 894 : model1 loss : 0.234882 model2 loss : 0.292809
iteration 895 : model1 loss : 0.222195 model2 loss : 0.232695
iteration 896 : model1 loss : 0.218112 model2 loss : 0.312669
iteration 897 : model1 loss : 0.216004 model2 loss : 0.352991
iteration 898 : model1 loss : 0.194732 model2 loss : 0.196354
iteration 899 : model1 loss : 0.195325 model2 loss : 0.411465
iteration 900 : model1 loss : 0.195726 model2 loss : 0.239837
iteration 901 : model1 loss : 0.184552 model2 loss : 0.318274
  9%|██▋                           | 53/589 [21:32<3:32:18, 23.77s/it]iteration 902 : model1 loss : 0.180898 model2 loss : 0.201728
iteration 903 : model1 loss : 0.201483 model2 loss : 0.322130
iteration 904 : model1 loss : 0.240184 model2 loss : 0.328949
iteration 905 : model1 loss : 0.236751 model2 loss : 0.335698
iteration 906 : model1 loss : 0.231194 model2 loss : 0.342039
iteration 907 : model1 loss : 0.159384 model2 loss : 0.215179
iteration 908 : model1 loss : 0.196104 model2 loss : 0.296499
iteration 909 : model1 loss : 0.170905 model2 loss : 0.304422
iteration 910 : model1 loss : 0.204103 model2 loss : 0.229709
iteration 911 : model1 loss : 0.160426 model2 loss : 0.274947
iteration 912 : model1 loss : 0.203910 model2 loss : 0.348040
iteration 913 : model1 loss : 0.201387 model2 loss : 0.303029
iteration 914 : model1 loss : 0.208221 model2 loss : 0.376740
iteration 915 : model1 loss : 0.200633 model2 loss : 0.354404
iteration 916 : model1 loss : 0.180066 model2 loss : 0.329654
iteration 917 : model1 loss : 0.216131 model2 loss : 0.292752
iteration 918 : model1 loss : 0.225820 model2 loss : 0.260689
  9%|██▊                           | 54/589 [21:54<3:29:02, 23.44s/it]iteration 919 : model1 loss : 0.210811 model2 loss : 0.328322
iteration 920 : model1 loss : 0.188777 model2 loss : 0.367061
iteration 921 : model1 loss : 0.214071 model2 loss : 0.280642
iteration 922 : model1 loss : 0.205131 model2 loss : 0.286958
iteration 923 : model1 loss : 0.193740 model2 loss : 0.290532
iteration 924 : model1 loss : 0.194992 model2 loss : 0.266482
iteration 925 : model1 loss : 0.196819 model2 loss : 0.249646
iteration 926 : model1 loss : 0.233604 model2 loss : 0.293690
iteration 927 : model1 loss : 0.175902 model2 loss : 0.325844
iteration 928 : model1 loss : 0.171870 model2 loss : 0.395761
iteration 929 : model1 loss : 0.197374 model2 loss : 0.343143
iteration 930 : model1 loss : 0.219365 model2 loss : 0.283648
iteration 931 : model1 loss : 0.242912 model2 loss : 0.301482
iteration 932 : model1 loss : 0.177953 model2 loss : 0.278421
iteration 933 : model1 loss : 0.187240 model2 loss : 0.424491
iteration 934 : model1 loss : 0.252109 model2 loss : 0.398971
iteration 935 : model1 loss : 0.188132 model2 loss : 0.299598
  9%|██▊                           | 55/589 [22:17<3:26:40, 23.22s/it]iteration 936 : model1 loss : 0.196060 model2 loss : 0.307435
iteration 937 : model1 loss : 0.205678 model2 loss : 0.309019
iteration 938 : model1 loss : 0.174658 model2 loss : 0.266368
iteration 939 : model1 loss : 0.210768 model2 loss : 0.407165
iteration 940 : model1 loss : 0.192597 model2 loss : 0.290125
iteration 941 : model1 loss : 0.244869 model2 loss : 0.287246
iteration 942 : model1 loss : 0.214598 model2 loss : 0.296988
iteration 943 : model1 loss : 0.162993 model2 loss : 0.258225
iteration 944 : model1 loss : 0.190249 model2 loss : 0.223506
iteration 945 : model1 loss : 0.204407 model2 loss : 0.337356
iteration 946 : model1 loss : 0.256888 model2 loss : 0.329672
iteration 947 : model1 loss : 0.223878 model2 loss : 0.251586
iteration 948 : model1 loss : 0.205842 model2 loss : 0.364495
iteration 949 : model1 loss : 0.205692 model2 loss : 0.297516
iteration 950 : model1 loss : 0.221431 model2 loss : 0.341876
iteration 951 : model1 loss : 0.203433 model2 loss : 0.203222
iteration 952 : model1 loss : 0.219205 model2 loss : 0.391721
 10%|██▊                           | 56/589 [22:40<3:25:13, 23.10s/it]iteration 953 : model1 loss : 0.200707 model2 loss : 0.324491
iteration 954 : model1 loss : 0.184270 model2 loss : 0.317269
iteration 955 : model1 loss : 0.188984 model2 loss : 0.508592
iteration 956 : model1 loss : 0.231357 model2 loss : 0.362458
iteration 957 : model1 loss : 0.214099 model2 loss : 0.236800
iteration 958 : model1 loss : 0.252272 model2 loss : 0.249351
iteration 959 : model1 loss : 0.225688 model2 loss : 0.297600
iteration 960 : model1 loss : 0.195264 model2 loss : 0.361670
iteration 961 : model1 loss : 0.224065 model2 loss : 0.286606
iteration 962 : model1 loss : 0.208283 model2 loss : 0.391372
iteration 963 : model1 loss : 0.168403 model2 loss : 0.298011
iteration 964 : model1 loss : 0.211453 model2 loss : 0.293791
iteration 965 : model1 loss : 0.197735 model2 loss : 0.318387
iteration 966 : model1 loss : 0.177883 model2 loss : 0.241627
iteration 967 : model1 loss : 0.244526 model2 loss : 0.289512
iteration 968 : model1 loss : 0.161606 model2 loss : 0.283854
iteration 969 : model1 loss : 0.196597 model2 loss : 0.257815
 10%|██▉                           | 57/589 [23:03<3:23:48, 22.99s/it]iteration 970 : model1 loss : 0.193525 model2 loss : 0.272406
iteration 971 : model1 loss : 0.221504 model2 loss : 0.345472
iteration 972 : model1 loss : 0.257990 model2 loss : 0.337875
iteration 973 : model1 loss : 0.205703 model2 loss : 0.276812
iteration 974 : model1 loss : 0.241425 model2 loss : 0.286357
iteration 975 : model1 loss : 0.193048 model2 loss : 0.269178
iteration 976 : model1 loss : 0.218777 model2 loss : 0.355339
iteration 977 : model1 loss : 0.197235 model2 loss : 0.288003
iteration 978 : model1 loss : 0.196000 model2 loss : 0.268390
iteration 979 : model1 loss : 0.212755 model2 loss : 0.318655
iteration 980 : model1 loss : 0.186289 model2 loss : 0.258227
iteration 981 : model1 loss : 0.225272 model2 loss : 0.306728
iteration 982 : model1 loss : 0.180881 model2 loss : 0.207242
iteration 983 : model1 loss : 0.220688 model2 loss : 0.256060
iteration 984 : model1 loss : 0.207410 model2 loss : 0.330387
iteration 985 : model1 loss : 0.201842 model2 loss : 0.228427
iteration 986 : model1 loss : 0.187435 model2 loss : 0.195101
 10%|██▉                           | 58/589 [23:25<3:22:39, 22.90s/it]iteration 987 : model1 loss : 0.269002 model2 loss : 0.291693
iteration 988 : model1 loss : 0.190679 model2 loss : 0.262286
iteration 989 : model1 loss : 0.197966 model2 loss : 0.312555
iteration 990 : model1 loss : 0.202798 model2 loss : 0.361639
iteration 991 : model1 loss : 0.207097 model2 loss : 0.293583
iteration 992 : model1 loss : 0.215764 model2 loss : 0.284412
iteration 993 : model1 loss : 0.173399 model2 loss : 0.246770
iteration 994 : model1 loss : 0.228752 model2 loss : 0.356700
iteration 995 : model1 loss : 0.197286 model2 loss : 0.269962
iteration 996 : model1 loss : 0.226714 model2 loss : 0.318773
iteration 997 : model1 loss : 0.204911 model2 loss : 0.286286
iteration 998 : model1 loss : 0.197785 model2 loss : 0.307994
iteration 999 : model1 loss : 0.180065 model2 loss : 0.283366
iteration 1000 : model1 loss : 0.201214 model2 loss : 0.320362
iteration 1000 : model1_mean_dice : 0.676318 model1_mean_hd95 : 81.955113 model1_mean_iou : 0.555500
iteration 1000 : model2_mean_dice : 0.772369 model2_mean_hd95 : 71.804646 model2_mean_iou : 0.664881
iteration 1001 : model1 loss : 0.197453 model2 loss : 0.315277
iteration 1002 : model1 loss : 0.197915 model2 loss : 0.244245
iteration 1003 : model1 loss : 0.166512 model2 loss : 0.231662
 10%|███                           | 59/589 [24:09<4:16:10, 29.00s/it]iteration 1004 : model1 loss : 0.211790 model2 loss : 0.241710
iteration 1005 : model1 loss : 0.202177 model2 loss : 0.282423
iteration 1006 : model1 loss : 0.164461 model2 loss : 0.201225
iteration 1007 : model1 loss : 0.182370 model2 loss : 0.267178
iteration 1008 : model1 loss : 0.177094 model2 loss : 0.303734
iteration 1009 : model1 loss : 0.211559 model2 loss : 0.279996
iteration 1010 : model1 loss : 0.149275 model2 loss : 0.208560
iteration 1011 : model1 loss : 0.183822 model2 loss : 0.278418
iteration 1012 : model1 loss : 0.195521 model2 loss : 0.298892
iteration 1013 : model1 loss : 0.221986 model2 loss : 0.280486
iteration 1014 : model1 loss : 0.255169 model2 loss : 0.259416
iteration 1015 : model1 loss : 0.197618 model2 loss : 0.273670
iteration 1016 : model1 loss : 0.210241 model2 loss : 0.413232
iteration 1017 : model1 loss : 0.210242 model2 loss : 0.349451
iteration 1018 : model1 loss : 0.244138 model2 loss : 0.297635
iteration 1019 : model1 loss : 0.158511 model2 loss : 0.204518
iteration 1020 : model1 loss : 0.199768 model2 loss : 0.336833
 10%|███                           | 60/589 [24:31<3:58:53, 27.10s/it]iteration 1021 : model1 loss : 0.234611 model2 loss : 0.329514
iteration 1022 : model1 loss : 0.200986 model2 loss : 0.288230
iteration 1023 : model1 loss : 0.181079 model2 loss : 0.255403
iteration 1024 : model1 loss : 0.196327 model2 loss : 0.313041
iteration 1025 : model1 loss : 0.191112 model2 loss : 0.268904
iteration 1026 : model1 loss : 0.248075 model2 loss : 0.340516
iteration 1027 : model1 loss : 0.200368 model2 loss : 0.288390
iteration 1028 : model1 loss : 0.182385 model2 loss : 0.315161
iteration 1029 : model1 loss : 0.187825 model2 loss : 0.306760
iteration 1030 : model1 loss : 0.230606 model2 loss : 0.452721
iteration 1031 : model1 loss : 0.204534 model2 loss : 0.310788
iteration 1032 : model1 loss : 0.243951 model2 loss : 0.292016
iteration 1033 : model1 loss : 0.172216 model2 loss : 0.227521
iteration 1034 : model1 loss : 0.176859 model2 loss : 0.250513
iteration 1035 : model1 loss : 0.229163 model2 loss : 0.278312
iteration 1036 : model1 loss : 0.197691 model2 loss : 0.314078
iteration 1037 : model1 loss : 0.183596 model2 loss : 0.342811
 10%|███                           | 61/589 [24:54<3:46:42, 25.76s/it]iteration 1038 : model1 loss : 0.203822 model2 loss : 0.293252
iteration 1039 : model1 loss : 0.201802 model2 loss : 0.328415
iteration 1040 : model1 loss : 0.169341 model2 loss : 0.315083
iteration 1041 : model1 loss : 0.168582 model2 loss : 0.277444
iteration 1042 : model1 loss : 0.231505 model2 loss : 0.221346
iteration 1043 : model1 loss : 0.199705 model2 loss : 0.254554
iteration 1044 : model1 loss : 0.229002 model2 loss : 0.308752
iteration 1045 : model1 loss : 0.195846 model2 loss : 0.267310
iteration 1046 : model1 loss : 0.184338 model2 loss : 0.334854
iteration 1047 : model1 loss : 0.249391 model2 loss : 0.334083
iteration 1048 : model1 loss : 0.198072 model2 loss : 0.290621
iteration 1049 : model1 loss : 0.222538 model2 loss : 0.254527
iteration 1050 : model1 loss : 0.192778 model2 loss : 0.342715
iteration 1051 : model1 loss : 0.163331 model2 loss : 0.223297
iteration 1052 : model1 loss : 0.230026 model2 loss : 0.313879
iteration 1053 : model1 loss : 0.201332 model2 loss : 0.243699
iteration 1054 : model1 loss : 0.195023 model2 loss : 0.268454
 11%|███▏                          | 62/589 [25:17<3:38:27, 24.87s/it]iteration 1055 : model1 loss : 0.194226 model2 loss : 0.288719
iteration 1056 : model1 loss : 0.192172 model2 loss : 0.324422
iteration 1057 : model1 loss : 0.202847 model2 loss : 0.225821
iteration 1058 : model1 loss : 0.168036 model2 loss : 0.224232
iteration 1059 : model1 loss : 0.183730 model2 loss : 0.201033
iteration 1060 : model1 loss : 0.194743 model2 loss : 0.304216
iteration 1061 : model1 loss : 0.220551 model2 loss : 0.255862
iteration 1062 : model1 loss : 0.183414 model2 loss : 0.288741
iteration 1063 : model1 loss : 0.171289 model2 loss : 0.261089
iteration 1064 : model1 loss : 0.184624 model2 loss : 0.344711
iteration 1065 : model1 loss : 0.272759 model2 loss : 0.318580
iteration 1066 : model1 loss : 0.173287 model2 loss : 0.208341
iteration 1067 : model1 loss : 0.188867 model2 loss : 0.250647
iteration 1068 : model1 loss : 0.142164 model2 loss : 0.189155
iteration 1069 : model1 loss : 0.230283 model2 loss : 0.293568
iteration 1070 : model1 loss : 0.184468 model2 loss : 0.229397
iteration 1071 : model1 loss : 0.207681 model2 loss : 0.318061
 11%|███▏                          | 63/589 [25:39<3:32:15, 24.21s/it]iteration 1072 : model1 loss : 0.197996 model2 loss : 0.302798
iteration 1073 : model1 loss : 0.212557 model2 loss : 0.255734
iteration 1074 : model1 loss : 0.178042 model2 loss : 0.257383
iteration 1075 : model1 loss : 0.194027 model2 loss : 0.235068
iteration 1076 : model1 loss : 0.181066 model2 loss : 0.235581
iteration 1077 : model1 loss : 0.164289 model2 loss : 0.222662
iteration 1078 : model1 loss : 0.206999 model2 loss : 0.294280
iteration 1079 : model1 loss : 0.255064 model2 loss : 0.332483
iteration 1080 : model1 loss : 0.223191 model2 loss : 0.293308
iteration 1081 : model1 loss : 0.201889 model2 loss : 0.331874
iteration 1082 : model1 loss : 0.198859 model2 loss : 0.224798
iteration 1083 : model1 loss : 0.212453 model2 loss : 0.347356
iteration 1084 : model1 loss : 0.190366 model2 loss : 0.250661
iteration 1085 : model1 loss : 0.207914 model2 loss : 0.257928
iteration 1086 : model1 loss : 0.177232 model2 loss : 0.290837
iteration 1087 : model1 loss : 0.163963 model2 loss : 0.224886
iteration 1088 : model1 loss : 0.180350 model2 loss : 0.225739
Traceback (most recent call last):
  File "/ext3/miniconda3/envs/ssl/lib/python3.9/multiprocessing/queues.py", line 251, in _feed
    send_bytes(obj)
  File "/ext3/miniconda3/envs/ssl/lib/python3.9/multiprocessing/connection.py", line 205, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/ext3/miniconda3/envs/ssl/lib/python3.9/multiprocessing/connection.py", line 416, in _send_bytes
    self._send(header + buf)
  File "/ext3/miniconda3/envs/ssl/lib/python3.9/multiprocessing/connection.py", line 373, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
 11%|███▎                          | 64/589 [26:02<3:27:52, 23.76s/it]iteration 1089 : model1 loss : 0.187812 model2 loss : 0.242217
iteration 1090 : model1 loss : 0.181985 model2 loss : 0.230654
iteration 1091 : model1 loss : 0.223455 model2 loss : 0.270353
iteration 1092 : model1 loss : 0.165278 model2 loss : 0.246880
iteration 1093 : model1 loss : 0.183081 model2 loss : 0.341465
iteration 1094 : model1 loss : 0.156084 model2 loss : 0.205940
iteration 1095 : model1 loss : 0.206375 model2 loss : 0.312959
iteration 1096 : model1 loss : 0.163905 model2 loss : 0.208495
iteration 1097 : model1 loss : 0.216188 model2 loss : 0.298092
iteration 1098 : model1 loss : 0.201490 model2 loss : 0.243403
iteration 1099 : model1 loss : 0.214373 model2 loss : 0.335159
iteration 1100 : model1 loss : 0.257454 model2 loss : 0.314204
iteration 1101 : model1 loss : 0.203418 model2 loss : 0.222682
iteration 1102 : model1 loss : 0.183658 model2 loss : 0.211028
iteration 1103 : model1 loss : 0.237683 model2 loss : 0.368103
iteration 1104 : model1 loss : 0.192074 model2 loss : 0.277195
iteration 1105 : model1 loss : 0.191354 model2 loss : 0.239433
 11%|███▎                          | 65/589 [26:25<3:25:05, 23.48s/it]iteration 1106 : model1 loss : 0.179461 model2 loss : 0.293942
iteration 1107 : model1 loss : 0.221137 model2 loss : 0.280294
iteration 1108 : model1 loss : 0.262364 model2 loss : 0.298194
iteration 1109 : model1 loss : 0.163954 model2 loss : 0.211618
iteration 1110 : model1 loss : 0.230767 model2 loss : 0.243420
iteration 1111 : model1 loss : 0.187963 model2 loss : 0.311173
iteration 1112 : model1 loss : 0.170013 model2 loss : 0.281514
iteration 1113 : model1 loss : 0.182901 model2 loss : 0.206224
iteration 1114 : model1 loss : 0.166780 model2 loss : 0.238858
iteration 1115 : model1 loss : 0.211696 model2 loss : 0.239966
iteration 1116 : model1 loss : 0.228381 model2 loss : 0.271770
iteration 1117 : model1 loss : 0.231726 model2 loss : 0.262501
iteration 1118 : model1 loss : 0.203349 model2 loss : 0.275069
iteration 1119 : model1 loss : 0.209700 model2 loss : 0.328438
iteration 1120 : model1 loss : 0.198953 model2 loss : 0.200647
iteration 1121 : model1 loss : 0.234806 model2 loss : 0.267809
iteration 1122 : model1 loss : 0.218446 model2 loss : 0.256841
 11%|███▎                          | 66/589 [26:48<3:22:42, 23.26s/it]iteration 1123 : model1 loss : 0.192905 model2 loss : 0.252859
iteration 1124 : model1 loss : 0.228667 model2 loss : 0.447687
iteration 1125 : model1 loss : 0.185070 model2 loss : 0.239960
iteration 1126 : model1 loss : 0.170378 model2 loss : 0.240749
iteration 1127 : model1 loss : 0.194790 model2 loss : 0.241590
iteration 1128 : model1 loss : 0.189427 model2 loss : 0.212372
iteration 1129 : model1 loss : 0.224855 model2 loss : 0.452804
iteration 1130 : model1 loss : 0.227285 model2 loss : 0.335010
iteration 1131 : model1 loss : 0.186650 model2 loss : 0.207848
iteration 1132 : model1 loss : 0.186960 model2 loss : 0.198690
iteration 1133 : model1 loss : 0.218500 model2 loss : 0.305388
iteration 1134 : model1 loss : 0.202546 model2 loss : 0.254552
iteration 1135 : model1 loss : 0.187448 model2 loss : 0.257461
iteration 1136 : model1 loss : 0.203818 model2 loss : 0.221937
iteration 1137 : model1 loss : 0.186831 model2 loss : 0.263072
iteration 1138 : model1 loss : 0.169855 model2 loss : 0.247105
iteration 1139 : model1 loss : 0.208230 model2 loss : 0.253214
 11%|███▍                          | 67/589 [27:10<3:20:58, 23.10s/it]iteration 1140 : model1 loss : 0.170326 model2 loss : 0.345870
iteration 1141 : model1 loss : 0.207125 model2 loss : 0.235180
iteration 1142 : model1 loss : 0.246808 model2 loss : 0.285278
iteration 1143 : model1 loss : 0.223410 model2 loss : 0.262371
iteration 1144 : model1 loss : 0.186140 model2 loss : 0.265089
iteration 1145 : model1 loss : 0.183769 model2 loss : 0.239725
iteration 1146 : model1 loss : 0.222360 model2 loss : 0.351333
iteration 1147 : model1 loss : 0.244359 model2 loss : 0.360517
iteration 1148 : model1 loss : 0.161221 model2 loss : 0.289690
iteration 1149 : model1 loss : 0.241493 model2 loss : 0.276157
iteration 1150 : model1 loss : 0.173720 model2 loss : 0.175639
iteration 1151 : model1 loss : 0.227978 model2 loss : 0.264956
iteration 1152 : model1 loss : 0.194854 model2 loss : 0.255969
iteration 1153 : model1 loss : 0.166702 model2 loss : 0.227449
iteration 1154 : model1 loss : 0.209314 model2 loss : 0.296144
iteration 1155 : model1 loss : 0.238055 model2 loss : 0.293897
iteration 1156 : model1 loss : 0.181191 model2 loss : 0.288983
 12%|███▍                          | 68/589 [27:33<3:19:55, 23.02s/it]iteration 1157 : model1 loss : 0.167256 model2 loss : 0.218104
iteration 1158 : model1 loss : 0.212212 model2 loss : 0.444811
iteration 1159 : model1 loss : 0.195269 model2 loss : 0.295110
iteration 1160 : model1 loss : 0.219585 model2 loss : 0.288858
iteration 1161 : model1 loss : 0.196673 model2 loss : 0.217019
iteration 1162 : model1 loss : 0.212630 model2 loss : 0.379437
iteration 1163 : model1 loss : 0.148377 model2 loss : 0.213696
iteration 1164 : model1 loss : 0.181550 model2 loss : 0.238295
iteration 1165 : model1 loss : 0.173718 model2 loss : 0.227368
iteration 1166 : model1 loss : 0.212409 model2 loss : 0.247390
iteration 1167 : model1 loss : 0.158153 model2 loss : 0.192335
iteration 1168 : model1 loss : 0.213704 model2 loss : 0.216274
iteration 1169 : model1 loss : 0.161801 model2 loss : 0.228327
iteration 1170 : model1 loss : 0.163967 model2 loss : 0.280227
iteration 1171 : model1 loss : 0.242740 model2 loss : 0.281367
iteration 1172 : model1 loss : 0.207106 model2 loss : 0.202641
iteration 1173 : model1 loss : 0.208476 model2 loss : 0.236825
 12%|███▌                          | 69/589 [27:56<3:18:50, 22.94s/it]iteration 1174 : model1 loss : 0.222413 model2 loss : 0.359234
iteration 1175 : model1 loss : 0.198170 model2 loss : 0.269602
iteration 1176 : model1 loss : 0.192809 model2 loss : 0.211510
iteration 1177 : model1 loss : 0.166018 model2 loss : 0.237531
iteration 1178 : model1 loss : 0.138017 model2 loss : 0.203999
iteration 1179 : model1 loss : 0.198316 model2 loss : 0.204630
iteration 1180 : model1 loss : 0.178282 model2 loss : 0.237025
iteration 1181 : model1 loss : 0.228555 model2 loss : 0.269020
iteration 1182 : model1 loss : 0.179692 model2 loss : 0.234896
iteration 1183 : model1 loss : 0.194279 model2 loss : 0.271052
iteration 1184 : model1 loss : 0.241302 model2 loss : 0.318411
iteration 1185 : model1 loss : 0.186880 model2 loss : 0.287714
iteration 1186 : model1 loss : 0.196832 model2 loss : 0.313960
iteration 1187 : model1 loss : 0.175291 model2 loss : 0.288277
iteration 1188 : model1 loss : 0.182784 model2 loss : 0.230509
iteration 1189 : model1 loss : 0.180005 model2 loss : 0.234049
iteration 1190 : model1 loss : 0.275170 model2 loss : 0.316530
 12%|███▌                          | 70/589 [28:19<3:17:58, 22.89s/it]iteration 1191 : model1 loss : 0.197486 model2 loss : 0.284158
iteration 1192 : model1 loss : 0.208001 model2 loss : 0.313274
iteration 1193 : model1 loss : 0.201500 model2 loss : 0.221598
iteration 1194 : model1 loss : 0.202580 model2 loss : 0.315029
iteration 1195 : model1 loss : 0.216965 model2 loss : 0.256948
iteration 1196 : model1 loss : 0.181003 model2 loss : 0.250439
iteration 1197 : model1 loss : 0.212979 model2 loss : 0.234876
iteration 1198 : model1 loss : 0.180548 model2 loss : 0.241922
iteration 1199 : model1 loss : 0.167612 model2 loss : 0.227343
iteration 1200 : model1 loss : 0.194215 model2 loss : 0.248748
iteration 1200 : model1_mean_dice : 0.632970 model1_mean_hd95 : 86.283348 model1_mean_iou : 0.513252
iteration 1200 : model2_mean_dice : 0.776511 model2_mean_hd95 : 67.676303 model2_mean_iou : 0.663338
iteration 1201 : model1 loss : 0.192222 model2 loss : 0.296033
iteration 1202 : model1 loss : 0.168933 model2 loss : 0.221428
iteration 1203 : model1 loss : 0.229071 model2 loss : 0.331616
iteration 1204 : model1 loss : 0.215340 model2 loss : 0.241570
iteration 1205 : model1 loss : 0.163476 model2 loss : 0.199509
iteration 1206 : model1 loss : 0.166136 model2 loss : 0.233390
iteration 1207 : model1 loss : 0.194546 model2 loss : 0.332007
 12%|███▌                          | 71/589 [29:02<4:09:17, 28.88s/it]iteration 1208 : model1 loss : 0.214703 model2 loss : 0.248997
iteration 1209 : model1 loss : 0.176390 model2 loss : 0.225616
iteration 1210 : model1 loss : 0.211662 model2 loss : 0.274087
iteration 1211 : model1 loss : 0.208371 model2 loss : 0.257703
iteration 1212 : model1 loss : 0.192666 model2 loss : 0.255039
iteration 1213 : model1 loss : 0.186825 model2 loss : 0.184215
iteration 1214 : model1 loss : 0.214064 model2 loss : 0.179604
iteration 1215 : model1 loss : 0.200388 model2 loss : 0.280352
iteration 1216 : model1 loss : 0.275137 model2 loss : 0.179199
iteration 1217 : model1 loss : 0.174934 model2 loss : 0.216401
iteration 1218 : model1 loss : 0.205392 model2 loss : 0.360458
iteration 1219 : model1 loss : 0.181833 model2 loss : 0.277395
iteration 1220 : model1 loss : 0.152508 model2 loss : 0.195940
iteration 1221 : model1 loss : 0.194101 model2 loss : 0.200062
iteration 1222 : model1 loss : 0.183957 model2 loss : 0.351362
iteration 1223 : model1 loss : 0.220949 model2 loss : 0.410184
iteration 1224 : model1 loss : 0.201451 model2 loss : 0.244494
 12%|███▋                          | 72/589 [29:24<3:52:49, 27.02s/it]iteration 1225 : model1 loss : 0.197767 model2 loss : 0.340967
iteration 1226 : model1 loss : 0.202801 model2 loss : 0.290809
iteration 1227 : model1 loss : 0.162754 model2 loss : 0.248261
iteration 1228 : model1 loss : 0.211445 model2 loss : 0.250187
iteration 1229 : model1 loss : 0.234507 model2 loss : 0.346263
iteration 1230 : model1 loss : 0.166436 model2 loss : 0.316783
iteration 1231 : model1 loss : 0.173568 model2 loss : 0.274957
iteration 1232 : model1 loss : 0.205726 model2 loss : 0.294876
iteration 1233 : model1 loss : 0.145810 model2 loss : 0.258533
iteration 1234 : model1 loss : 0.167301 model2 loss : 0.237186
iteration 1235 : model1 loss : 0.274921 model2 loss : 0.432726
iteration 1236 : model1 loss : 0.214723 model2 loss : 0.291467
iteration 1237 : model1 loss : 0.223647 model2 loss : 0.321812
iteration 1238 : model1 loss : 0.176483 model2 loss : 0.395486
iteration 1239 : model1 loss : 0.220240 model2 loss : 0.306833
iteration 1240 : model1 loss : 0.208458 model2 loss : 0.255459
iteration 1241 : model1 loss : 0.204644 model2 loss : 0.299243
 12%|███▋                          | 73/589 [29:47<3:41:13, 25.72s/it]iteration 1242 : model1 loss : 0.185783 model2 loss : 0.293774
iteration 1243 : model1 loss : 0.147806 model2 loss : 0.227288
iteration 1244 : model1 loss : 0.211515 model2 loss : 0.339136
iteration 1245 : model1 loss : 0.176179 model2 loss : 0.276822
iteration 1246 : model1 loss : 0.256766 model2 loss : 0.281326
iteration 1247 : model1 loss : 0.147525 model2 loss : 0.221120
iteration 1248 : model1 loss : 0.230295 model2 loss : 0.300546
iteration 1249 : model1 loss : 0.236482 model2 loss : 0.257595
iteration 1250 : model1 loss : 0.148990 model2 loss : 0.191502
iteration 1251 : model1 loss : 0.195751 model2 loss : 0.262398
iteration 1252 : model1 loss : 0.197084 model2 loss : 0.267651
iteration 1253 : model1 loss : 0.221539 model2 loss : 0.303231
iteration 1254 : model1 loss : 0.207243 model2 loss : 0.263051
iteration 1255 : model1 loss : 0.213233 model2 loss : 0.217831
iteration 1256 : model1 loss : 0.197688 model2 loss : 0.277006
iteration 1257 : model1 loss : 0.166564 model2 loss : 0.300517
iteration 1258 : model1 loss : 0.217829 model2 loss : 0.268074
 13%|███▊                          | 74/589 [30:10<3:33:17, 24.85s/it]iteration 1259 : model1 loss : 0.170824 model2 loss : 0.297456
iteration 1260 : model1 loss : 0.182765 model2 loss : 0.244489
iteration 1261 : model1 loss : 0.185000 model2 loss : 0.222221
iteration 1262 : model1 loss : 0.178601 model2 loss : 0.198261
iteration 1263 : model1 loss : 0.188880 model2 loss : 0.281522
iteration 1264 : model1 loss : 0.172142 model2 loss : 0.209660
iteration 1265 : model1 loss : 0.216065 model2 loss : 0.252792
iteration 1266 : model1 loss : 0.258606 model2 loss : 0.281889
iteration 1267 : model1 loss : 0.205405 model2 loss : 0.262303
iteration 1268 : model1 loss : 0.196589 model2 loss : 0.240640
iteration 1269 : model1 loss : 0.171337 model2 loss : 0.234397
iteration 1270 : model1 loss : 0.178512 model2 loss : 0.264683
iteration 1271 : model1 loss : 0.224483 model2 loss : 0.230814
iteration 1272 : model1 loss : 0.165299 model2 loss : 0.281151
iteration 1273 : model1 loss : 0.191852 model2 loss : 0.200372
iteration 1274 : model1 loss : 0.177867 model2 loss : 0.234810
iteration 1275 : model1 loss : 0.189210 model2 loss : 0.263681
 13%|███▊                          | 75/589 [30:33<3:27:23, 24.21s/it]iteration 1276 : model1 loss : 0.202197 model2 loss : 0.258043
iteration 1277 : model1 loss : 0.204484 model2 loss : 0.233042
iteration 1278 : model1 loss : 0.157874 model2 loss : 0.189787
iteration 1279 : model1 loss : 0.214419 model2 loss : 0.261742
iteration 1280 : model1 loss : 0.234999 model2 loss : 0.297026
iteration 1281 : model1 loss : 0.141322 model2 loss : 0.162004
iteration 1282 : model1 loss : 0.230725 model2 loss : 0.329638
iteration 1283 : model1 loss : 0.179934 model2 loss : 0.204017
iteration 1284 : model1 loss : 0.170074 model2 loss : 0.156525
iteration 1285 : model1 loss : 0.194025 model2 loss : 0.236400
iteration 1286 : model1 loss : 0.222133 model2 loss : 0.283158
iteration 1287 : model1 loss : 0.209876 model2 loss : 0.302727
iteration 1288 : model1 loss : 0.157952 model2 loss : 0.237056
iteration 1289 : model1 loss : 0.216538 model2 loss : 0.317088
iteration 1290 : model1 loss : 0.163459 model2 loss : 0.196316
iteration 1291 : model1 loss : 0.184599 model2 loss : 0.240537
iteration 1292 : model1 loss : 0.239783 model2 loss : 0.302497
 13%|███▊                          | 76/589 [30:55<3:23:06, 23.75s/it]iteration 1293 : model1 loss : 0.203537 model2 loss : 0.262426
iteration 1294 : model1 loss : 0.199300 model2 loss : 0.261198
iteration 1295 : model1 loss : 0.215474 model2 loss : 0.383512
iteration 1296 : model1 loss : 0.248657 model2 loss : 0.238425
iteration 1297 : model1 loss : 0.185352 model2 loss : 0.198843
iteration 1298 : model1 loss : 0.223187 model2 loss : 0.195112
iteration 1299 : model1 loss : 0.155176 model2 loss : 0.225676
iteration 1300 : model1 loss : 0.159960 model2 loss : 0.236532
iteration 1301 : model1 loss : 0.224923 model2 loss : 0.278907
iteration 1302 : model1 loss : 0.163688 model2 loss : 0.278840
iteration 1303 : model1 loss : 0.166633 model2 loss : 0.163277
iteration 1304 : model1 loss : 0.236192 model2 loss : 0.280106
iteration 1305 : model1 loss : 0.215788 model2 loss : 0.401027
iteration 1306 : model1 loss : 0.215214 model2 loss : 0.259202
iteration 1307 : model1 loss : 0.184987 model2 loss : 0.210113
iteration 1308 : model1 loss : 0.153047 model2 loss : 0.209477
iteration 1309 : model1 loss : 0.204303 model2 loss : 0.245942
 13%|███▉                          | 77/589 [31:18<3:20:17, 23.47s/it]iteration 1310 : model1 loss : 0.182473 model2 loss : 0.332805
iteration 1311 : model1 loss : 0.205423 model2 loss : 0.252204
iteration 1312 : model1 loss : 0.205667 model2 loss : 0.317181
iteration 1313 : model1 loss : 0.196585 model2 loss : 0.258596
iteration 1314 : model1 loss : 0.176625 model2 loss : 0.214303
iteration 1315 : model1 loss : 0.178007 model2 loss : 0.200376
iteration 1316 : model1 loss : 0.221021 model2 loss : 0.245264
iteration 1317 : model1 loss : 0.192150 model2 loss : 0.213550
iteration 1318 : model1 loss : 0.189370 model2 loss : 0.215611
iteration 1319 : model1 loss : 0.197566 model2 loss : 0.286764
iteration 1320 : model1 loss : 0.176113 model2 loss : 0.246469
iteration 1321 : model1 loss : 0.213891 model2 loss : 0.277097
iteration 1322 : model1 loss : 0.180683 model2 loss : 0.241470
iteration 1323 : model1 loss : 0.191330 model2 loss : 0.213887
iteration 1324 : model1 loss : 0.185591 model2 loss : 0.222023
iteration 1325 : model1 loss : 0.201390 model2 loss : 0.206583
iteration 1326 : model1 loss : 0.196182 model2 loss : 0.189216
 13%|███▉                          | 78/589 [31:41<3:17:58, 23.25s/it]iteration 1327 : model1 loss : 0.219158 model2 loss : 0.232990
iteration 1328 : model1 loss : 0.204282 model2 loss : 0.244005
iteration 1329 : model1 loss : 0.155807 model2 loss : 0.241969
iteration 1330 : model1 loss : 0.163319 model2 loss : 0.207490
iteration 1331 : model1 loss : 0.186773 model2 loss : 0.243093
iteration 1332 : model1 loss : 0.185794 model2 loss : 0.304405
iteration 1333 : model1 loss : 0.260015 model2 loss : 0.389737
iteration 1334 : model1 loss : 0.173062 model2 loss : 0.247026
iteration 1335 : model1 loss : 0.199403 model2 loss : 0.322665
iteration 1336 : model1 loss : 0.142989 model2 loss : 0.214077
iteration 1337 : model1 loss : 0.199455 model2 loss : 0.242837
iteration 1338 : model1 loss : 0.192270 model2 loss : 0.198251
iteration 1339 : model1 loss : 0.225278 model2 loss : 0.330588
iteration 1340 : model1 loss : 0.171573 model2 loss : 0.191820
iteration 1341 : model1 loss : 0.181831 model2 loss : 0.206284
iteration 1342 : model1 loss : 0.196117 model2 loss : 0.220614
iteration 1343 : model1 loss : 0.172408 model2 loss : 0.213715
 13%|████                          | 79/589 [32:04<3:16:15, 23.09s/it]iteration 1344 : model1 loss : 0.223218 model2 loss : 0.239386
iteration 1345 : model1 loss : 0.196620 model2 loss : 0.294170
iteration 1346 : model1 loss : 0.177011 model2 loss : 0.233846
iteration 1347 : model1 loss : 0.256579 model2 loss : 0.320481
iteration 1348 : model1 loss : 0.208493 model2 loss : 0.218885
iteration 1349 : model1 loss : 0.194249 model2 loss : 0.239975
iteration 1350 : model1 loss : 0.170215 model2 loss : 0.236607
iteration 1351 : model1 loss : 0.185739 model2 loss : 0.268718
iteration 1352 : model1 loss : 0.172502 model2 loss : 0.263600
iteration 1353 : model1 loss : 0.189364 model2 loss : 0.241341
iteration 1354 : model1 loss : 0.165939 model2 loss : 0.242899
iteration 1355 : model1 loss : 0.170267 model2 loss : 0.257710
iteration 1356 : model1 loss : 0.198609 model2 loss : 0.248760
iteration 1357 : model1 loss : 0.246835 model2 loss : 0.360372
iteration 1358 : model1 loss : 0.227016 model2 loss : 0.346322
iteration 1359 : model1 loss : 0.230338 model2 loss : 0.207399
iteration 1360 : model1 loss : 0.180684 model2 loss : 0.201329
 14%|████                          | 80/589 [32:26<3:15:11, 23.01s/it]iteration 1361 : model1 loss : 0.197866 model2 loss : 0.266777
iteration 1362 : model1 loss : 0.247233 model2 loss : 0.327548
iteration 1363 : model1 loss : 0.178998 model2 loss : 0.339814
iteration 1364 : model1 loss : 0.173386 model2 loss : 0.236454
iteration 1365 : model1 loss : 0.197803 model2 loss : 0.228532
iteration 1366 : model1 loss : 0.167691 model2 loss : 0.250195
iteration 1367 : model1 loss : 0.206237 model2 loss : 0.230286
iteration 1368 : model1 loss : 0.186195 model2 loss : 0.240408
iteration 1369 : model1 loss : 0.146802 model2 loss : 0.269779
iteration 1370 : model1 loss : 0.234331 model2 loss : 0.302828
iteration 1371 : model1 loss : 0.192018 model2 loss : 0.237149
iteration 1372 : model1 loss : 0.195315 model2 loss : 0.236913
iteration 1373 : model1 loss : 0.204764 model2 loss : 0.320601
iteration 1374 : model1 loss : 0.219461 model2 loss : 0.201500
iteration 1375 : model1 loss : 0.179081 model2 loss : 0.221730
iteration 1376 : model1 loss : 0.209526 model2 loss : 0.222038
iteration 1377 : model1 loss : 0.178418 model2 loss : 0.226908
 14%|████▏                         | 81/589 [32:49<3:14:09, 22.93s/it]iteration 1378 : model1 loss : 0.230723 model2 loss : 0.241756
iteration 1379 : model1 loss : 0.180747 model2 loss : 0.230540
iteration 1380 : model1 loss : 0.194970 model2 loss : 0.208541
iteration 1381 : model1 loss : 0.202685 model2 loss : 0.262353
iteration 1382 : model1 loss : 0.243085 model2 loss : 0.293393
iteration 1383 : model1 loss : 0.202712 model2 loss : 0.212680
iteration 1384 : model1 loss : 0.176037 model2 loss : 0.200740
iteration 1385 : model1 loss : 0.198172 model2 loss : 0.251079
iteration 1386 : model1 loss : 0.204232 model2 loss : 0.288278
iteration 1387 : model1 loss : 0.184512 model2 loss : 0.230942
iteration 1388 : model1 loss : 0.168851 model2 loss : 0.199253
iteration 1389 : model1 loss : 0.169497 model2 loss : 0.279458
iteration 1390 : model1 loss : 0.190061 model2 loss : 0.195290
iteration 1391 : model1 loss : 0.234774 model2 loss : 0.296561
iteration 1392 : model1 loss : 0.206641 model2 loss : 0.275539
iteration 1393 : model1 loss : 0.146602 model2 loss : 0.167336
iteration 1394 : model1 loss : 0.199917 model2 loss : 0.290529
 14%|████▏                         | 82/589 [33:12<3:13:22, 22.88s/it]iteration 1395 : model1 loss : 0.179883 model2 loss : 0.255052
iteration 1396 : model1 loss : 0.218643 model2 loss : 0.322181
iteration 1397 : model1 loss : 0.227277 model2 loss : 0.250401
iteration 1398 : model1 loss : 0.212904 model2 loss : 0.244654
iteration 1399 : model1 loss : 0.184336 model2 loss : 0.219271
iteration 1400 : model1 loss : 0.161459 model2 loss : 0.266589
iteration 1400 : model1_mean_dice : 0.632587 model1_mean_hd95 : 87.505361 model1_mean_iou : 0.510506
iteration 1400 : model2_mean_dice : 0.768167 model2_mean_hd95 : 71.584868 model2_mean_iou : 0.662342
iteration 1401 : model1 loss : 0.202205 model2 loss : 0.251908
iteration 1402 : model1 loss : 0.194173 model2 loss : 0.246434
iteration 1403 : model1 loss : 0.190102 model2 loss : 0.307375
iteration 1404 : model1 loss : 0.165411 model2 loss : 0.277511
iteration 1405 : model1 loss : 0.182389 model2 loss : 0.212454
iteration 1406 : model1 loss : 0.167355 model2 loss : 0.191693
iteration 1407 : model1 loss : 0.180126 model2 loss : 0.219405
iteration 1408 : model1 loss : 0.228695 model2 loss : 0.288753
iteration 1409 : model1 loss : 0.212952 model2 loss : 0.259327
iteration 1410 : model1 loss : 0.173719 model2 loss : 0.232302
iteration 1411 : model1 loss : 0.199410 model2 loss : 0.259944
 14%|████▏                         | 83/589 [33:54<4:02:34, 28.76s/it]iteration 1412 : model1 loss : 0.146413 model2 loss : 0.233024
iteration 1413 : model1 loss : 0.233731 model2 loss : 0.301795
iteration 1414 : model1 loss : 0.173756 model2 loss : 0.268842
iteration 1415 : model1 loss : 0.192473 model2 loss : 0.213440
iteration 1416 : model1 loss : 0.182649 model2 loss : 0.214596
iteration 1417 : model1 loss : 0.188083 model2 loss : 0.255548
iteration 1418 : model1 loss : 0.212795 model2 loss : 0.256664
iteration 1419 : model1 loss : 0.165618 model2 loss : 0.184917
iteration 1420 : model1 loss : 0.157778 model2 loss : 0.181518
iteration 1421 : model1 loss : 0.173927 model2 loss : 0.242376
iteration 1422 : model1 loss : 0.241629 model2 loss : 0.308015
iteration 1423 : model1 loss : 0.223163 model2 loss : 0.361025
iteration 1424 : model1 loss : 0.168273 model2 loss : 0.257863
iteration 1425 : model1 loss : 0.217029 model2 loss : 0.298232
iteration 1426 : model1 loss : 0.205591 model2 loss : 0.207645
iteration 1427 : model1 loss : 0.248606 model2 loss : 0.270213
iteration 1428 : model1 loss : 0.160832 model2 loss : 0.280585
 14%|████▎                         | 84/589 [34:17<3:46:36, 26.92s/it]iteration 1429 : model1 loss : 0.180701 model2 loss : 0.306063
iteration 1430 : model1 loss : 0.164434 model2 loss : 0.246331
iteration 1431 : model1 loss : 0.180012 model2 loss : 0.248001
iteration 1432 : model1 loss : 0.212298 model2 loss : 0.246253
iteration 1433 : model1 loss : 0.184298 model2 loss : 0.214060
iteration 1434 : model1 loss : 0.208043 model2 loss : 0.192321
iteration 1435 : model1 loss : 0.239247 model2 loss : 0.272619
iteration 1436 : model1 loss : 0.203330 model2 loss : 0.276829
iteration 1437 : model1 loss : 0.164872 model2 loss : 0.218871
iteration 1438 : model1 loss : 0.181484 model2 loss : 0.223510
iteration 1439 : model1 loss : 0.251170 model2 loss : 0.239630
iteration 1440 : model1 loss : 0.243440 model2 loss : 0.269434
iteration 1441 : model1 loss : 0.183060 model2 loss : 0.239539
iteration 1442 : model1 loss : 0.166331 model2 loss : 0.213748
iteration 1443 : model1 loss : 0.211359 model2 loss : 0.268285
iteration 1444 : model1 loss : 0.188279 model2 loss : 0.210859
iteration 1445 : model1 loss : 0.179095 model2 loss : 0.190962
 14%|████▎                         | 85/589 [34:40<3:35:29, 25.65s/it]iteration 1446 : model1 loss : 0.184372 model2 loss : 0.285220
iteration 1447 : model1 loss : 0.183470 model2 loss : 0.239770
iteration 1448 : model1 loss : 0.165001 model2 loss : 0.185671
iteration 1449 : model1 loss : 0.198483 model2 loss : 0.221837
iteration 1450 : model1 loss : 0.189471 model2 loss : 0.234175
iteration 1451 : model1 loss : 0.189554 model2 loss : 0.196409
iteration 1452 : model1 loss : 0.161121 model2 loss : 0.164029
iteration 1453 : model1 loss : 0.205235 model2 loss : 0.200128
iteration 1454 : model1 loss : 0.193332 model2 loss : 0.222289
iteration 1455 : model1 loss : 0.235734 model2 loss : 0.327048
iteration 1456 : model1 loss : 0.164827 model2 loss : 0.285810
iteration 1457 : model1 loss : 0.147464 model2 loss : 0.232273
iteration 1458 : model1 loss : 0.231357 model2 loss : 0.322796
iteration 1459 : model1 loss : 0.177732 model2 loss : 0.257191
iteration 1460 : model1 loss : 0.206141 model2 loss : 0.187774
iteration 1461 : model1 loss : 0.205178 model2 loss : 0.203360
iteration 1462 : model1 loss : 0.211350 model2 loss : 0.298941
 15%|████▍                         | 86/589 [35:02<3:27:50, 24.79s/it]iteration 1463 : model1 loss : 0.152317 model2 loss : 0.231127
iteration 1464 : model1 loss : 0.149162 model2 loss : 0.222195
iteration 1465 : model1 loss : 0.191135 model2 loss : 0.205651
iteration 1466 : model1 loss : 0.199885 model2 loss : 0.234361
iteration 1467 : model1 loss : 0.188997 model2 loss : 0.223072
iteration 1468 : model1 loss : 0.179767 model2 loss : 0.222763
iteration 1469 : model1 loss : 0.197094 model2 loss : 0.199238
iteration 1470 : model1 loss : 0.199353 model2 loss : 0.271303
iteration 1471 : model1 loss : 0.181187 model2 loss : 0.233516
iteration 1472 : model1 loss : 0.173058 model2 loss : 0.194508
iteration 1473 : model1 loss : 0.228808 model2 loss : 0.326835
iteration 1474 : model1 loss : 0.202337 model2 loss : 0.249915
iteration 1475 : model1 loss : 0.175620 model2 loss : 0.299794
iteration 1476 : model1 loss : 0.178813 model2 loss : 0.239695
iteration 1477 : model1 loss : 0.216586 model2 loss : 0.290322
iteration 1478 : model1 loss : 0.190982 model2 loss : 0.289696
iteration 1479 : model1 loss : 0.214731 model2 loss : 0.303376
 15%|████▍                         | 87/589 [35:25<3:22:13, 24.17s/it]iteration 1480 : model1 loss : 0.226959 model2 loss : 0.240252
iteration 1481 : model1 loss : 0.183938 model2 loss : 0.245124
iteration 1482 : model1 loss : 0.179186 model2 loss : 0.225225
iteration 1483 : model1 loss : 0.168986 model2 loss : 0.272059
iteration 1484 : model1 loss : 0.195836 model2 loss : 0.172153
iteration 1485 : model1 loss : 0.215310 model2 loss : 0.249611
iteration 1486 : model1 loss : 0.197741 model2 loss : 0.262328
iteration 1487 : model1 loss : 0.188161 model2 loss : 0.307371
iteration 1488 : model1 loss : 0.253616 model2 loss : 0.217630
iteration 1489 : model1 loss : 0.195625 model2 loss : 0.224947
iteration 1490 : model1 loss : 0.258858 model2 loss : 0.235679
iteration 1491 : model1 loss : 0.164811 model2 loss : 0.207996
iteration 1492 : model1 loss : 0.193328 model2 loss : 0.181029
iteration 1493 : model1 loss : 0.183617 model2 loss : 0.231151
iteration 1494 : model1 loss : 0.186239 model2 loss : 0.261191
iteration 1495 : model1 loss : 0.202518 model2 loss : 0.229757
iteration 1496 : model1 loss : 0.177824 model2 loss : 0.292736
 15%|████▍                         | 88/589 [35:48<3:18:14, 23.74s/it]iteration 1497 : model1 loss : 0.176909 model2 loss : 0.225952
iteration 1498 : model1 loss : 0.230110 model2 loss : 0.317610
iteration 1499 : model1 loss : 0.249707 model2 loss : 0.324285
iteration 1500 : model1 loss : 0.184856 model2 loss : 0.242775
iteration 1501 : model1 loss : 0.200791 model2 loss : 0.203105
iteration 1502 : model1 loss : 0.251247 model2 loss : 0.258729
iteration 1503 : model1 loss : 0.203001 model2 loss : 0.242396
iteration 1504 : model1 loss : 0.180182 model2 loss : 0.183158
iteration 1505 : model1 loss : 0.189085 model2 loss : 0.236348
iteration 1506 : model1 loss : 0.194373 model2 loss : 0.242164
iteration 1507 : model1 loss : 0.190686 model2 loss : 0.209713
iteration 1508 : model1 loss : 0.196276 model2 loss : 0.231378
iteration 1509 : model1 loss : 0.152630 model2 loss : 0.185435
iteration 1510 : model1 loss : 0.219546 model2 loss : 0.226106
iteration 1511 : model1 loss : 0.194969 model2 loss : 0.217225
iteration 1512 : model1 loss : 0.199400 model2 loss : 0.269244
iteration 1513 : model1 loss : 0.172922 model2 loss : 0.265610
 15%|████▌                         | 89/589 [36:11<3:15:42, 23.49s/it]iteration 1514 : model1 loss : 0.220469 model2 loss : 0.183733
iteration 1515 : model1 loss : 0.200053 model2 loss : 0.219120
iteration 1516 : model1 loss : 0.231792 model2 loss : 0.310871
iteration 1517 : model1 loss : 0.205858 model2 loss : 0.275706
iteration 1518 : model1 loss : 0.150660 model2 loss : 0.172486
iteration 1519 : model1 loss : 0.209453 model2 loss : 0.215714
iteration 1520 : model1 loss : 0.225111 model2 loss : 0.212424
iteration 1521 : model1 loss : 0.181599 model2 loss : 0.221535
iteration 1522 : model1 loss : 0.238716 model2 loss : 0.273651
iteration 1523 : model1 loss : 0.157150 model2 loss : 0.235345
iteration 1524 : model1 loss : 0.174950 model2 loss : 0.244629
iteration 1525 : model1 loss : 0.191869 model2 loss : 0.221453
iteration 1526 : model1 loss : 0.226623 model2 loss : 0.256616
iteration 1527 : model1 loss : 0.180210 model2 loss : 0.234754
iteration 1528 : model1 loss : 0.225531 model2 loss : 0.251303
iteration 1529 : model1 loss : 0.236146 model2 loss : 0.247808
iteration 1530 : model1 loss : 0.204330 model2 loss : 0.278746
 15%|████▌                         | 90/589 [36:34<3:13:29, 23.26s/it]iteration 1531 : model1 loss : 0.161342 model2 loss : 0.211903
iteration 1532 : model1 loss : 0.206689 model2 loss : 0.269104
iteration 1533 : model1 loss : 0.167084 model2 loss : 0.165028
iteration 1534 : model1 loss : 0.190650 model2 loss : 0.270123
iteration 1535 : model1 loss : 0.164818 model2 loss : 0.209479
iteration 1536 : model1 loss : 0.175295 model2 loss : 0.200337
iteration 1537 : model1 loss : 0.262312 model2 loss : 0.242842
iteration 1538 : model1 loss : 0.182055 model2 loss : 0.212362
iteration 1539 : model1 loss : 0.211732 model2 loss : 0.224201
iteration 1540 : model1 loss : 0.178346 model2 loss : 0.258375
iteration 1541 : model1 loss : 0.221823 model2 loss : 0.318711
iteration 1542 : model1 loss : 0.186726 model2 loss : 0.219067
iteration 1543 : model1 loss : 0.182956 model2 loss : 0.213186
iteration 1544 : model1 loss : 0.215168 model2 loss : 0.215216
iteration 1545 : model1 loss : 0.246960 model2 loss : 0.276711
iteration 1546 : model1 loss : 0.202182 model2 loss : 0.282467
iteration 1547 : model1 loss : 0.166885 model2 loss : 0.262188
 15%|████▋                         | 91/589 [36:56<3:11:45, 23.10s/it]iteration 1548 : model1 loss : 0.216458 model2 loss : 0.274904
iteration 1549 : model1 loss : 0.191541 model2 loss : 0.237513
iteration 1550 : model1 loss : 0.182450 model2 loss : 0.269419
iteration 1551 : model1 loss : 0.164016 model2 loss : 0.183077
iteration 1552 : model1 loss : 0.198071 model2 loss : 0.281942
iteration 1553 : model1 loss : 0.191810 model2 loss : 0.196218
iteration 1554 : model1 loss : 0.198104 model2 loss : 0.225546
iteration 1555 : model1 loss : 0.231274 model2 loss : 0.326803
iteration 1556 : model1 loss : 0.190387 model2 loss : 0.210699
iteration 1557 : model1 loss : 0.213881 model2 loss : 0.210392
iteration 1558 : model1 loss : 0.186189 model2 loss : 0.230127
iteration 1559 : model1 loss : 0.218203 model2 loss : 0.206251
iteration 1560 : model1 loss : 0.189917 model2 loss : 0.227391
iteration 1561 : model1 loss : 0.218983 model2 loss : 0.232230
iteration 1562 : model1 loss : 0.186279 model2 loss : 0.248242
iteration 1563 : model1 loss : 0.176086 model2 loss : 0.241381
iteration 1564 : model1 loss : 0.205684 model2 loss : 0.279894
 16%|████▋                         | 92/589 [37:19<3:10:54, 23.05s/it]iteration 1565 : model1 loss : 0.194513 model2 loss : 0.205813
iteration 1566 : model1 loss : 0.212679 model2 loss : 0.267070
iteration 1567 : model1 loss : 0.217424 model2 loss : 0.272373
iteration 1568 : model1 loss : 0.176935 model2 loss : 0.282512
iteration 1569 : model1 loss : 0.194474 model2 loss : 0.236978
iteration 1570 : model1 loss : 0.199234 model2 loss : 0.190225
iteration 1571 : model1 loss : 0.204591 model2 loss : 0.298713
iteration 1572 : model1 loss : 0.224378 model2 loss : 0.240356
iteration 1573 : model1 loss : 0.193343 model2 loss : 0.262418
iteration 1574 : model1 loss : 0.193611 model2 loss : 0.235871
iteration 1575 : model1 loss : 0.184979 model2 loss : 0.255869
iteration 1576 : model1 loss : 0.176877 model2 loss : 0.223033
iteration 1577 : model1 loss : 0.201230 model2 loss : 0.225699
iteration 1578 : model1 loss : 0.181740 model2 loss : 0.179195
iteration 1579 : model1 loss : 0.154020 model2 loss : 0.189435
iteration 1580 : model1 loss : 0.171006 model2 loss : 0.311089
iteration 1581 : model1 loss : 0.206192 model2 loss : 0.270969
 16%|████▋                         | 93/589 [37:42<3:09:39, 22.94s/it]iteration 1582 : model1 loss : 0.208777 model2 loss : 0.244586
iteration 1583 : model1 loss : 0.175086 model2 loss : 0.227623
iteration 1584 : model1 loss : 0.168467 model2 loss : 0.202776
iteration 1585 : model1 loss : 0.245220 model2 loss : 0.285587
iteration 1586 : model1 loss : 0.175073 model2 loss : 0.169288
iteration 1587 : model1 loss : 0.141613 model2 loss : 0.205300
iteration 1588 : model1 loss : 0.164559 model2 loss : 0.206851
iteration 1589 : model1 loss : 0.203479 model2 loss : 0.311513
iteration 1590 : model1 loss : 0.204015 model2 loss : 0.278121
iteration 1591 : model1 loss : 0.215103 model2 loss : 0.267165
iteration 1592 : model1 loss : 0.168275 model2 loss : 0.183068
iteration 1593 : model1 loss : 0.180916 model2 loss : 0.214023
iteration 1594 : model1 loss : 0.192325 model2 loss : 0.208759
iteration 1595 : model1 loss : 0.234246 model2 loss : 0.256009
iteration 1596 : model1 loss : 0.189334 model2 loss : 0.256794
iteration 1597 : model1 loss : 0.208547 model2 loss : 0.245542
iteration 1598 : model1 loss : 0.209063 model2 loss : 0.265657
 16%|████▊                         | 94/589 [38:05<3:08:42, 22.87s/it]iteration 1599 : model1 loss : 0.177759 model2 loss : 0.225153
iteration 1600 : model1 loss : 0.205933 model2 loss : 0.213778
iteration 1600 : model1_mean_dice : 0.682458 model1_mean_hd95 : 78.062783 model1_mean_iou : 0.557021
iteration 1600 : model2_mean_dice : 0.790712 model2_mean_hd95 : 64.329628 model2_mean_iou : 0.682164
iteration 1601 : model1 loss : 0.130644 model2 loss : 0.172383
iteration 1602 : model1 loss : 0.197150 model2 loss : 0.338135
iteration 1603 : model1 loss : 0.195291 model2 loss : 0.229808
iteration 1604 : model1 loss : 0.163771 model2 loss : 0.232503
iteration 1605 : model1 loss : 0.166544 model2 loss : 0.217027
iteration 1606 : model1 loss : 0.229356 model2 loss : 0.239529
iteration 1607 : model1 loss : 0.175135 model2 loss : 0.182232
iteration 1608 : model1 loss : 0.208677 model2 loss : 0.268891
iteration 1609 : model1 loss : 0.219307 model2 loss : 0.282386
iteration 1610 : model1 loss : 0.198461 model2 loss : 0.242646
iteration 1611 : model1 loss : 0.195383 model2 loss : 0.197236
iteration 1612 : model1 loss : 0.175586 model2 loss : 0.203152
iteration 1613 : model1 loss : 0.180052 model2 loss : 0.218822
iteration 1614 : model1 loss : 0.207532 model2 loss : 0.204361
iteration 1615 : model1 loss : 0.190727 model2 loss : 0.223074
 16%|████▊                         | 95/589 [38:48<3:58:29, 28.97s/it]iteration 1616 : model1 loss : 0.176605 model2 loss : 0.168456
iteration 1617 : model1 loss : 0.216247 model2 loss : 0.274185
iteration 1618 : model1 loss : 0.133970 model2 loss : 0.160907
iteration 1619 : model1 loss : 0.211050 model2 loss : 0.216873
iteration 1620 : model1 loss : 0.189407 model2 loss : 0.220121
iteration 1621 : model1 loss : 0.186969 model2 loss : 0.229473
iteration 1622 : model1 loss : 0.180667 model2 loss : 0.208055
iteration 1623 : model1 loss : 0.174680 model2 loss : 0.225864
iteration 1624 : model1 loss : 0.161783 model2 loss : 0.216247
iteration 1625 : model1 loss : 0.211982 model2 loss : 0.258390
iteration 1626 : model1 loss : 0.210490 model2 loss : 0.306073
iteration 1627 : model1 loss : 0.189840 model2 loss : 0.225922
iteration 1628 : model1 loss : 0.187673 model2 loss : 0.213084
iteration 1629 : model1 loss : 0.192691 model2 loss : 0.207882
iteration 1630 : model1 loss : 0.238999 model2 loss : 0.339144
iteration 1631 : model1 loss : 0.137696 model2 loss : 0.158355
iteration 1632 : model1 loss : 0.221120 model2 loss : 0.244555
 16%|████▉                         | 96/589 [39:10<3:42:18, 27.06s/it]iteration 1633 : model1 loss : 0.235119 model2 loss : 0.298650
iteration 1634 : model1 loss : 0.191948 model2 loss : 0.208989
iteration 1635 : model1 loss : 0.170879 model2 loss : 0.199640
iteration 1636 : model1 loss : 0.213685 model2 loss : 0.214256
iteration 1637 : model1 loss : 0.151389 model2 loss : 0.178453
iteration 1638 : model1 loss : 0.199299 model2 loss : 0.264699
iteration 1639 : model1 loss : 0.162653 model2 loss : 0.155735
iteration 1640 : model1 loss : 0.200532 model2 loss : 0.222925
iteration 1641 : model1 loss : 0.212864 model2 loss : 0.285114
iteration 1642 : model1 loss : 0.209086 model2 loss : 0.226682
iteration 1643 : model1 loss : 0.207306 model2 loss : 0.247297
iteration 1644 : model1 loss : 0.197156 model2 loss : 0.193609
iteration 1645 : model1 loss : 0.220625 model2 loss : 0.282073
iteration 1646 : model1 loss : 0.184273 model2 loss : 0.191804
iteration 1647 : model1 loss : 0.136613 model2 loss : 0.162377
iteration 1648 : model1 loss : 0.179851 model2 loss : 0.254676
iteration 1649 : model1 loss : 0.179894 model2 loss : 0.231841
 16%|████▉                         | 97/589 [39:33<3:31:00, 25.73s/it]iteration 1650 : model1 loss : 0.174831 model2 loss : 0.182928
iteration 1651 : model1 loss : 0.219897 model2 loss : 0.262608
iteration 1652 : model1 loss : 0.177732 model2 loss : 0.208091
iteration 1653 : model1 loss : 0.200518 model2 loss : 0.300093
iteration 1654 : model1 loss : 0.226600 model2 loss : 0.275005
iteration 1655 : model1 loss : 0.194687 model2 loss : 0.180887
iteration 1656 : model1 loss : 0.172154 model2 loss : 0.187774
iteration 1657 : model1 loss : 0.181881 model2 loss : 0.186587
iteration 1658 : model1 loss : 0.194006 model2 loss : 0.213052
iteration 1659 : model1 loss : 0.226097 model2 loss : 0.226932
iteration 1660 : model1 loss : 0.188464 model2 loss : 0.185113
iteration 1661 : model1 loss : 0.230953 model2 loss : 0.269616
iteration 1662 : model1 loss : 0.182724 model2 loss : 0.246583
iteration 1663 : model1 loss : 0.174449 model2 loss : 0.244841
iteration 1664 : model1 loss : 0.188521 model2 loss : 0.184379
iteration 1665 : model1 loss : 0.178864 model2 loss : 0.212110
iteration 1666 : model1 loss : 0.201316 model2 loss : 0.226777
 17%|████▉                         | 98/589 [39:56<3:23:27, 24.86s/it]iteration 1667 : model1 loss : 0.186747 model2 loss : 0.189904
iteration 1668 : model1 loss : 0.149160 model2 loss : 0.177400
iteration 1669 : model1 loss : 0.189447 model2 loss : 0.221325
iteration 1670 : model1 loss : 0.231246 model2 loss : 0.278336
iteration 1671 : model1 loss : 0.156198 model2 loss : 0.178232
iteration 1672 : model1 loss : 0.160550 model2 loss : 0.171064
iteration 1673 : model1 loss : 0.236411 model2 loss : 0.258350
iteration 1674 : model1 loss : 0.162065 model2 loss : 0.179425
iteration 1675 : model1 loss : 0.226356 model2 loss : 0.381936
iteration 1676 : model1 loss : 0.213063 model2 loss : 0.228722
iteration 1677 : model1 loss : 0.205497 model2 loss : 0.243023
iteration 1678 : model1 loss : 0.166718 model2 loss : 0.229706
iteration 1679 : model1 loss : 0.186912 model2 loss : 0.244322
iteration 1680 : model1 loss : 0.244050 model2 loss : 0.227645
iteration 1681 : model1 loss : 0.169817 model2 loss : 0.239849
iteration 1682 : model1 loss : 0.218428 model2 loss : 0.229236
iteration 1683 : model1 loss : 0.177001 model2 loss : 0.198254
 17%|█████                         | 99/589 [40:19<3:17:47, 24.22s/it]iteration 1684 : model1 loss : 0.176473 model2 loss : 0.165418
iteration 1685 : model1 loss : 0.192636 model2 loss : 0.211626
iteration 1686 : model1 loss : 0.164349 model2 loss : 0.227813
iteration 1687 : model1 loss : 0.203308 model2 loss : 0.238212
iteration 1688 : model1 loss : 0.231650 model2 loss : 0.229431
iteration 1689 : model1 loss : 0.181144 model2 loss : 0.224545
iteration 1690 : model1 loss : 0.190925 model2 loss : 0.197936
iteration 1691 : model1 loss : 0.218763 model2 loss : 0.274910
iteration 1692 : model1 loss : 0.195337 model2 loss : 0.217865
iteration 1693 : model1 loss : 0.229336 model2 loss : 0.196719
iteration 1694 : model1 loss : 0.161146 model2 loss : 0.213550
iteration 1695 : model1 loss : 0.176512 model2 loss : 0.182157
iteration 1696 : model1 loss : 0.221350 model2 loss : 0.207119
iteration 1697 : model1 loss : 0.186014 model2 loss : 0.194838
iteration 1698 : model1 loss : 0.213074 model2 loss : 0.158343
iteration 1699 : model1 loss : 0.168248 model2 loss : 0.183217
iteration 1700 : model1 loss : 0.291020 model2 loss : 0.299449
 17%|████▉                        | 100/589 [40:41<3:13:56, 23.80s/it]iteration 1701 : model1 loss : 0.188265 model2 loss : 0.235957
iteration 1702 : model1 loss : 0.228302 model2 loss : 0.279845
iteration 1703 : model1 loss : 0.223682 model2 loss : 0.270788
iteration 1704 : model1 loss : 0.167157 model2 loss : 0.197037
iteration 1705 : model1 loss : 0.225945 model2 loss : 0.224819
iteration 1706 : model1 loss : 0.167192 model2 loss : 0.197551
iteration 1707 : model1 loss : 0.225782 model2 loss : 0.208627
iteration 1708 : model1 loss : 0.175289 model2 loss : 0.184830
iteration 1709 : model1 loss : 0.213125 model2 loss : 0.278331
iteration 1710 : model1 loss : 0.214016 model2 loss : 0.269886
iteration 1711 : model1 loss : 0.195983 model2 loss : 0.268948
iteration 1712 : model1 loss : 0.259295 model2 loss : 0.210031
iteration 1713 : model1 loss : 0.163908 model2 loss : 0.230708
iteration 1714 : model1 loss : 0.172298 model2 loss : 0.185328
iteration 1715 : model1 loss : 0.217029 model2 loss : 0.238735
iteration 1716 : model1 loss : 0.195679 model2 loss : 0.279719
iteration 1717 : model1 loss : 0.166306 model2 loss : 0.206684
 17%|████▉                        | 101/589 [41:04<3:10:54, 23.47s/it]iteration 1718 : model1 loss : 0.173163 model2 loss : 0.240908
iteration 1719 : model1 loss : 0.201074 model2 loss : 0.262861
iteration 1720 : model1 loss : 0.154508 model2 loss : 0.184917
iteration 1721 : model1 loss : 0.185862 model2 loss : 0.199639
iteration 1722 : model1 loss : 0.207450 model2 loss : 0.176507
iteration 1723 : model1 loss : 0.205430 model2 loss : 0.233718
iteration 1724 : model1 loss : 0.203032 model2 loss : 0.253901
iteration 1725 : model1 loss : 0.203204 model2 loss : 0.267898
iteration 1726 : model1 loss : 0.222916 model2 loss : 0.253572
iteration 1727 : model1 loss : 0.193540 model2 loss : 0.231200
iteration 1728 : model1 loss : 0.214009 model2 loss : 0.266398
iteration 1729 : model1 loss : 0.165349 model2 loss : 0.252850
iteration 1730 : model1 loss : 0.192996 model2 loss : 0.234918
iteration 1731 : model1 loss : 0.154019 model2 loss : 0.184828
iteration 1732 : model1 loss : 0.177854 model2 loss : 0.193822
iteration 1733 : model1 loss : 0.187271 model2 loss : 0.180702
iteration 1734 : model1 loss : 0.203138 model2 loss : 0.246589
 17%|█████                        | 102/589 [41:27<3:08:40, 23.24s/it]iteration 1735 : model1 loss : 0.214094 model2 loss : 0.222279
iteration 1736 : model1 loss : 0.173439 model2 loss : 0.192702
iteration 1737 : model1 loss : 0.173486 model2 loss : 0.144380
iteration 1738 : model1 loss : 0.145980 model2 loss : 0.177094
iteration 1739 : model1 loss : 0.150754 model2 loss : 0.165970
iteration 1740 : model1 loss : 0.218960 model2 loss : 0.239756
iteration 1741 : model1 loss : 0.226196 model2 loss : 0.240920
iteration 1742 : model1 loss : 0.208764 model2 loss : 0.229167
iteration 1743 : model1 loss : 0.188934 model2 loss : 0.179858
iteration 1744 : model1 loss : 0.233596 model2 loss : 0.163451
iteration 1745 : model1 loss : 0.181234 model2 loss : 0.220162
iteration 1746 : model1 loss : 0.188979 model2 loss : 0.221855
iteration 1747 : model1 loss : 0.219058 model2 loss : 0.332432
iteration 1748 : model1 loss : 0.195595 model2 loss : 0.214665
iteration 1749 : model1 loss : 0.214601 model2 loss : 0.189797
iteration 1750 : model1 loss : 0.189704 model2 loss : 0.284870
iteration 1751 : model1 loss : 0.300978 model2 loss : 0.228994
 17%|█████                        | 103/589 [41:50<3:07:23, 23.14s/it]iteration 1752 : model1 loss : 0.215788 model2 loss : 0.318791
iteration 1753 : model1 loss : 0.253948 model2 loss : 0.320948
iteration 1754 : model1 loss : 0.188437 model2 loss : 0.207577
iteration 1755 : model1 loss : 0.215519 model2 loss : 0.243209
iteration 1756 : model1 loss : 0.148331 model2 loss : 0.180563
iteration 1757 : model1 loss : 0.216036 model2 loss : 0.276159
iteration 1758 : model1 loss : 0.205313 model2 loss : 0.236268
iteration 1759 : model1 loss : 0.161364 model2 loss : 0.159798
iteration 1760 : model1 loss : 0.199198 model2 loss : 0.206032
iteration 1761 : model1 loss : 0.182533 model2 loss : 0.198345
iteration 1762 : model1 loss : 0.144827 model2 loss : 0.189093
iteration 1763 : model1 loss : 0.141950 model2 loss : 0.168524
iteration 1764 : model1 loss : 0.253490 model2 loss : 0.352801
iteration 1765 : model1 loss : 0.173815 model2 loss : 0.214998
iteration 1766 : model1 loss : 0.191690 model2 loss : 0.221525
iteration 1767 : model1 loss : 0.144516 model2 loss : 0.138532
iteration 1768 : model1 loss : 0.173405 model2 loss : 0.221442
 18%|█████                        | 104/589 [42:12<3:05:54, 23.00s/it]iteration 1769 : model1 loss : 0.218075 model2 loss : 0.288038
iteration 1770 : model1 loss : 0.170283 model2 loss : 0.182650
iteration 1771 : model1 loss : 0.220414 model2 loss : 0.298258
iteration 1772 : model1 loss : 0.192300 model2 loss : 0.251781
iteration 1773 : model1 loss : 0.159069 model2 loss : 0.215409
iteration 1774 : model1 loss : 0.189290 model2 loss : 0.178924
iteration 1775 : model1 loss : 0.195389 model2 loss : 0.189730
iteration 1776 : model1 loss : 0.168882 model2 loss : 0.222151
iteration 1777 : model1 loss : 0.184841 model2 loss : 0.247748
iteration 1778 : model1 loss : 0.199586 model2 loss : 0.249732
iteration 1779 : model1 loss : 0.189925 model2 loss : 0.225869
iteration 1780 : model1 loss : 0.167010 model2 loss : 0.212612
iteration 1781 : model1 loss : 0.219121 model2 loss : 0.240487
iteration 1782 : model1 loss : 0.236107 model2 loss : 0.273024
iteration 1783 : model1 loss : 0.193303 model2 loss : 0.235503
iteration 1784 : model1 loss : 0.132842 model2 loss : 0.213364
iteration 1785 : model1 loss : 0.167491 model2 loss : 0.213178
 18%|█████▏                       | 105/589 [42:35<3:04:53, 22.92s/it]iteration 1786 : model1 loss : 0.172953 model2 loss : 0.230611
iteration 1787 : model1 loss : 0.193553 model2 loss : 0.292090
iteration 1788 : model1 loss : 0.198975 model2 loss : 0.261422
iteration 1789 : model1 loss : 0.177563 model2 loss : 0.229267
iteration 1790 : model1 loss : 0.167888 model2 loss : 0.206941
iteration 1791 : model1 loss : 0.183249 model2 loss : 0.180568
iteration 1792 : model1 loss : 0.198623 model2 loss : 0.274994
iteration 1793 : model1 loss : 0.221911 model2 loss : 0.239662
iteration 1794 : model1 loss : 0.206372 model2 loss : 0.273777
iteration 1795 : model1 loss : 0.179111 model2 loss : 0.277896
iteration 1796 : model1 loss : 0.165649 model2 loss : 0.187737
iteration 1797 : model1 loss : 0.174411 model2 loss : 0.252450
iteration 1798 : model1 loss : 0.161908 model2 loss : 0.197018
iteration 1799 : model1 loss : 0.210515 model2 loss : 0.279359
iteration 1800 : model1 loss : 0.195424 model2 loss : 0.282093
iteration 1800 : model1_mean_dice : 0.554904 model1_mean_hd95 : 92.610936 model1_mean_iou : 0.432135
iteration 1800 : model2_mean_dice : 0.745171 model2_mean_hd95 : 72.461207 model2_mean_iou : 0.633711
iteration 1801 : model1 loss : 0.177442 model2 loss : 0.169615
iteration 1802 : model1 loss : 0.175949 model2 loss : 0.197018
 18%|█████▏                       | 106/589 [43:18<3:52:01, 28.82s/it]iteration 1803 : model1 loss : 0.140629 model2 loss : 0.183946
iteration 1804 : model1 loss : 0.165425 model2 loss : 0.170643
iteration 1805 : model1 loss : 0.172265 model2 loss : 0.206354
iteration 1806 : model1 loss : 0.165782 model2 loss : 0.214264
iteration 1807 : model1 loss : 0.193606 model2 loss : 0.199670
iteration 1808 : model1 loss : 0.196410 model2 loss : 0.298022
iteration 1809 : model1 loss : 0.206046 model2 loss : 0.235325
iteration 1810 : model1 loss : 0.182639 model2 loss : 0.213918
iteration 1811 : model1 loss : 0.191775 model2 loss : 0.211157
iteration 1812 : model1 loss : 0.187237 model2 loss : 0.182906
iteration 1813 : model1 loss : 0.211057 model2 loss : 0.251746
iteration 1814 : model1 loss : 0.165215 model2 loss : 0.198563
iteration 1815 : model1 loss : 0.162630 model2 loss : 0.213341
iteration 1816 : model1 loss : 0.194296 model2 loss : 0.229191
iteration 1817 : model1 loss : 0.173722 model2 loss : 0.175376
iteration 1818 : model1 loss : 0.169446 model2 loss : 0.254923
iteration 1819 : model1 loss : 0.200206 model2 loss : 0.283524
 18%|█████▎                       | 107/589 [43:40<3:36:33, 26.96s/it]iteration 1820 : model1 loss : 0.194176 model2 loss : 0.216961
iteration 1821 : model1 loss : 0.252317 model2 loss : 0.281308
iteration 1822 : model1 loss : 0.182196 model2 loss : 0.246091
iteration 1823 : model1 loss : 0.170674 model2 loss : 0.197428
iteration 1824 : model1 loss : 0.153917 model2 loss : 0.156731
iteration 1825 : model1 loss : 0.144007 model2 loss : 0.175921
iteration 1826 : model1 loss : 0.213193 model2 loss : 0.204452
iteration 1827 : model1 loss : 0.237528 model2 loss : 0.202676
iteration 1828 : model1 loss : 0.186426 model2 loss : 0.233067
iteration 1829 : model1 loss : 0.191364 model2 loss : 0.195113
iteration 1830 : model1 loss : 0.169672 model2 loss : 0.166390
iteration 1831 : model1 loss : 0.215672 model2 loss : 0.240837
iteration 1832 : model1 loss : 0.180089 model2 loss : 0.199455
iteration 1833 : model1 loss : 0.174975 model2 loss : 0.210147
iteration 1834 : model1 loss : 0.158648 model2 loss : 0.226959
iteration 1835 : model1 loss : 0.159032 model2 loss : 0.221002
iteration 1836 : model1 loss : 0.184756 model2 loss : 0.245992
 18%|█████▎                       | 108/589 [44:03<3:25:45, 25.67s/it]iteration 1837 : model1 loss : 0.142897 model2 loss : 0.181169
iteration 1838 : model1 loss : 0.142925 model2 loss : 0.149118
iteration 1839 : model1 loss : 0.249619 model2 loss : 0.349601
iteration 1840 : model1 loss : 0.152605 model2 loss : 0.195748
iteration 1841 : model1 loss : 0.189667 model2 loss : 0.214031
iteration 1842 : model1 loss : 0.158525 model2 loss : 0.161050
iteration 1843 : model1 loss : 0.148806 model2 loss : 0.199144
iteration 1844 : model1 loss : 0.261055 model2 loss : 0.285202
iteration 1845 : model1 loss : 0.146676 model2 loss : 0.179313
iteration 1846 : model1 loss : 0.191924 model2 loss : 0.220592
iteration 1847 : model1 loss : 0.232809 model2 loss : 0.286886
iteration 1848 : model1 loss : 0.217755 model2 loss : 0.229381
iteration 1849 : model1 loss : 0.186121 model2 loss : 0.157483
iteration 1850 : model1 loss : 0.185318 model2 loss : 0.187636
iteration 1851 : model1 loss : 0.165112 model2 loss : 0.141074
iteration 1852 : model1 loss : 0.176159 model2 loss : 0.223302
iteration 1853 : model1 loss : 0.155643 model2 loss : 0.178695
 19%|█████▎                       | 109/589 [44:26<3:18:37, 24.83s/it]iteration 1854 : model1 loss : 0.232477 model2 loss : 0.275598
iteration 1855 : model1 loss : 0.191422 model2 loss : 0.225052
iteration 1856 : model1 loss : 0.191068 model2 loss : 0.204533
iteration 1857 : model1 loss : 0.222581 model2 loss : 0.208768
iteration 1858 : model1 loss : 0.193383 model2 loss : 0.213310
iteration 1859 : model1 loss : 0.170468 model2 loss : 0.198035
iteration 1860 : model1 loss : 0.180296 model2 loss : 0.156397
iteration 1861 : model1 loss : 0.243925 model2 loss : 0.286808
iteration 1862 : model1 loss : 0.165444 model2 loss : 0.200924
iteration 1863 : model1 loss : 0.160487 model2 loss : 0.219781
iteration 1864 : model1 loss : 0.198719 model2 loss : 0.181776
iteration 1865 : model1 loss : 0.182611 model2 loss : 0.192942
iteration 1866 : model1 loss : 0.174530 model2 loss : 0.151128
iteration 1867 : model1 loss : 0.211059 model2 loss : 0.266832
iteration 1868 : model1 loss : 0.160237 model2 loss : 0.192658
iteration 1869 : model1 loss : 0.192772 model2 loss : 0.263450
iteration 1870 : model1 loss : 0.189350 model2 loss : 0.288495
 19%|█████▍                       | 110/589 [44:49<3:13:18, 24.21s/it]iteration 1871 : model1 loss : 0.169501 model2 loss : 0.208609
iteration 1872 : model1 loss : 0.160526 model2 loss : 0.166790
iteration 1873 : model1 loss : 0.170341 model2 loss : 0.217439
iteration 1874 : model1 loss : 0.196479 model2 loss : 0.218216
iteration 1875 : model1 loss : 0.265416 model2 loss : 0.235650
iteration 1876 : model1 loss : 0.190421 model2 loss : 0.180544
iteration 1877 : model1 loss : 0.226073 model2 loss : 0.298168
iteration 1878 : model1 loss : 0.207818 model2 loss : 0.178076
iteration 1879 : model1 loss : 0.153353 model2 loss : 0.159211
iteration 1880 : model1 loss : 0.329044 model2 loss : 0.327108
iteration 1881 : model1 loss : 0.200599 model2 loss : 0.278318
iteration 1882 : model1 loss : 0.186186 model2 loss : 0.267988
iteration 1883 : model1 loss : 0.184825 model2 loss : 0.207448
iteration 1884 : model1 loss : 0.154191 model2 loss : 0.171577
iteration 1885 : model1 loss : 0.155868 model2 loss : 0.221956
iteration 1886 : model1 loss : 0.182828 model2 loss : 0.251525
iteration 1887 : model1 loss : 0.156883 model2 loss : 0.213233
 19%|█████▍                       | 111/589 [45:11<3:09:22, 23.77s/it]iteration 1888 : model1 loss : 0.163635 model2 loss : 0.187460
iteration 1889 : model1 loss : 0.154859 model2 loss : 0.172255
iteration 1890 : model1 loss : 0.235644 model2 loss : 0.213100
iteration 1891 : model1 loss : 0.189584 model2 loss : 0.207639
iteration 1892 : model1 loss : 0.199661 model2 loss : 0.280902
iteration 1893 : model1 loss : 0.195604 model2 loss : 0.257630
iteration 1894 : model1 loss : 0.163329 model2 loss : 0.160193
iteration 1895 : model1 loss : 0.165583 model2 loss : 0.217311
iteration 1896 : model1 loss : 0.238238 model2 loss : 0.302182
iteration 1897 : model1 loss : 0.234540 model2 loss : 0.293934
iteration 1898 : model1 loss : 0.203101 model2 loss : 0.252241
iteration 1899 : model1 loss : 0.158003 model2 loss : 0.234216
iteration 1900 : model1 loss : 0.167905 model2 loss : 0.229424
iteration 1901 : model1 loss : 0.168221 model2 loss : 0.295833
iteration 1902 : model1 loss : 0.179167 model2 loss : 0.226142
iteration 1903 : model1 loss : 0.211451 model2 loss : 0.214947
iteration 1904 : model1 loss : 0.182194 model2 loss : 0.180229
 19%|█████▌                       | 112/589 [45:34<3:06:51, 23.50s/it]iteration 1905 : model1 loss : 0.205421 model2 loss : 0.218171
iteration 1906 : model1 loss : 0.191603 model2 loss : 0.189170
iteration 1907 : model1 loss : 0.198620 model2 loss : 0.208835
iteration 1908 : model1 loss : 0.210493 model2 loss : 0.218671
iteration 1909 : model1 loss : 0.220271 model2 loss : 0.211985
iteration 1910 : model1 loss : 0.138322 model2 loss : 0.178740
iteration 1911 : model1 loss : 0.192958 model2 loss : 0.245783
iteration 1912 : model1 loss : 0.138907 model2 loss : 0.186111
iteration 1913 : model1 loss : 0.166432 model2 loss : 0.150160
iteration 1914 : model1 loss : 0.178758 model2 loss : 0.179436
iteration 1915 : model1 loss : 0.184991 model2 loss : 0.261433
iteration 1916 : model1 loss : 0.243921 model2 loss : 0.239641
iteration 1917 : model1 loss : 0.166914 model2 loss : 0.187530
iteration 1918 : model1 loss : 0.206173 model2 loss : 0.241692
iteration 1919 : model1 loss : 0.188968 model2 loss : 0.278014
iteration 1920 : model1 loss : 0.220316 model2 loss : 0.245814
iteration 1921 : model1 loss : 0.209027 model2 loss : 0.156935
 19%|█████▌                       | 113/589 [45:57<3:04:43, 23.28s/it]iteration 1922 : model1 loss : 0.200788 model2 loss : 0.238970
iteration 1923 : model1 loss : 0.147609 model2 loss : 0.153609
iteration 1924 : model1 loss : 0.169066 model2 loss : 0.211447
iteration 1925 : model1 loss : 0.185612 model2 loss : 0.175377
iteration 1926 : model1 loss : 0.182164 model2 loss : 0.239356
iteration 1927 : model1 loss : 0.200761 model2 loss : 0.225996
iteration 1928 : model1 loss : 0.191592 model2 loss : 0.224352
iteration 1929 : model1 loss : 0.206106 model2 loss : 0.201322
iteration 1930 : model1 loss : 0.174745 model2 loss : 0.196655
iteration 1931 : model1 loss : 0.174984 model2 loss : 0.196570
iteration 1932 : model1 loss : 0.193928 model2 loss : 0.222654
iteration 1933 : model1 loss : 0.238346 model2 loss : 0.189374
iteration 1934 : model1 loss : 0.195596 model2 loss : 0.235019
iteration 1935 : model1 loss : 0.146487 model2 loss : 0.269451
iteration 1936 : model1 loss : 0.184145 model2 loss : 0.191493
iteration 1937 : model1 loss : 0.169658 model2 loss : 0.176244
iteration 1938 : model1 loss : 0.181552 model2 loss : 0.210519
 19%|█████▌                       | 114/589 [46:20<3:03:03, 23.12s/it]iteration 1939 : model1 loss : 0.178034 model2 loss : 0.254999
iteration 1940 : model1 loss : 0.198752 model2 loss : 0.207603
iteration 1941 : model1 loss : 0.147384 model2 loss : 0.176863
iteration 1942 : model1 loss : 0.161105 model2 loss : 0.248748
iteration 1943 : model1 loss : 0.192879 model2 loss : 0.176877
iteration 1944 : model1 loss : 0.174152 model2 loss : 0.192378
iteration 1945 : model1 loss : 0.200216 model2 loss : 0.229914
iteration 1946 : model1 loss : 0.209154 model2 loss : 0.258183
iteration 1947 : model1 loss : 0.246326 model2 loss : 0.224695
iteration 1948 : model1 loss : 0.155865 model2 loss : 0.169701
iteration 1949 : model1 loss : 0.172739 model2 loss : 0.225482
iteration 1950 : model1 loss : 0.205209 model2 loss : 0.297397
iteration 1951 : model1 loss : 0.175596 model2 loss : 0.193009
iteration 1952 : model1 loss : 0.168342 model2 loss : 0.196994
iteration 1953 : model1 loss : 0.180856 model2 loss : 0.185157
iteration 1954 : model1 loss : 0.187446 model2 loss : 0.192027
iteration 1955 : model1 loss : 0.208731 model2 loss : 0.210226
 20%|█████▋                       | 115/589 [46:43<3:02:06, 23.05s/it]iteration 1956 : model1 loss : 0.145477 model2 loss : 0.193091
iteration 1957 : model1 loss : 0.136793 model2 loss : 0.169364
iteration 1958 : model1 loss : 0.171305 model2 loss : 0.174062
iteration 1959 : model1 loss : 0.219477 model2 loss : 0.239896
iteration 1960 : model1 loss : 0.250069 model2 loss : 0.204609
iteration 1961 : model1 loss : 0.172553 model2 loss : 0.166280
iteration 1962 : model1 loss : 0.174966 model2 loss : 0.165120
iteration 1963 : model1 loss : 0.213149 model2 loss : 0.259905
iteration 1964 : model1 loss : 0.155831 model2 loss : 0.210785
iteration 1965 : model1 loss : 0.179370 model2 loss : 0.229790
iteration 1966 : model1 loss : 0.150242 model2 loss : 0.173167
iteration 1967 : model1 loss : 0.215578 model2 loss : 0.203102
iteration 1968 : model1 loss : 0.176997 model2 loss : 0.190744
iteration 1969 : model1 loss : 0.197205 model2 loss : 0.314142
iteration 1970 : model1 loss : 0.191052 model2 loss : 0.227274
iteration 1971 : model1 loss : 0.235899 model2 loss : 0.173165
iteration 1972 : model1 loss : 0.212733 model2 loss : 0.299409
 20%|█████▋                       | 116/589 [47:05<3:00:47, 22.93s/it]iteration 1973 : model1 loss : 0.173531 model2 loss : 0.190650
iteration 1974 : model1 loss : 0.180461 model2 loss : 0.226539
iteration 1975 : model1 loss : 0.185920 model2 loss : 0.171626
iteration 1976 : model1 loss : 0.191280 model2 loss : 0.213450
iteration 1977 : model1 loss : 0.194509 model2 loss : 0.228333
iteration 1978 : model1 loss : 0.137506 model2 loss : 0.169266
iteration 1979 : model1 loss : 0.211021 model2 loss : 0.249240
iteration 1980 : model1 loss : 0.174102 model2 loss : 0.232823
iteration 1981 : model1 loss : 0.195840 model2 loss : 0.200950
iteration 1982 : model1 loss : 0.213189 model2 loss : 0.248895
iteration 1983 : model1 loss : 0.168594 model2 loss : 0.193706
iteration 1984 : model1 loss : 0.175208 model2 loss : 0.206236
iteration 1985 : model1 loss : 0.157439 model2 loss : 0.201626
iteration 1986 : model1 loss : 0.187810 model2 loss : 0.205786
iteration 1987 : model1 loss : 0.197003 model2 loss : 0.246354
iteration 1988 : model1 loss : 0.192330 model2 loss : 0.183181
iteration 1989 : model1 loss : 0.183334 model2 loss : 0.203125
 20%|█████▊                       | 117/589 [47:28<2:59:51, 22.86s/it]iteration 1990 : model1 loss : 0.168297 model2 loss : 0.203116
iteration 1991 : model1 loss : 0.202643 model2 loss : 0.190148
iteration 1992 : model1 loss : 0.167543 model2 loss : 0.174418
iteration 1993 : model1 loss : 0.218808 model2 loss : 0.323371
iteration 1994 : model1 loss : 0.193977 model2 loss : 0.202323
iteration 1995 : model1 loss : 0.215821 model2 loss : 0.215779
iteration 1996 : model1 loss : 0.180577 model2 loss : 0.183026
iteration 1997 : model1 loss : 0.184384 model2 loss : 0.284046
iteration 1998 : model1 loss : 0.189133 model2 loss : 0.162029
iteration 1999 : model1 loss : 0.201877 model2 loss : 0.339622
iteration 2000 : model1 loss : 0.219609 model2 loss : 0.200845
iteration 2000 : model1_mean_dice : 0.712538 model1_mean_hd95 : 74.141291 model1_mean_iou : 0.593745
iteration 2000 : model2_mean_dice : 0.775872 model2_mean_hd95 : 67.128718 model2_mean_iou : 0.661977
iteration 2001 : model1 loss : 0.168513 model2 loss : 0.194850
iteration 2002 : model1 loss : 0.172031 model2 loss : 0.171692
iteration 2003 : model1 loss : 0.209238 model2 loss : 0.240396
iteration 2004 : model1 loss : 0.182103 model2 loss : 0.183350
iteration 2005 : model1 loss : 0.167091 model2 loss : 0.142429
iteration 2006 : model1 loss : 0.193554 model2 loss : 0.213072
 20%|█████▊                       | 118/589 [48:11<3:47:05, 28.93s/it]iteration 2007 : model1 loss : 0.225954 model2 loss : 0.239327
iteration 2008 : model1 loss : 0.177394 model2 loss : 0.189971
iteration 2009 : model1 loss : 0.196994 model2 loss : 0.221587
iteration 2010 : model1 loss : 0.159074 model2 loss : 0.175765
iteration 2011 : model1 loss : 0.169531 model2 loss : 0.161420
iteration 2012 : model1 loss : 0.221784 model2 loss : 0.230509
iteration 2013 : model1 loss : 0.213117 model2 loss : 0.240737
iteration 2014 : model1 loss : 0.170507 model2 loss : 0.197071
iteration 2015 : model1 loss : 0.274056 model2 loss : 0.275444
iteration 2016 : model1 loss : 0.161724 model2 loss : 0.168704
iteration 2017 : model1 loss : 0.186199 model2 loss : 0.170885
iteration 2018 : model1 loss : 0.186061 model2 loss : 0.206979
iteration 2019 : model1 loss : 0.169901 model2 loss : 0.167531
iteration 2020 : model1 loss : 0.175814 model2 loss : 0.188175
iteration 2021 : model1 loss : 0.188171 model2 loss : 0.223792
iteration 2022 : model1 loss : 0.151040 model2 loss : 0.168326
iteration 2023 : model1 loss : 0.174287 model2 loss : 0.212591
 20%|█████▊                       | 119/589 [48:34<3:31:46, 27.04s/it]iteration 2024 : model1 loss : 0.179791 model2 loss : 0.186939
iteration 2025 : model1 loss : 0.164236 model2 loss : 0.207327
iteration 2026 : model1 loss : 0.202789 model2 loss : 0.255009
iteration 2027 : model1 loss : 0.212087 model2 loss : 0.210809
iteration 2028 : model1 loss : 0.177075 model2 loss : 0.184088
iteration 2029 : model1 loss : 0.206853 model2 loss : 0.205455
iteration 2030 : model1 loss : 0.169783 model2 loss : 0.165785
iteration 2031 : model1 loss : 0.174532 model2 loss : 0.183197
iteration 2032 : model1 loss : 0.166447 model2 loss : 0.159540
iteration 2033 : model1 loss : 0.185678 model2 loss : 0.199121
iteration 2034 : model1 loss : 0.197358 model2 loss : 0.234680
iteration 2035 : model1 loss : 0.164800 model2 loss : 0.175621
iteration 2036 : model1 loss : 0.209469 model2 loss : 0.221702
iteration 2037 : model1 loss : 0.204999 model2 loss : 0.178174
iteration 2038 : model1 loss : 0.215945 model2 loss : 0.230122
iteration 2039 : model1 loss : 0.168567 model2 loss : 0.173515
iteration 2040 : model1 loss : 0.186619 model2 loss : 0.291710
 20%|█████▉                       | 120/589 [48:56<3:21:13, 25.74s/it]iteration 2041 : model1 loss : 0.168424 model2 loss : 0.193965
iteration 2042 : model1 loss : 0.195074 model2 loss : 0.234779
iteration 2043 : model1 loss : 0.164581 model2 loss : 0.173807
iteration 2044 : model1 loss : 0.219457 model2 loss : 0.233871
iteration 2045 : model1 loss : 0.152309 model2 loss : 0.259226
iteration 2046 : model1 loss : 0.178843 model2 loss : 0.245886
iteration 2047 : model1 loss : 0.168992 model2 loss : 0.229528
iteration 2048 : model1 loss : 0.187075 model2 loss : 0.215034
iteration 2049 : model1 loss : 0.162753 model2 loss : 0.167855
iteration 2050 : model1 loss : 0.184823 model2 loss : 0.180712
iteration 2051 : model1 loss : 0.201183 model2 loss : 0.199002
iteration 2052 : model1 loss : 0.203877 model2 loss : 0.212478
iteration 2053 : model1 loss : 0.185117 model2 loss : 0.238904
iteration 2054 : model1 loss : 0.226657 model2 loss : 0.228551
iteration 2055 : model1 loss : 0.129925 model2 loss : 0.172349
iteration 2056 : model1 loss : 0.191848 model2 loss : 0.232425
iteration 2057 : model1 loss : 0.203759 model2 loss : 0.341603
 21%|█████▉                       | 121/589 [49:19<3:14:12, 24.90s/it]iteration 2058 : model1 loss : 0.131389 model2 loss : 0.137197
iteration 2059 : model1 loss : 0.217293 model2 loss : 0.210841
iteration 2060 : model1 loss : 0.181959 model2 loss : 0.207060
iteration 2061 : model1 loss : 0.192947 model2 loss : 0.299708
iteration 2062 : model1 loss : 0.196190 model2 loss : 0.214363
iteration 2063 : model1 loss : 0.196714 model2 loss : 0.235786
iteration 2064 : model1 loss : 0.176449 model2 loss : 0.188356
iteration 2065 : model1 loss : 0.149835 model2 loss : 0.189135
iteration 2066 : model1 loss : 0.190713 model2 loss : 0.182167
iteration 2067 : model1 loss : 0.228040 model2 loss : 0.258809
iteration 2068 : model1 loss : 0.179340 model2 loss : 0.233417
iteration 2069 : model1 loss : 0.209453 model2 loss : 0.200378
iteration 2070 : model1 loss : 0.193882 model2 loss : 0.185470
iteration 2071 : model1 loss : 0.171354 model2 loss : 0.170496
iteration 2072 : model1 loss : 0.204330 model2 loss : 0.311751
iteration 2073 : model1 loss : 0.191234 model2 loss : 0.251206
iteration 2074 : model1 loss : 0.181663 model2 loss : 0.177295
 21%|██████                       | 122/589 [49:42<3:08:47, 24.26s/it]iteration 2075 : model1 loss : 0.154762 model2 loss : 0.197476
iteration 2076 : model1 loss : 0.206248 model2 loss : 0.206812
iteration 2077 : model1 loss : 0.156808 model2 loss : 0.146029
iteration 2078 : model1 loss : 0.170117 model2 loss : 0.200461
iteration 2079 : model1 loss : 0.228543 model2 loss : 0.229838
iteration 2080 : model1 loss : 0.189805 model2 loss : 0.211350
iteration 2081 : model1 loss : 0.209504 model2 loss : 0.216130
iteration 2082 : model1 loss : 0.165535 model2 loss : 0.181126
iteration 2083 : model1 loss : 0.207540 model2 loss : 0.176514
iteration 2084 : model1 loss : 0.210803 model2 loss : 0.220380
iteration 2085 : model1 loss : 0.192186 model2 loss : 0.174241
iteration 2086 : model1 loss : 0.218201 model2 loss : 0.195328
iteration 2087 : model1 loss : 0.195122 model2 loss : 0.221137
iteration 2088 : model1 loss : 0.229478 model2 loss : 0.202232
iteration 2089 : model1 loss : 0.186461 model2 loss : 0.242164
iteration 2090 : model1 loss : 0.217860 model2 loss : 0.195753
iteration 2091 : model1 loss : 0.183947 model2 loss : 0.218548
 21%|██████                       | 123/589 [50:05<3:04:51, 23.80s/it]iteration 2092 : model1 loss : 0.152701 model2 loss : 0.169244
iteration 2093 : model1 loss : 0.189069 model2 loss : 0.212107
iteration 2094 : model1 loss : 0.244092 model2 loss : 0.267974
iteration 2095 : model1 loss : 0.186787 model2 loss : 0.177190
iteration 2096 : model1 loss : 0.200380 model2 loss : 0.231824
iteration 2097 : model1 loss : 0.179575 model2 loss : 0.198076
iteration 2098 : model1 loss : 0.201137 model2 loss : 0.231200
iteration 2099 : model1 loss : 0.207051 model2 loss : 0.261445
iteration 2100 : model1 loss : 0.183305 model2 loss : 0.183356
iteration 2101 : model1 loss : 0.172655 model2 loss : 0.185489
iteration 2102 : model1 loss : 0.173147 model2 loss : 0.221148
iteration 2103 : model1 loss : 0.239249 model2 loss : 0.246618
iteration 2104 : model1 loss : 0.145366 model2 loss : 0.147436
iteration 2105 : model1 loss : 0.200867 model2 loss : 0.235882
iteration 2106 : model1 loss : 0.193402 model2 loss : 0.331580
iteration 2107 : model1 loss : 0.162177 model2 loss : 0.145730
iteration 2108 : model1 loss : 0.147165 model2 loss : 0.142160
 21%|██████                       | 124/589 [50:28<3:02:17, 23.52s/it]iteration 2109 : model1 loss : 0.166381 model2 loss : 0.176004
iteration 2110 : model1 loss : 0.195708 model2 loss : 0.202403
iteration 2111 : model1 loss : 0.182727 model2 loss : 0.217950
iteration 2112 : model1 loss : 0.183212 model2 loss : 0.194603
iteration 2113 : model1 loss : 0.234094 model2 loss : 0.215395
iteration 2114 : model1 loss : 0.152525 model2 loss : 0.177030
iteration 2115 : model1 loss : 0.148509 model2 loss : 0.151964
iteration 2116 : model1 loss : 0.247452 model2 loss : 0.261306
iteration 2117 : model1 loss : 0.175256 model2 loss : 0.251111
iteration 2118 : model1 loss : 0.206827 model2 loss : 0.234200
iteration 2119 : model1 loss : 0.158231 model2 loss : 0.194956
iteration 2120 : model1 loss : 0.182750 model2 loss : 0.163620
iteration 2121 : model1 loss : 0.173252 model2 loss : 0.186338
iteration 2122 : model1 loss : 0.172392 model2 loss : 0.207441
iteration 2123 : model1 loss : 0.176054 model2 loss : 0.190808
iteration 2124 : model1 loss : 0.234480 model2 loss : 0.191921
iteration 2125 : model1 loss : 0.179696 model2 loss : 0.207083
 21%|██████▏                      | 125/589 [50:50<3:00:07, 23.29s/it]iteration 2126 : model1 loss : 0.151574 model2 loss : 0.179959
iteration 2127 : model1 loss : 0.152118 model2 loss : 0.166273
iteration 2128 : model1 loss : 0.152104 model2 loss : 0.192817
iteration 2129 : model1 loss : 0.225102 model2 loss : 0.272828
iteration 2130 : model1 loss : 0.218223 model2 loss : 0.239399
iteration 2131 : model1 loss : 0.158941 model2 loss : 0.179370
iteration 2132 : model1 loss : 0.183526 model2 loss : 0.258095
iteration 2133 : model1 loss : 0.215434 model2 loss : 0.274916
iteration 2134 : model1 loss : 0.184597 model2 loss : 0.169007
iteration 2135 : model1 loss : 0.166386 model2 loss : 0.179689
iteration 2136 : model1 loss : 0.189426 model2 loss : 0.203333
iteration 2137 : model1 loss : 0.193497 model2 loss : 0.218043
iteration 2138 : model1 loss : 0.194990 model2 loss : 0.177454
iteration 2139 : model1 loss : 0.190117 model2 loss : 0.218369
iteration 2140 : model1 loss : 0.187611 model2 loss : 0.230904
iteration 2141 : model1 loss : 0.186879 model2 loss : 0.203039
iteration 2142 : model1 loss : 0.183942 model2 loss : 0.239311
 21%|██████▏                      | 126/589 [51:13<2:58:22, 23.12s/it]iteration 2143 : model1 loss : 0.188433 model2 loss : 0.219560
iteration 2144 : model1 loss : 0.202997 model2 loss : 0.203003
iteration 2145 : model1 loss : 0.167446 model2 loss : 0.159607
iteration 2146 : model1 loss : 0.192873 model2 loss : 0.234290
iteration 2147 : model1 loss : 0.167257 model2 loss : 0.135979
iteration 2148 : model1 loss : 0.204921 model2 loss : 0.192925
iteration 2149 : model1 loss : 0.173534 model2 loss : 0.189216
iteration 2150 : model1 loss : 0.208889 model2 loss : 0.175963
iteration 2151 : model1 loss : 0.168448 model2 loss : 0.155152
iteration 2152 : model1 loss : 0.202797 model2 loss : 0.187947
iteration 2153 : model1 loss : 0.169930 model2 loss : 0.180356
iteration 2154 : model1 loss : 0.287916 model2 loss : 0.246149
iteration 2155 : model1 loss : 0.184175 model2 loss : 0.201264
iteration 2156 : model1 loss : 0.199254 model2 loss : 0.233669
iteration 2157 : model1 loss : 0.216942 model2 loss : 0.257021
iteration 2158 : model1 loss : 0.188634 model2 loss : 0.163873
iteration 2159 : model1 loss : 0.142563 model2 loss : 0.153340
 22%|██████▎                      | 127/589 [51:36<2:57:17, 23.03s/it]iteration 2160 : model1 loss : 0.154482 model2 loss : 0.150995
iteration 2161 : model1 loss : 0.225003 model2 loss : 0.267387
iteration 2162 : model1 loss : 0.203825 model2 loss : 0.182938
iteration 2163 : model1 loss : 0.233274 model2 loss : 0.208133
iteration 2164 : model1 loss : 0.168409 model2 loss : 0.215890
iteration 2165 : model1 loss : 0.175693 model2 loss : 0.182724
iteration 2166 : model1 loss : 0.221165 model2 loss : 0.183445
iteration 2167 : model1 loss : 0.180613 model2 loss : 0.182617
iteration 2168 : model1 loss : 0.184963 model2 loss : 0.191689
iteration 2169 : model1 loss : 0.163619 model2 loss : 0.143038
iteration 2170 : model1 loss : 0.169191 model2 loss : 0.155482
iteration 2171 : model1 loss : 0.193371 model2 loss : 0.217883
iteration 2172 : model1 loss : 0.197935 model2 loss : 0.216397
iteration 2173 : model1 loss : 0.228203 model2 loss : 0.196497
iteration 2174 : model1 loss : 0.145960 model2 loss : 0.136981
iteration 2175 : model1 loss : 0.157392 model2 loss : 0.191816
iteration 2176 : model1 loss : 0.161853 model2 loss : 0.185517
 22%|██████▎                      | 128/589 [51:59<2:56:10, 22.93s/it]iteration 2177 : model1 loss : 0.205091 model2 loss : 0.228621
iteration 2178 : model1 loss : 0.211535 model2 loss : 0.262620
iteration 2179 : model1 loss : 0.160882 model2 loss : 0.168185
iteration 2180 : model1 loss : 0.162815 model2 loss : 0.177234
iteration 2181 : model1 loss : 0.195687 model2 loss : 0.186668
iteration 2182 : model1 loss : 0.176115 model2 loss : 0.193392
iteration 2183 : model1 loss : 0.202388 model2 loss : 0.225443
iteration 2184 : model1 loss : 0.187481 model2 loss : 0.234850
iteration 2185 : model1 loss : 0.166878 model2 loss : 0.156703
iteration 2186 : model1 loss : 0.151465 model2 loss : 0.168620
iteration 2187 : model1 loss : 0.225611 model2 loss : 0.273876
iteration 2188 : model1 loss : 0.179045 model2 loss : 0.184611
iteration 2189 : model1 loss : 0.154956 model2 loss : 0.195220
iteration 2190 : model1 loss : 0.203596 model2 loss : 0.184757
iteration 2191 : model1 loss : 0.190344 model2 loss : 0.210503
iteration 2192 : model1 loss : 0.218456 model2 loss : 0.222434
iteration 2193 : model1 loss : 0.207607 model2 loss : 0.226941
 22%|██████▎                      | 129/589 [52:21<2:55:25, 22.88s/it]iteration 2194 : model1 loss : 0.183458 model2 loss : 0.224650
iteration 2195 : model1 loss : 0.177609 model2 loss : 0.180554
iteration 2196 : model1 loss : 0.179317 model2 loss : 0.186770
iteration 2197 : model1 loss : 0.198063 model2 loss : 0.224430
iteration 2198 : model1 loss : 0.171277 model2 loss : 0.183346
iteration 2199 : model1 loss : 0.170593 model2 loss : 0.217520
iteration 2200 : model1 loss : 0.217190 model2 loss : 0.239976
iteration 2200 : model1_mean_dice : 0.638917 model1_mean_hd95 : 95.769876 model1_mean_iou : 0.507120
iteration 2200 : model2_mean_dice : 0.795708 model2_mean_hd95 : 65.380791 model2_mean_iou : 0.691668
iteration 2201 : model1 loss : 0.172256 model2 loss : 0.229297
iteration 2202 : model1 loss : 0.206251 model2 loss : 0.142315
iteration 2203 : model1 loss : 0.210242 model2 loss : 0.226622
iteration 2204 : model1 loss : 0.158194 model2 loss : 0.154185
iteration 2205 : model1 loss : 0.139919 model2 loss : 0.141896
iteration 2206 : model1 loss : 0.216508 model2 loss : 0.207740
iteration 2207 : model1 loss : 0.167466 model2 loss : 0.188712
iteration 2208 : model1 loss : 0.230741 model2 loss : 0.249761
iteration 2209 : model1 loss : 0.179219 model2 loss : 0.230264
iteration 2210 : model1 loss : 0.197469 model2 loss : 0.195492
 22%|██████▍                      | 130/589 [53:05<3:41:32, 28.96s/it]iteration 2211 : model1 loss : 0.237912 model2 loss : 0.281959
iteration 2212 : model1 loss : 0.146763 model2 loss : 0.201956
iteration 2213 : model1 loss : 0.148043 model2 loss : 0.207627
iteration 2214 : model1 loss : 0.171664 model2 loss : 0.234292
iteration 2215 : model1 loss : 0.202938 model2 loss : 0.260414
iteration 2216 : model1 loss : 0.215396 model2 loss : 0.248112
iteration 2217 : model1 loss : 0.195578 model2 loss : 0.194479
iteration 2218 : model1 loss : 0.173123 model2 loss : 0.155971
iteration 2219 : model1 loss : 0.141804 model2 loss : 0.199605
iteration 2220 : model1 loss : 0.167689 model2 loss : 0.171009
iteration 2221 : model1 loss : 0.155827 model2 loss : 0.199740
iteration 2222 : model1 loss : 0.207370 model2 loss : 0.235396
iteration 2223 : model1 loss : 0.152801 model2 loss : 0.159770
iteration 2224 : model1 loss : 0.181408 model2 loss : 0.249600
iteration 2225 : model1 loss : 0.176129 model2 loss : 0.203066
iteration 2226 : model1 loss : 0.254100 model2 loss : 0.200440
iteration 2227 : model1 loss : 0.198625 model2 loss : 0.192813
 22%|██████▍                      | 131/589 [53:27<3:26:35, 27.06s/it]iteration 2228 : model1 loss : 0.171226 model2 loss : 0.165207
iteration 2229 : model1 loss : 0.180763 model2 loss : 0.208593
iteration 2230 : model1 loss : 0.174017 model2 loss : 0.202879
iteration 2231 : model1 loss : 0.166873 model2 loss : 0.139523
iteration 2232 : model1 loss : 0.162536 model2 loss : 0.201095
iteration 2233 : model1 loss : 0.179226 model2 loss : 0.154682
iteration 2234 : model1 loss : 0.209217 model2 loss : 0.211510
iteration 2235 : model1 loss : 0.179095 model2 loss : 0.194022
iteration 2236 : model1 loss : 0.193846 model2 loss : 0.195105
iteration 2237 : model1 loss : 0.174332 model2 loss : 0.179774
iteration 2238 : model1 loss : 0.197667 model2 loss : 0.179387
iteration 2239 : model1 loss : 0.180471 model2 loss : 0.276470
iteration 2240 : model1 loss : 0.165321 model2 loss : 0.138520
iteration 2241 : model1 loss : 0.209589 model2 loss : 0.182318
iteration 2242 : model1 loss : 0.164405 model2 loss : 0.174577
iteration 2243 : model1 loss : 0.199796 model2 loss : 0.184469
iteration 2244 : model1 loss : 0.151651 model2 loss : 0.158438
 22%|██████▍                      | 132/589 [53:50<3:15:58, 25.73s/it]iteration 2245 : model1 loss : 0.223347 model2 loss : 0.220435
iteration 2246 : model1 loss : 0.170935 model2 loss : 0.220175
iteration 2247 : model1 loss : 0.116033 model2 loss : 0.173173
iteration 2248 : model1 loss : 0.222055 model2 loss : 0.239169
iteration 2249 : model1 loss : 0.138014 model2 loss : 0.146230
iteration 2250 : model1 loss : 0.189411 model2 loss : 0.204875
iteration 2251 : model1 loss : 0.171142 model2 loss : 0.201829
iteration 2252 : model1 loss : 0.190032 model2 loss : 0.192497
iteration 2253 : model1 loss : 0.194400 model2 loss : 0.215921
iteration 2254 : model1 loss : 0.184557 model2 loss : 0.166802
iteration 2255 : model1 loss : 0.171901 model2 loss : 0.138905
iteration 2256 : model1 loss : 0.174996 model2 loss : 0.246749
iteration 2257 : model1 loss : 0.156311 model2 loss : 0.184918
iteration 2258 : model1 loss : 0.175368 model2 loss : 0.163805
iteration 2259 : model1 loss : 0.174776 model2 loss : 0.198601
iteration 2260 : model1 loss : 0.202425 model2 loss : 0.253482
iteration 2261 : model1 loss : 0.197418 model2 loss : 0.222477
 23%|██████▌                      | 133/589 [54:13<3:08:58, 24.87s/it]iteration 2262 : model1 loss : 0.172010 model2 loss : 0.213503
iteration 2263 : model1 loss : 0.188185 model2 loss : 0.164677
iteration 2264 : model1 loss : 0.150393 model2 loss : 0.152185
iteration 2265 : model1 loss : 0.149820 model2 loss : 0.159494
iteration 2266 : model1 loss : 0.207566 model2 loss : 0.253275
iteration 2267 : model1 loss : 0.236213 model2 loss : 0.252835
iteration 2268 : model1 loss : 0.215018 model2 loss : 0.257095
iteration 2269 : model1 loss : 0.182971 model2 loss : 0.197543
iteration 2270 : model1 loss : 0.153505 model2 loss : 0.144188
iteration 2271 : model1 loss : 0.145287 model2 loss : 0.187615
iteration 2272 : model1 loss : 0.163761 model2 loss : 0.182520
iteration 2273 : model1 loss : 0.171674 model2 loss : 0.221336
iteration 2274 : model1 loss : 0.171644 model2 loss : 0.227789
iteration 2275 : model1 loss : 0.162011 model2 loss : 0.213043
iteration 2276 : model1 loss : 0.207104 model2 loss : 0.212877
iteration 2277 : model1 loss : 0.216576 model2 loss : 0.189341
iteration 2278 : model1 loss : 0.194384 model2 loss : 0.172146
 23%|██████▌                      | 134/589 [54:35<3:03:43, 24.23s/it]iteration 2279 : model1 loss : 0.196118 model2 loss : 0.154059
iteration 2280 : model1 loss : 0.195240 model2 loss : 0.155148
iteration 2281 : model1 loss : 0.164390 model2 loss : 0.261813
iteration 2282 : model1 loss : 0.184727 model2 loss : 0.243501
iteration 2283 : model1 loss : 0.174742 model2 loss : 0.186007
iteration 2284 : model1 loss : 0.190153 model2 loss : 0.190838
iteration 2285 : model1 loss : 0.161074 model2 loss : 0.152963
iteration 2286 : model1 loss : 0.236360 model2 loss : 0.208519
iteration 2287 : model1 loss : 0.164876 model2 loss : 0.202368
iteration 2288 : model1 loss : 0.135238 model2 loss : 0.160522
iteration 2289 : model1 loss : 0.160596 model2 loss : 0.194778
iteration 2290 : model1 loss : 0.160617 model2 loss : 0.211302
iteration 2291 : model1 loss : 0.211686 model2 loss : 0.250534
iteration 2292 : model1 loss : 0.184753 model2 loss : 0.206390
iteration 2293 : model1 loss : 0.171914 model2 loss : 0.156371
iteration 2294 : model1 loss : 0.176979 model2 loss : 0.241087
iteration 2295 : model1 loss : 0.260748 model2 loss : 0.315138
 23%|██████▋                      | 135/589 [54:58<2:59:54, 23.78s/it]iteration 2296 : model1 loss : 0.165386 model2 loss : 0.184547
iteration 2297 : model1 loss : 0.210076 model2 loss : 0.202484
iteration 2298 : model1 loss : 0.210816 model2 loss : 0.272363
iteration 2299 : model1 loss : 0.146318 model2 loss : 0.185828
iteration 2300 : model1 loss : 0.152660 model2 loss : 0.166339
iteration 2301 : model1 loss : 0.182959 model2 loss : 0.202476
iteration 2302 : model1 loss : 0.231015 model2 loss : 0.260708
iteration 2303 : model1 loss : 0.161798 model2 loss : 0.174005
iteration 2304 : model1 loss : 0.174710 model2 loss : 0.198207
iteration 2305 : model1 loss : 0.184152 model2 loss : 0.218055
iteration 2306 : model1 loss : 0.169908 model2 loss : 0.208417
iteration 2307 : model1 loss : 0.182069 model2 loss : 0.244757
iteration 2308 : model1 loss : 0.155458 model2 loss : 0.160827
iteration 2309 : model1 loss : 0.155649 model2 loss : 0.219340
iteration 2310 : model1 loss : 0.181305 model2 loss : 0.171118
iteration 2311 : model1 loss : 0.218748 model2 loss : 0.224384
iteration 2312 : model1 loss : 0.209088 model2 loss : 0.218535
 23%|██████▋                      | 136/589 [55:21<2:57:25, 23.50s/it]iteration 2313 : model1 loss : 0.186835 model2 loss : 0.191659
iteration 2314 : model1 loss : 0.279974 model2 loss : 0.281696
iteration 2315 : model1 loss : 0.192859 model2 loss : 0.172474
iteration 2316 : model1 loss : 0.139862 model2 loss : 0.154366
iteration 2317 : model1 loss : 0.193417 model2 loss : 0.261959
iteration 2318 : model1 loss : 0.207703 model2 loss : 0.204131
iteration 2319 : model1 loss : 0.168545 model2 loss : 0.136790
iteration 2320 : model1 loss : 0.205787 model2 loss : 0.186026
iteration 2321 : model1 loss : 0.168590 model2 loss : 0.168914
iteration 2322 : model1 loss : 0.157760 model2 loss : 0.123990
iteration 2323 : model1 loss : 0.192676 model2 loss : 0.195280
iteration 2324 : model1 loss : 0.214482 model2 loss : 0.195885
iteration 2325 : model1 loss : 0.206012 model2 loss : 0.199270
iteration 2326 : model1 loss : 0.207461 model2 loss : 0.219459
iteration 2327 : model1 loss : 0.193224 model2 loss : 0.183501
iteration 2328 : model1 loss : 0.173073 model2 loss : 0.191354
iteration 2329 : model1 loss : 0.164606 model2 loss : 0.170279
 23%|██████▋                      | 137/589 [55:44<2:55:27, 23.29s/it]iteration 2330 : model1 loss : 0.174972 model2 loss : 0.183268
iteration 2331 : model1 loss : 0.167712 model2 loss : 0.233751
iteration 2332 : model1 loss : 0.175235 model2 loss : 0.180262
iteration 2333 : model1 loss : 0.177353 model2 loss : 0.194045
iteration 2334 : model1 loss : 0.220493 model2 loss : 0.175840
iteration 2335 : model1 loss : 0.189914 model2 loss : 0.213076
iteration 2336 : model1 loss : 0.248166 model2 loss : 0.156531
iteration 2337 : model1 loss : 0.190738 model2 loss : 0.194209
iteration 2338 : model1 loss : 0.194468 model2 loss : 0.215026
iteration 2339 : model1 loss : 0.232641 model2 loss : 0.256827
iteration 2340 : model1 loss : 0.221356 model2 loss : 0.216825
iteration 2341 : model1 loss : 0.160121 model2 loss : 0.141936
iteration 2342 : model1 loss : 0.162712 model2 loss : 0.183523
iteration 2343 : model1 loss : 0.139513 model2 loss : 0.181650
iteration 2344 : model1 loss : 0.169686 model2 loss : 0.194539
iteration 2345 : model1 loss : 0.191839 model2 loss : 0.209331
iteration 2346 : model1 loss : 0.189739 model2 loss : 0.254685
 23%|██████▊                      | 138/589 [56:07<2:53:43, 23.11s/it]iteration 2347 : model1 loss : 0.229112 model2 loss : 0.203361
iteration 2348 : model1 loss : 0.173281 model2 loss : 0.286899
iteration 2349 : model1 loss : 0.252752 model2 loss : 0.187792
iteration 2350 : model1 loss : 0.202462 model2 loss : 0.199589
iteration 2351 : model1 loss : 0.191058 model2 loss : 0.239723
iteration 2352 : model1 loss : 0.193689 model2 loss : 0.199267
iteration 2353 : model1 loss : 0.191453 model2 loss : 0.200805
iteration 2354 : model1 loss : 0.191242 model2 loss : 0.274730
iteration 2355 : model1 loss : 0.168718 model2 loss : 0.226370
iteration 2356 : model1 loss : 0.221833 model2 loss : 0.226873
iteration 2357 : model1 loss : 0.180386 model2 loss : 0.177789
iteration 2358 : model1 loss : 0.174597 model2 loss : 0.201612
iteration 2359 : model1 loss : 0.186040 model2 loss : 0.215864
iteration 2360 : model1 loss : 0.189035 model2 loss : 0.187462
iteration 2361 : model1 loss : 0.166181 model2 loss : 0.208152
iteration 2362 : model1 loss : 0.176618 model2 loss : 0.163091
iteration 2363 : model1 loss : 0.159090 model2 loss : 0.215956
 24%|██████▊                      | 139/589 [56:29<2:52:45, 23.03s/it]iteration 2364 : model1 loss : 0.129182 model2 loss : 0.153075
iteration 2365 : model1 loss : 0.172565 model2 loss : 0.150405
iteration 2366 : model1 loss : 0.175318 model2 loss : 0.188972
iteration 2367 : model1 loss : 0.235788 model2 loss : 0.268136
iteration 2368 : model1 loss : 0.188281 model2 loss : 0.205968
iteration 2369 : model1 loss : 0.228236 model2 loss : 0.276842
iteration 2370 : model1 loss : 0.180566 model2 loss : 0.233975
iteration 2371 : model1 loss : 0.170439 model2 loss : 0.205299
iteration 2372 : model1 loss : 0.207836 model2 loss : 0.304664
iteration 2373 : model1 loss : 0.146124 model2 loss : 0.194124
iteration 2374 : model1 loss : 0.187037 model2 loss : 0.212938
iteration 2375 : model1 loss : 0.166851 model2 loss : 0.182186
iteration 2376 : model1 loss : 0.141297 model2 loss : 0.155393
iteration 2377 : model1 loss : 0.147859 model2 loss : 0.147736
iteration 2378 : model1 loss : 0.191794 model2 loss : 0.222013
iteration 2379 : model1 loss : 0.171213 model2 loss : 0.175588
iteration 2380 : model1 loss : 0.219527 model2 loss : 0.232904
 24%|██████▉                      | 140/589 [56:52<2:51:40, 22.94s/it]iteration 2381 : model1 loss : 0.193311 model2 loss : 0.202460
iteration 2382 : model1 loss : 0.222761 model2 loss : 0.219326
iteration 2383 : model1 loss : 0.168153 model2 loss : 0.179526
iteration 2384 : model1 loss : 0.185145 model2 loss : 0.200410
iteration 2385 : model1 loss : 0.189889 model2 loss : 0.163621
iteration 2386 : model1 loss : 0.162078 model2 loss : 0.186789
iteration 2387 : model1 loss : 0.146117 model2 loss : 0.195425
iteration 2388 : model1 loss : 0.182181 model2 loss : 0.198065
iteration 2389 : model1 loss : 0.179288 model2 loss : 0.218283
iteration 2390 : model1 loss : 0.239013 model2 loss : 0.296839
iteration 2391 : model1 loss : 0.156563 model2 loss : 0.162201
iteration 2392 : model1 loss : 0.195447 model2 loss : 0.211202
iteration 2393 : model1 loss : 0.196081 model2 loss : 0.249503
iteration 2394 : model1 loss : 0.178426 model2 loss : 0.165190
iteration 2395 : model1 loss : 0.173881 model2 loss : 0.260651
iteration 2396 : model1 loss : 0.220042 model2 loss : 0.158661
iteration 2397 : model1 loss : 0.178804 model2 loss : 0.164272
 24%|██████▉                      | 141/589 [57:15<2:50:44, 22.87s/it]iteration 2398 : model1 loss : 0.192820 model2 loss : 0.267819
iteration 2399 : model1 loss : 0.166395 model2 loss : 0.119731
iteration 2400 : model1 loss : 0.204891 model2 loss : 0.211458
iteration 2400 : model1_mean_dice : 0.652111 model1_mean_hd95 : 84.919832 model1_mean_iou : 0.530209
iteration 2400 : model2_mean_dice : 0.772309 model2_mean_hd95 : 66.235802 model2_mean_iou : 0.663518
iteration 2401 : model1 loss : 0.164402 model2 loss : 0.198857
iteration 2402 : model1 loss : 0.185081 model2 loss : 0.183283
iteration 2403 : model1 loss : 0.157495 model2 loss : 0.192904
iteration 2404 : model1 loss : 0.150958 model2 loss : 0.226031
iteration 2405 : model1 loss : 0.169708 model2 loss : 0.160737
iteration 2406 : model1 loss : 0.187736 model2 loss : 0.189016
iteration 2407 : model1 loss : 0.193430 model2 loss : 0.187219
iteration 2408 : model1 loss : 0.200880 model2 loss : 0.218024
iteration 2409 : model1 loss : 0.152456 model2 loss : 0.198136
iteration 2410 : model1 loss : 0.216280 model2 loss : 0.384482
iteration 2411 : model1 loss : 0.175789 model2 loss : 0.151476
iteration 2412 : model1 loss : 0.180960 model2 loss : 0.203208
iteration 2413 : model1 loss : 0.179145 model2 loss : 0.179456
iteration 2414 : model1 loss : 0.210855 model2 loss : 0.180639
 24%|██████▉                      | 142/589 [57:57<3:34:23, 28.78s/it]iteration 2415 : model1 loss : 0.175440 model2 loss : 0.238417
iteration 2416 : model1 loss : 0.181568 model2 loss : 0.218819
iteration 2417 : model1 loss : 0.170819 model2 loss : 0.196392
iteration 2418 : model1 loss : 0.158130 model2 loss : 0.166053
iteration 2419 : model1 loss : 0.172335 model2 loss : 0.182183
iteration 2420 : model1 loss : 0.192237 model2 loss : 0.235509
iteration 2421 : model1 loss : 0.183856 model2 loss : 0.196564
iteration 2422 : model1 loss : 0.177431 model2 loss : 0.190340
iteration 2423 : model1 loss : 0.172638 model2 loss : 0.189738
iteration 2424 : model1 loss : 0.120039 model2 loss : 0.112438
iteration 2425 : model1 loss : 0.183773 model2 loss : 0.190699
iteration 2426 : model1 loss : 0.212939 model2 loss : 0.291748
iteration 2427 : model1 loss : 0.204611 model2 loss : 0.229973
iteration 2428 : model1 loss : 0.151391 model2 loss : 0.170571
iteration 2429 : model1 loss : 0.177184 model2 loss : 0.179433
iteration 2430 : model1 loss : 0.194170 model2 loss : 0.197418
iteration 2431 : model1 loss : 0.165435 model2 loss : 0.213113
 24%|███████                      | 143/589 [58:20<3:20:18, 26.95s/it]iteration 2432 : model1 loss : 0.140390 model2 loss : 0.106866
iteration 2433 : model1 loss : 0.216370 model2 loss : 0.227771
iteration 2434 : model1 loss : 0.158396 model2 loss : 0.211229
iteration 2435 : model1 loss : 0.189775 model2 loss : 0.179713
iteration 2436 : model1 loss : 0.174382 model2 loss : 0.173935
iteration 2437 : model1 loss : 0.187101 model2 loss : 0.162687
iteration 2438 : model1 loss : 0.168034 model2 loss : 0.235438
iteration 2439 : model1 loss : 0.226542 model2 loss : 0.262636
iteration 2440 : model1 loss : 0.151942 model2 loss : 0.170254
iteration 2441 : model1 loss : 0.179915 model2 loss : 0.208263
iteration 2442 : model1 loss : 0.187076 model2 loss : 0.197352
iteration 2443 : model1 loss : 0.198415 model2 loss : 0.217586
iteration 2444 : model1 loss : 0.158721 model2 loss : 0.165804
iteration 2445 : model1 loss : 0.133537 model2 loss : 0.149678
iteration 2446 : model1 loss : 0.184154 model2 loss : 0.171226
iteration 2447 : model1 loss : 0.195566 model2 loss : 0.180992
iteration 2448 : model1 loss : 0.174266 model2 loss : 0.152386
 24%|███████                      | 144/589 [58:43<3:10:16, 25.65s/it]iteration 2449 : model1 loss : 0.174325 model2 loss : 0.187224
iteration 2450 : model1 loss : 0.147972 model2 loss : 0.137141
iteration 2451 : model1 loss : 0.174661 model2 loss : 0.183044
iteration 2452 : model1 loss : 0.179331 model2 loss : 0.187807
iteration 2453 : model1 loss : 0.143190 model2 loss : 0.139102
iteration 2454 : model1 loss : 0.194492 model2 loss : 0.229576
iteration 2455 : model1 loss : 0.163701 model2 loss : 0.133212
iteration 2456 : model1 loss : 0.206688 model2 loss : 0.363652
iteration 2457 : model1 loss : 0.194298 model2 loss : 0.182638
iteration 2458 : model1 loss : 0.127694 model2 loss : 0.206193
iteration 2459 : model1 loss : 0.190240 model2 loss : 0.249316
iteration 2460 : model1 loss : 0.181671 model2 loss : 0.222021
iteration 2461 : model1 loss : 0.204785 model2 loss : 0.226793
iteration 2462 : model1 loss : 0.178394 model2 loss : 0.197591
iteration 2463 : model1 loss : 0.152482 model2 loss : 0.244041
iteration 2464 : model1 loss : 0.260191 model2 loss : 0.287437
iteration 2465 : model1 loss : 0.168824 model2 loss : 0.205934
 25%|███████▏                     | 145/589 [59:06<3:03:36, 24.81s/it]iteration 2466 : model1 loss : 0.185685 model2 loss : 0.208707
iteration 2467 : model1 loss : 0.165101 model2 loss : 0.193971
iteration 2468 : model1 loss : 0.186195 model2 loss : 0.187567
iteration 2469 : model1 loss : 0.221569 model2 loss : 0.231885
iteration 2470 : model1 loss : 0.147923 model2 loss : 0.213527
iteration 2471 : model1 loss : 0.158968 model2 loss : 0.165786
iteration 2472 : model1 loss : 0.178217 model2 loss : 0.203660
iteration 2473 : model1 loss : 0.149666 model2 loss : 0.199215
iteration 2474 : model1 loss : 0.160981 model2 loss : 0.205039
iteration 2475 : model1 loss : 0.183222 model2 loss : 0.217330
iteration 2476 : model1 loss : 0.212672 model2 loss : 0.267169
iteration 2477 : model1 loss : 0.192333 model2 loss : 0.230879
iteration 2478 : model1 loss : 0.172547 model2 loss : 0.194292
iteration 2479 : model1 loss : 0.175011 model2 loss : 0.208596
iteration 2480 : model1 loss : 0.202756 model2 loss : 0.319514
iteration 2481 : model1 loss : 0.192606 model2 loss : 0.234598
iteration 2482 : model1 loss : 0.189193 model2 loss : 0.138830
 25%|███████▏                     | 146/589 [59:28<2:58:31, 24.18s/it]iteration 2483 : model1 loss : 0.204221 model2 loss : 0.214411
iteration 2484 : model1 loss : 0.172511 model2 loss : 0.214892
iteration 2485 : model1 loss : 0.216627 model2 loss : 0.220339
iteration 2486 : model1 loss : 0.159489 model2 loss : 0.177425
iteration 2487 : model1 loss : 0.170563 model2 loss : 0.194246
iteration 2488 : model1 loss : 0.244756 model2 loss : 0.258967
iteration 2489 : model1 loss : 0.184394 model2 loss : 0.179277
iteration 2490 : model1 loss : 0.190375 model2 loss : 0.182792
iteration 2491 : model1 loss : 0.149836 model2 loss : 0.140864
iteration 2492 : model1 loss : 0.204950 model2 loss : 0.188170
iteration 2493 : model1 loss : 0.180208 model2 loss : 0.163748
iteration 2494 : model1 loss : 0.179370 model2 loss : 0.223111
iteration 2495 : model1 loss : 0.164817 model2 loss : 0.150978
iteration 2496 : model1 loss : 0.163686 model2 loss : 0.140037
iteration 2497 : model1 loss : 0.147051 model2 loss : 0.160841
iteration 2498 : model1 loss : 0.148274 model2 loss : 0.168500
iteration 2499 : model1 loss : 0.182903 model2 loss : 0.245768
 25%|███████▏                     | 147/589 [59:51<2:54:55, 23.75s/it]iteration 2500 : model1 loss : 0.183190 model2 loss : 0.157402
iteration 2501 : model1 loss : 0.189645 model2 loss : 0.188569
iteration 2502 : model1 loss : 0.154239 model2 loss : 0.144010
iteration 2503 : model1 loss : 0.134627 model2 loss : 0.151750
iteration 2504 : model1 loss : 0.164816 model2 loss : 0.182945
iteration 2505 : model1 loss : 0.179541 model2 loss : 0.166469
iteration 2506 : model1 loss : 0.183548 model2 loss : 0.169277
iteration 2507 : model1 loss : 0.146300 model2 loss : 0.143995
iteration 2508 : model1 loss : 0.198406 model2 loss : 0.184529
iteration 2509 : model1 loss : 0.219046 model2 loss : 0.282953
iteration 2510 : model1 loss : 0.139537 model2 loss : 0.143427
iteration 2511 : model1 loss : 0.181224 model2 loss : 0.206698
iteration 2512 : model1 loss : 0.226827 model2 loss : 0.250704
iteration 2513 : model1 loss : 0.198100 model2 loss : 0.195413
iteration 2514 : model1 loss : 0.208107 model2 loss : 0.192108
iteration 2515 : model1 loss : 0.161107 model2 loss : 0.175360
iteration 2516 : model1 loss : 0.189042 model2 loss : 0.211540
 25%|██████▊                    | 148/589 [1:00:14<2:52:40, 23.49s/it]iteration 2517 : model1 loss : 0.173676 model2 loss : 0.170707
iteration 2518 : model1 loss : 0.183326 model2 loss : 0.191726
iteration 2519 : model1 loss : 0.167597 model2 loss : 0.170093
iteration 2520 : model1 loss : 0.159907 model2 loss : 0.174566
iteration 2521 : model1 loss : 0.163796 model2 loss : 0.172318
iteration 2522 : model1 loss : 0.200423 model2 loss : 0.191937
iteration 2523 : model1 loss : 0.153734 model2 loss : 0.220183
iteration 2524 : model1 loss : 0.183331 model2 loss : 0.205823
iteration 2525 : model1 loss : 0.201723 model2 loss : 0.303124
iteration 2526 : model1 loss : 0.136007 model2 loss : 0.172252
iteration 2527 : model1 loss : 0.179996 model2 loss : 0.192212
iteration 2528 : model1 loss : 0.214014 model2 loss : 0.163761
iteration 2529 : model1 loss : 0.166120 model2 loss : 0.195060
iteration 2530 : model1 loss : 0.179373 model2 loss : 0.189668
iteration 2531 : model1 loss : 0.181341 model2 loss : 0.180488
iteration 2532 : model1 loss : 0.150547 model2 loss : 0.134437
iteration 2533 : model1 loss : 0.176550 model2 loss : 0.168980
 25%|██████▊                    | 149/589 [1:00:37<2:50:40, 23.27s/it]iteration 2534 : model1 loss : 0.238349 model2 loss : 0.189887
iteration 2535 : model1 loss : 0.185054 model2 loss : 0.202274
iteration 2536 : model1 loss : 0.167369 model2 loss : 0.156829
iteration 2537 : model1 loss : 0.197308 model2 loss : 0.201932
iteration 2538 : model1 loss : 0.192434 model2 loss : 0.195491
iteration 2539 : model1 loss : 0.189115 model2 loss : 0.206739
iteration 2540 : model1 loss : 0.176459 model2 loss : 0.187944
iteration 2541 : model1 loss : 0.187356 model2 loss : 0.210680
iteration 2542 : model1 loss : 0.208999 model2 loss : 0.194588
iteration 2543 : model1 loss : 0.154366 model2 loss : 0.129917
iteration 2544 : model1 loss : 0.162481 model2 loss : 0.195982
iteration 2545 : model1 loss : 0.188467 model2 loss : 0.153585
iteration 2546 : model1 loss : 0.184865 model2 loss : 0.153008
iteration 2547 : model1 loss : 0.182630 model2 loss : 0.194829
iteration 2548 : model1 loss : 0.183833 model2 loss : 0.166914
iteration 2549 : model1 loss : 0.141660 model2 loss : 0.178873
iteration 2550 : model1 loss : 0.204410 model2 loss : 0.219428
 25%|██████▉                    | 150/589 [1:01:00<2:49:23, 23.15s/it]iteration 2551 : model1 loss : 0.163528 model2 loss : 0.225432
iteration 2552 : model1 loss : 0.186709 model2 loss : 0.223993
iteration 2553 : model1 loss : 0.160934 model2 loss : 0.165584
iteration 2554 : model1 loss : 0.203378 model2 loss : 0.211995
iteration 2555 : model1 loss : 0.174433 model2 loss : 0.149563
iteration 2556 : model1 loss : 0.204992 model2 loss : 0.186864
iteration 2557 : model1 loss : 0.156862 model2 loss : 0.163894
iteration 2558 : model1 loss : 0.174180 model2 loss : 0.194726
iteration 2559 : model1 loss : 0.159506 model2 loss : 0.160911
iteration 2560 : model1 loss : 0.153295 model2 loss : 0.128882
iteration 2561 : model1 loss : 0.148567 model2 loss : 0.156607
iteration 2562 : model1 loss : 0.167242 model2 loss : 0.211899
iteration 2563 : model1 loss : 0.155925 model2 loss : 0.163973
iteration 2564 : model1 loss : 0.226682 model2 loss : 0.224818
iteration 2565 : model1 loss : 0.248768 model2 loss : 0.258700
iteration 2566 : model1 loss : 0.193620 model2 loss : 0.174146
iteration 2567 : model1 loss : 0.170425 model2 loss : 0.136695
Traceback (most recent call last):
  File "/ext3/miniconda3/envs/ssl/lib/python3.9/multiprocessing/queues.py", line 251, in _feed
    send_bytes(obj)
  File "/ext3/miniconda3/envs/ssl/lib/python3.9/multiprocessing/connection.py", line 205, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/ext3/miniconda3/envs/ssl/lib/python3.9/multiprocessing/connection.py", line 416, in _send_bytes
    self._send(header + buf)
  File "/ext3/miniconda3/envs/ssl/lib/python3.9/multiprocessing/connection.py", line 373, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
 26%|██████▉                    | 151/589 [1:01:22<2:48:05, 23.03s/it]iteration 2568 : model1 loss : 0.193614 model2 loss : 0.195223
iteration 2569 : model1 loss : 0.160832 model2 loss : 0.135313
iteration 2570 : model1 loss : 0.209390 model2 loss : 0.194906
iteration 2571 : model1 loss : 0.177336 model2 loss : 0.198336
iteration 2572 : model1 loss : 0.185167 model2 loss : 0.234497
iteration 2573 : model1 loss : 0.215530 model2 loss : 0.161990
iteration 2574 : model1 loss : 0.164984 model2 loss : 0.244106
iteration 2575 : model1 loss : 0.152560 model2 loss : 0.140373
iteration 2576 : model1 loss : 0.209719 model2 loss : 0.182357
iteration 2577 : model1 loss : 0.167717 model2 loss : 0.171390
iteration 2578 : model1 loss : 0.146903 model2 loss : 0.137687
iteration 2579 : model1 loss : 0.194327 model2 loss : 0.141244
iteration 2580 : model1 loss : 0.211974 model2 loss : 0.183527
iteration 2581 : model1 loss : 0.141111 model2 loss : 0.126975
iteration 2582 : model1 loss : 0.175191 model2 loss : 0.169590
iteration 2583 : model1 loss : 0.153671 model2 loss : 0.189035
iteration 2584 : model1 loss : 0.184875 model2 loss : 0.210707
 26%|██████▉                    | 152/589 [1:01:45<2:47:02, 22.93s/it]iteration 2585 : model1 loss : 0.178537 model2 loss : 0.190186
iteration 2586 : model1 loss : 0.149197 model2 loss : 0.171354
iteration 2587 : model1 loss : 0.191718 model2 loss : 0.232696
iteration 2588 : model1 loss : 0.189102 model2 loss : 0.121960
iteration 2589 : model1 loss : 0.161536 model2 loss : 0.224967
iteration 2590 : model1 loss : 0.146205 model2 loss : 0.122096
iteration 2591 : model1 loss : 0.142536 model2 loss : 0.143314
iteration 2592 : model1 loss : 0.195817 model2 loss : 0.205944
iteration 2593 : model1 loss : 0.186534 model2 loss : 0.220064
iteration 2594 : model1 loss : 0.179942 model2 loss : 0.263006
iteration 2595 : model1 loss : 0.200552 model2 loss : 0.175046
iteration 2596 : model1 loss : 0.167660 model2 loss : 0.159695
iteration 2597 : model1 loss : 0.223050 model2 loss : 0.217067
iteration 2598 : model1 loss : 0.147468 model2 loss : 0.127009
iteration 2599 : model1 loss : 0.221593 model2 loss : 0.270226
iteration 2600 : model1 loss : 0.209214 model2 loss : 0.224956
iteration 2600 : model1_mean_dice : 0.631037 model1_mean_hd95 : 85.261525 model1_mean_iou : 0.511121
iteration 2600 : model2_mean_dice : 0.763568 model2_mean_hd95 : 65.035096 model2_mean_iou : 0.655798
iteration 2601 : model1 loss : 0.173450 model2 loss : 0.165336
 26%|███████                    | 153/589 [1:02:28<3:29:27, 28.83s/it]iteration 2602 : model1 loss : 0.174739 model2 loss : 0.202035
iteration 2603 : model1 loss : 0.176018 model2 loss : 0.151611
iteration 2604 : model1 loss : 0.221004 model2 loss : 0.190785
iteration 2605 : model1 loss : 0.161172 model2 loss : 0.179715
iteration 2606 : model1 loss : 0.147047 model2 loss : 0.189695
iteration 2607 : model1 loss : 0.188327 model2 loss : 0.199414
iteration 2608 : model1 loss : 0.175444 model2 loss : 0.184791
iteration 2609 : model1 loss : 0.172573 model2 loss : 0.155111
iteration 2610 : model1 loss : 0.182606 model2 loss : 0.205761
iteration 2611 : model1 loss : 0.155364 model2 loss : 0.178781
iteration 2612 : model1 loss : 0.216725 model2 loss : 0.248701
iteration 2613 : model1 loss : 0.163129 model2 loss : 0.158866
iteration 2614 : model1 loss : 0.214212 model2 loss : 0.223069
iteration 2615 : model1 loss : 0.196895 model2 loss : 0.169398
iteration 2616 : model1 loss : 0.161081 model2 loss : 0.208104
iteration 2617 : model1 loss : 0.191727 model2 loss : 0.260215
iteration 2618 : model1 loss : 0.168902 model2 loss : 0.246026
 26%|███████                    | 154/589 [1:02:50<3:15:33, 26.97s/it]iteration 2619 : model1 loss : 0.156315 model2 loss : 0.186463
iteration 2620 : model1 loss : 0.188297 model2 loss : 0.250679
iteration 2621 : model1 loss : 0.171857 model2 loss : 0.178189
iteration 2622 : model1 loss : 0.207699 model2 loss : 0.310905
iteration 2623 : model1 loss : 0.176244 model2 loss : 0.209729
iteration 2624 : model1 loss : 0.162925 model2 loss : 0.198757
iteration 2625 : model1 loss : 0.173410 model2 loss : 0.184727
iteration 2626 : model1 loss : 0.136270 model2 loss : 0.222715
iteration 2627 : model1 loss : 0.177377 model2 loss : 0.213281
iteration 2628 : model1 loss : 0.179039 model2 loss : 0.263262
iteration 2629 : model1 loss : 0.170424 model2 loss : 0.196914
iteration 2630 : model1 loss : 0.184281 model2 loss : 0.218633
iteration 2631 : model1 loss : 0.160067 model2 loss : 0.158015
iteration 2632 : model1 loss : 0.163888 model2 loss : 0.178614
iteration 2633 : model1 loss : 0.173972 model2 loss : 0.210040
iteration 2634 : model1 loss : 0.168927 model2 loss : 0.181554
iteration 2635 : model1 loss : 0.209288 model2 loss : 0.246215
 26%|███████                    | 155/589 [1:03:13<3:05:43, 25.68s/it]iteration 2636 : model1 loss : 0.189994 model2 loss : 0.208191
iteration 2637 : model1 loss : 0.146245 model2 loss : 0.170195
iteration 2638 : model1 loss : 0.157657 model2 loss : 0.189238
iteration 2639 : model1 loss : 0.197728 model2 loss : 0.175879
iteration 2640 : model1 loss : 0.151113 model2 loss : 0.175541
iteration 2641 : model1 loss : 0.186369 model2 loss : 0.189685
iteration 2642 : model1 loss : 0.191956 model2 loss : 0.203042
iteration 2643 : model1 loss : 0.220065 model2 loss : 0.179106
iteration 2644 : model1 loss : 0.183121 model2 loss : 0.170693
iteration 2645 : model1 loss : 0.195611 model2 loss : 0.187016
iteration 2646 : model1 loss : 0.155172 model2 loss : 0.175848
iteration 2647 : model1 loss : 0.212328 model2 loss : 0.217252
iteration 2648 : model1 loss : 0.176332 model2 loss : 0.234524
iteration 2649 : model1 loss : 0.174256 model2 loss : 0.176824
iteration 2650 : model1 loss : 0.160567 model2 loss : 0.148958
iteration 2651 : model1 loss : 0.193798 model2 loss : 0.209412
iteration 2652 : model1 loss : 0.152670 model2 loss : 0.169221
 26%|███████▏                   | 156/589 [1:03:36<2:59:06, 24.82s/it]iteration 2653 : model1 loss : 0.156460 model2 loss : 0.138167
iteration 2654 : model1 loss : 0.175127 model2 loss : 0.139094
iteration 2655 : model1 loss : 0.214810 model2 loss : 0.182621
iteration 2656 : model1 loss : 0.182210 model2 loss : 0.261341
iteration 2657 : model1 loss : 0.163938 model2 loss : 0.190322
iteration 2658 : model1 loss : 0.179283 model2 loss : 0.177776
iteration 2659 : model1 loss : 0.196133 model2 loss : 0.208323
iteration 2660 : model1 loss : 0.223931 model2 loss : 0.204606
iteration 2661 : model1 loss : 0.189863 model2 loss : 0.197462
iteration 2662 : model1 loss : 0.172251 model2 loss : 0.185866
iteration 2663 : model1 loss : 0.163226 model2 loss : 0.168047
iteration 2664 : model1 loss : 0.167140 model2 loss : 0.219899
iteration 2665 : model1 loss : 0.183463 model2 loss : 0.159471
iteration 2666 : model1 loss : 0.194778 model2 loss : 0.223848
iteration 2667 : model1 loss : 0.139494 model2 loss : 0.165472
iteration 2668 : model1 loss : 0.119550 model2 loss : 0.149258
iteration 2669 : model1 loss : 0.208693 model2 loss : 0.243569
 27%|███████▏                   | 157/589 [1:03:58<2:54:14, 24.20s/it]iteration 2670 : model1 loss : 0.197552 model2 loss : 0.224152
iteration 2671 : model1 loss : 0.154699 model2 loss : 0.190851
iteration 2672 : model1 loss : 0.175314 model2 loss : 0.163937
iteration 2673 : model1 loss : 0.162421 model2 loss : 0.160042
iteration 2674 : model1 loss : 0.152714 model2 loss : 0.149105
iteration 2675 : model1 loss : 0.131646 model2 loss : 0.143499
iteration 2676 : model1 loss : 0.183588 model2 loss : 0.246573
iteration 2677 : model1 loss : 0.184187 model2 loss : 0.199154
iteration 2678 : model1 loss : 0.200467 model2 loss : 0.189137
iteration 2679 : model1 loss : 0.176748 model2 loss : 0.217370
iteration 2680 : model1 loss : 0.173292 model2 loss : 0.232652
iteration 2681 : model1 loss : 0.155563 model2 loss : 0.163459
iteration 2682 : model1 loss : 0.157780 model2 loss : 0.177747
iteration 2683 : model1 loss : 0.187872 model2 loss : 0.168951
iteration 2684 : model1 loss : 0.164807 model2 loss : 0.159060
iteration 2685 : model1 loss : 0.176019 model2 loss : 0.276074
iteration 2686 : model1 loss : 0.192639 model2 loss : 0.174360
 27%|███████▏                   | 158/589 [1:04:21<2:50:25, 23.72s/it]iteration 2687 : model1 loss : 0.173778 model2 loss : 0.158195
iteration 2688 : model1 loss : 0.204312 model2 loss : 0.183279
iteration 2689 : model1 loss : 0.200698 model2 loss : 0.175639
iteration 2690 : model1 loss : 0.176506 model2 loss : 0.202059
iteration 2691 : model1 loss : 0.141209 model2 loss : 0.200410
iteration 2692 : model1 loss : 0.186593 model2 loss : 0.187313
iteration 2693 : model1 loss : 0.157585 model2 loss : 0.157916
iteration 2694 : model1 loss : 0.179823 model2 loss : 0.205888
iteration 2695 : model1 loss : 0.140754 model2 loss : 0.190137
iteration 2696 : model1 loss : 0.162942 model2 loss : 0.181046
iteration 2697 : model1 loss : 0.287533 model2 loss : 0.250332
iteration 2698 : model1 loss : 0.193808 model2 loss : 0.220797
iteration 2699 : model1 loss : 0.168582 model2 loss : 0.147882
iteration 2700 : model1 loss : 0.165639 model2 loss : 0.160915
iteration 2701 : model1 loss : 0.173841 model2 loss : 0.218940
iteration 2702 : model1 loss : 0.201028 model2 loss : 0.231922
iteration 2703 : model1 loss : 0.160805 model2 loss : 0.207557
 27%|███████▎                   | 159/589 [1:04:44<2:48:03, 23.45s/it]iteration 2704 : model1 loss : 0.162932 model2 loss : 0.163815
iteration 2705 : model1 loss : 0.137669 model2 loss : 0.136715
iteration 2706 : model1 loss : 0.168951 model2 loss : 0.184965
iteration 2707 : model1 loss : 0.181130 model2 loss : 0.183336
iteration 2708 : model1 loss : 0.184199 model2 loss : 0.173587
iteration 2709 : model1 loss : 0.157571 model2 loss : 0.210071
iteration 2710 : model1 loss : 0.199963 model2 loss : 0.164836
iteration 2711 : model1 loss : 0.207224 model2 loss : 0.253301
iteration 2712 : model1 loss : 0.146428 model2 loss : 0.146189
iteration 2713 : model1 loss : 0.164125 model2 loss : 0.174929
iteration 2714 : model1 loss : 0.132091 model2 loss : 0.159417
iteration 2715 : model1 loss : 0.164817 model2 loss : 0.181357
iteration 2716 : model1 loss : 0.213665 model2 loss : 0.142251
iteration 2717 : model1 loss : 0.193418 model2 loss : 0.210005
iteration 2718 : model1 loss : 0.197355 model2 loss : 0.198404
iteration 2719 : model1 loss : 0.206398 model2 loss : 0.179953
iteration 2720 : model1 loss : 0.237988 model2 loss : 0.231822
 27%|███████▎                   | 160/589 [1:05:06<2:45:53, 23.20s/it]iteration 2721 : model1 loss : 0.199382 model2 loss : 0.167926
iteration 2722 : model1 loss : 0.230637 model2 loss : 0.222175
iteration 2723 : model1 loss : 0.182053 model2 loss : 0.129176
iteration 2724 : model1 loss : 0.182648 model2 loss : 0.153524
iteration 2725 : model1 loss : 0.138862 model2 loss : 0.160647
iteration 2726 : model1 loss : 0.159939 model2 loss : 0.194396
iteration 2727 : model1 loss : 0.160871 model2 loss : 0.152576
iteration 2728 : model1 loss : 0.171317 model2 loss : 0.132335
iteration 2729 : model1 loss : 0.176827 model2 loss : 0.144630
iteration 2730 : model1 loss : 0.226651 model2 loss : 0.175808
iteration 2731 : model1 loss : 0.197720 model2 loss : 0.229964
iteration 2732 : model1 loss : 0.178904 model2 loss : 0.201548
iteration 2733 : model1 loss : 0.154031 model2 loss : 0.158827
iteration 2734 : model1 loss : 0.232129 model2 loss : 0.177026
iteration 2735 : model1 loss : 0.171790 model2 loss : 0.131072
iteration 2736 : model1 loss : 0.145348 model2 loss : 0.155957
iteration 2737 : model1 loss : 0.218086 model2 loss : 0.253297
 27%|███████▍                   | 161/589 [1:05:29<2:44:22, 23.04s/it]iteration 2738 : model1 loss : 0.193853 model2 loss : 0.177515
iteration 2739 : model1 loss : 0.205011 model2 loss : 0.187381
iteration 2740 : model1 loss : 0.166750 model2 loss : 0.207626
iteration 2741 : model1 loss : 0.159857 model2 loss : 0.162011
iteration 2742 : model1 loss : 0.183477 model2 loss : 0.219067
iteration 2743 : model1 loss : 0.159171 model2 loss : 0.137705
iteration 2744 : model1 loss : 0.187810 model2 loss : 0.167458
iteration 2745 : model1 loss : 0.146817 model2 loss : 0.170580
iteration 2746 : model1 loss : 0.151212 model2 loss : 0.148642
iteration 2747 : model1 loss : 0.138334 model2 loss : 0.131788
iteration 2748 : model1 loss : 0.189270 model2 loss : 0.160823
iteration 2749 : model1 loss : 0.219042 model2 loss : 0.213007
iteration 2750 : model1 loss : 0.244816 model2 loss : 0.213465
iteration 2751 : model1 loss : 0.188864 model2 loss : 0.257973
iteration 2752 : model1 loss : 0.145822 model2 loss : 0.149305
iteration 2753 : model1 loss : 0.168501 model2 loss : 0.190035
iteration 2754 : model1 loss : 0.161101 model2 loss : 0.160450
 28%|███████▍                   | 162/589 [1:05:52<2:43:26, 22.97s/it]iteration 2755 : model1 loss : 0.225241 model2 loss : 0.196452
iteration 2756 : model1 loss : 0.200626 model2 loss : 0.220088
iteration 2757 : model1 loss : 0.180563 model2 loss : 0.175663
iteration 2758 : model1 loss : 0.144469 model2 loss : 0.160206
iteration 2759 : model1 loss : 0.158616 model2 loss : 0.138391
iteration 2760 : model1 loss : 0.184019 model2 loss : 0.198119
iteration 2761 : model1 loss : 0.182787 model2 loss : 0.222288
iteration 2762 : model1 loss : 0.148763 model2 loss : 0.158438
iteration 2763 : model1 loss : 0.155444 model2 loss : 0.195250
iteration 2764 : model1 loss : 0.190146 model2 loss : 0.175771
iteration 2765 : model1 loss : 0.156252 model2 loss : 0.167833
iteration 2766 : model1 loss : 0.203539 model2 loss : 0.166986
iteration 2767 : model1 loss : 0.205741 model2 loss : 0.158232
iteration 2768 : model1 loss : 0.153207 model2 loss : 0.166817
iteration 2769 : model1 loss : 0.152542 model2 loss : 0.165422
iteration 2770 : model1 loss : 0.165854 model2 loss : 0.195447
iteration 2771 : model1 loss : 0.132034 model2 loss : 0.124366
Traceback (most recent call last):
  File "/ext3/miniconda3/envs/ssl/lib/python3.9/multiprocessing/queues.py", line 251, in _feed
    send_bytes(obj)
  File "/ext3/miniconda3/envs/ssl/lib/python3.9/multiprocessing/connection.py", line 205, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/ext3/miniconda3/envs/ssl/lib/python3.9/multiprocessing/connection.py", line 416, in _send_bytes
    self._send(header + buf)
  File "/ext3/miniconda3/envs/ssl/lib/python3.9/multiprocessing/connection.py", line 373, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
 28%|███████▍                   | 163/589 [1:06:15<2:42:41, 22.92s/it]iteration 2772 : model1 loss : 0.224763 model2 loss : 0.247775
iteration 2773 : model1 loss : 0.159119 model2 loss : 0.138102
iteration 2774 : model1 loss : 0.248821 model2 loss : 0.261596
iteration 2775 : model1 loss : 0.164637 model2 loss : 0.175417
iteration 2776 : model1 loss : 0.176854 model2 loss : 0.217987
iteration 2777 : model1 loss : 0.201191 model2 loss : 0.195315
iteration 2778 : model1 loss : 0.162577 model2 loss : 0.182180
iteration 2779 : model1 loss : 0.153680 model2 loss : 0.156629
iteration 2780 : model1 loss : 0.139671 model2 loss : 0.150272
iteration 2781 : model1 loss : 0.189360 model2 loss : 0.176789
iteration 2782 : model1 loss : 0.177421 model2 loss : 0.169306
iteration 2783 : model1 loss : 0.155535 model2 loss : 0.153634
iteration 2784 : model1 loss : 0.151854 model2 loss : 0.167031
iteration 2785 : model1 loss : 0.222959 model2 loss : 0.213066
iteration 2786 : model1 loss : 0.158525 model2 loss : 0.169437
iteration 2787 : model1 loss : 0.204691 model2 loss : 0.168604
iteration 2788 : model1 loss : 0.154213 model2 loss : 0.144136
 28%|███████▌                   | 164/589 [1:06:37<2:41:49, 22.85s/it]iteration 2789 : model1 loss : 0.170227 model2 loss : 0.163815
iteration 2790 : model1 loss : 0.170369 model2 loss : 0.192654
iteration 2791 : model1 loss : 0.186542 model2 loss : 0.152379
iteration 2792 : model1 loss : 0.310356 model2 loss : 0.263938
iteration 2793 : model1 loss : 0.170121 model2 loss : 0.158591
iteration 2794 : model1 loss : 0.197636 model2 loss : 0.155076
iteration 2795 : model1 loss : 0.184589 model2 loss : 0.172169
iteration 2796 : model1 loss : 0.169010 model2 loss : 0.218047
iteration 2797 : model1 loss : 0.191644 model2 loss : 0.189730
iteration 2798 : model1 loss : 0.189863 model2 loss : 0.175652
iteration 2799 : model1 loss : 0.157772 model2 loss : 0.173764
iteration 2800 : model1 loss : 0.189385 model2 loss : 0.187830
iteration 2800 : model1_mean_dice : 0.665211 model1_mean_hd95 : 84.832925 model1_mean_iou : 0.535919
iteration 2800 : model2_mean_dice : 0.745475 model2_mean_hd95 : 71.676549 model2_mean_iou : 0.632908
iteration 2801 : model1 loss : 0.163543 model2 loss : 0.163811
iteration 2802 : model1 loss : 0.188613 model2 loss : 0.224296
iteration 2803 : model1 loss : 0.151900 model2 loss : 0.211970
iteration 2804 : model1 loss : 0.152284 model2 loss : 0.160507
iteration 2805 : model1 loss : 0.168283 model2 loss : 0.152050
 28%|███████▌                   | 165/589 [1:07:20<3:23:54, 28.86s/it]iteration 2806 : model1 loss : 0.177013 model2 loss : 0.147326
iteration 2807 : model1 loss : 0.200702 model2 loss : 0.225078
iteration 2808 : model1 loss : 0.190576 model2 loss : 0.190837
iteration 2809 : model1 loss : 0.172699 model2 loss : 0.130962
iteration 2810 : model1 loss : 0.159613 model2 loss : 0.175717
iteration 2811 : model1 loss : 0.165709 model2 loss : 0.178855
iteration 2812 : model1 loss : 0.249805 model2 loss : 0.327413
iteration 2813 : model1 loss : 0.199498 model2 loss : 0.165603
iteration 2814 : model1 loss : 0.168275 model2 loss : 0.155154
iteration 2815 : model1 loss : 0.153207 model2 loss : 0.156063
iteration 2816 : model1 loss : 0.179446 model2 loss : 0.197905
iteration 2817 : model1 loss : 0.130740 model2 loss : 0.133626
iteration 2818 : model1 loss : 0.177445 model2 loss : 0.194239
iteration 2819 : model1 loss : 0.150380 model2 loss : 0.119202
iteration 2820 : model1 loss : 0.174685 model2 loss : 0.215332
iteration 2821 : model1 loss : 0.194540 model2 loss : 0.177703
iteration 2822 : model1 loss : 0.157343 model2 loss : 0.141792
 28%|███████▌                   | 166/589 [1:07:43<3:10:24, 27.01s/it]iteration 2823 : model1 loss : 0.197534 model2 loss : 0.204069
iteration 2824 : model1 loss : 0.154904 model2 loss : 0.176122
iteration 2825 : model1 loss : 0.164239 model2 loss : 0.249679
iteration 2826 : model1 loss : 0.170259 model2 loss : 0.151506
iteration 2827 : model1 loss : 0.165880 model2 loss : 0.160929
iteration 2828 : model1 loss : 0.167974 model2 loss : 0.184250
iteration 2829 : model1 loss : 0.164625 model2 loss : 0.145737
iteration 2830 : model1 loss : 0.172444 model2 loss : 0.197164
iteration 2831 : model1 loss : 0.165737 model2 loss : 0.204471
iteration 2832 : model1 loss : 0.143802 model2 loss : 0.140732
iteration 2833 : model1 loss : 0.170841 model2 loss : 0.174562
iteration 2834 : model1 loss : 0.175498 model2 loss : 0.167958
iteration 2835 : model1 loss : 0.147755 model2 loss : 0.159899
iteration 2836 : model1 loss : 0.229143 model2 loss : 0.181248
iteration 2837 : model1 loss : 0.180916 model2 loss : 0.175643
iteration 2838 : model1 loss : 0.162245 model2 loss : 0.176138
iteration 2839 : model1 loss : 0.188289 model2 loss : 0.166457
 28%|███████▋                   | 167/589 [1:08:06<3:00:51, 25.71s/it]iteration 2840 : model1 loss : 0.158683 model2 loss : 0.161230
iteration 2841 : model1 loss : 0.204007 model2 loss : 0.221239
iteration 2842 : model1 loss : 0.170729 model2 loss : 0.196997
iteration 2843 : model1 loss : 0.194421 model2 loss : 0.222288
iteration 2844 : model1 loss : 0.166076 model2 loss : 0.134940
iteration 2845 : model1 loss : 0.148333 model2 loss : 0.130216
iteration 2846 : model1 loss : 0.178825 model2 loss : 0.187835
iteration 2847 : model1 loss : 0.221129 model2 loss : 0.191748
iteration 2848 : model1 loss : 0.175257 model2 loss : 0.167673
iteration 2849 : model1 loss : 0.146598 model2 loss : 0.147739
iteration 2850 : model1 loss : 0.200149 model2 loss : 0.176227
iteration 2851 : model1 loss : 0.196202 model2 loss : 0.183676
iteration 2852 : model1 loss : 0.176037 model2 loss : 0.194735
iteration 2853 : model1 loss : 0.202558 model2 loss : 0.186977
iteration 2854 : model1 loss : 0.151265 model2 loss : 0.119853
iteration 2855 : model1 loss : 0.128967 model2 loss : 0.202603
iteration 2856 : model1 loss : 0.181574 model2 loss : 0.145007
 29%|███████▋                   | 168/589 [1:08:28<2:54:16, 24.84s/it]iteration 2857 : model1 loss : 0.175400 model2 loss : 0.211023
iteration 2858 : model1 loss : 0.164593 model2 loss : 0.206545
iteration 2859 : model1 loss : 0.167352 model2 loss : 0.131653
iteration 2860 : model1 loss : 0.173247 model2 loss : 0.150296
iteration 2861 : model1 loss : 0.157909 model2 loss : 0.148005
iteration 2862 : model1 loss : 0.176870 model2 loss : 0.128999
iteration 2863 : model1 loss : 0.205596 model2 loss : 0.226821
iteration 2864 : model1 loss : 0.150905 model2 loss : 0.107109
iteration 2865 : model1 loss : 0.210621 model2 loss : 0.165808
iteration 2866 : model1 loss : 0.150330 model2 loss : 0.174950
iteration 2867 : model1 loss : 0.169621 model2 loss : 0.156703
iteration 2868 : model1 loss : 0.182120 model2 loss : 0.175723
iteration 2869 : model1 loss : 0.191321 model2 loss : 0.198288
iteration 2870 : model1 loss : 0.214038 model2 loss : 0.198428
iteration 2871 : model1 loss : 0.180805 model2 loss : 0.164660
iteration 2872 : model1 loss : 0.153217 model2 loss : 0.175278
iteration 2873 : model1 loss : 0.241061 model2 loss : 0.190317
 29%|███████▋                   | 169/589 [1:08:51<2:49:25, 24.20s/it]iteration 2874 : model1 loss : 0.150711 model2 loss : 0.221454
iteration 2875 : model1 loss : 0.154363 model2 loss : 0.132477
iteration 2876 : model1 loss : 0.184661 model2 loss : 0.174980
iteration 2877 : model1 loss : 0.142881 model2 loss : 0.152950
iteration 2878 : model1 loss : 0.143894 model2 loss : 0.157658
iteration 2879 : model1 loss : 0.129697 model2 loss : 0.169074
iteration 2880 : model1 loss : 0.190493 model2 loss : 0.279283
iteration 2881 : model1 loss : 0.163271 model2 loss : 0.168356
iteration 2882 : model1 loss : 0.132436 model2 loss : 0.137535
iteration 2883 : model1 loss : 0.211786 model2 loss : 0.233745
iteration 2884 : model1 loss : 0.186023 model2 loss : 0.163902
iteration 2885 : model1 loss : 0.230737 model2 loss : 0.188749
iteration 2886 : model1 loss : 0.173500 model2 loss : 0.174126
iteration 2887 : model1 loss : 0.167539 model2 loss : 0.172758
iteration 2888 : model1 loss : 0.197275 model2 loss : 0.226891
iteration 2889 : model1 loss : 0.176025 model2 loss : 0.146443
iteration 2890 : model1 loss : 0.196536 model2 loss : 0.229502
 29%|███████▊                   | 170/589 [1:09:14<2:45:54, 23.76s/it]iteration 2891 : model1 loss : 0.169637 model2 loss : 0.179367
iteration 2892 : model1 loss : 0.165141 model2 loss : 0.203774
iteration 2893 : model1 loss : 0.151873 model2 loss : 0.121236
iteration 2894 : model1 loss : 0.159718 model2 loss : 0.146537
iteration 2895 : model1 loss : 0.146854 model2 loss : 0.160711
iteration 2896 : model1 loss : 0.202855 model2 loss : 0.214331
iteration 2897 : model1 loss : 0.150306 model2 loss : 0.140667
iteration 2898 : model1 loss : 0.186448 model2 loss : 0.140986
iteration 2899 : model1 loss : 0.156282 model2 loss : 0.157598
iteration 2900 : model1 loss : 0.180286 model2 loss : 0.155273
iteration 2901 : model1 loss : 0.189355 model2 loss : 0.180653
iteration 2902 : model1 loss : 0.242884 model2 loss : 0.182824
iteration 2903 : model1 loss : 0.196631 model2 loss : 0.205246
iteration 2904 : model1 loss : 0.182014 model2 loss : 0.202015
iteration 2905 : model1 loss : 0.181742 model2 loss : 0.205634
iteration 2906 : model1 loss : 0.159038 model2 loss : 0.158148
iteration 2907 : model1 loss : 0.163636 model2 loss : 0.137551
 29%|███████▊                   | 171/589 [1:09:37<2:43:36, 23.48s/it]iteration 2908 : model1 loss : 0.138655 model2 loss : 0.165472
iteration 2909 : model1 loss : 0.209029 model2 loss : 0.179372
iteration 2910 : model1 loss : 0.177970 model2 loss : 0.159526
iteration 2911 : model1 loss : 0.212549 model2 loss : 0.237222
iteration 2912 : model1 loss : 0.146350 model2 loss : 0.137820
iteration 2913 : model1 loss : 0.192384 model2 loss : 0.194956
iteration 2914 : model1 loss : 0.197233 model2 loss : 0.179824
iteration 2915 : model1 loss : 0.158577 model2 loss : 0.163656
iteration 2916 : model1 loss : 0.121780 model2 loss : 0.121019
iteration 2917 : model1 loss : 0.166150 model2 loss : 0.196274
iteration 2918 : model1 loss : 0.174952 model2 loss : 0.195046
iteration 2919 : model1 loss : 0.211541 model2 loss : 0.182144
iteration 2920 : model1 loss : 0.142385 model2 loss : 0.145753
iteration 2921 : model1 loss : 0.203710 model2 loss : 0.164092
iteration 2922 : model1 loss : 0.162311 model2 loss : 0.154320
iteration 2923 : model1 loss : 0.169112 model2 loss : 0.172883
iteration 2924 : model1 loss : 0.196223 model2 loss : 0.217758
 29%|███████▉                   | 172/589 [1:09:59<2:41:35, 23.25s/it]iteration 2925 : model1 loss : 0.163594 model2 loss : 0.142814
iteration 2926 : model1 loss : 0.241028 model2 loss : 0.268227
iteration 2927 : model1 loss : 0.168529 model2 loss : 0.176464
iteration 2928 : model1 loss : 0.178173 model2 loss : 0.233866
iteration 2929 : model1 loss : 0.176658 model2 loss : 0.172980
iteration 2930 : model1 loss : 0.181728 model2 loss : 0.168810
iteration 2931 : model1 loss : 0.154928 model2 loss : 0.180379
iteration 2932 : model1 loss : 0.188646 model2 loss : 0.169959
iteration 2933 : model1 loss : 0.172593 model2 loss : 0.163728
iteration 2934 : model1 loss : 0.144898 model2 loss : 0.176465
iteration 2935 : model1 loss : 0.166452 model2 loss : 0.212919
iteration 2936 : model1 loss : 0.186225 model2 loss : 0.213084
iteration 2937 : model1 loss : 0.146135 model2 loss : 0.181351
iteration 2938 : model1 loss : 0.161158 model2 loss : 0.121442
iteration 2939 : model1 loss : 0.164846 model2 loss : 0.163336
iteration 2940 : model1 loss : 0.161528 model2 loss : 0.139150
iteration 2941 : model1 loss : 0.164050 model2 loss : 0.190476
 29%|███████▉                   | 173/589 [1:10:22<2:40:05, 23.09s/it]iteration 2942 : model1 loss : 0.182091 model2 loss : 0.143932
iteration 2943 : model1 loss : 0.139367 model2 loss : 0.153635
iteration 2944 : model1 loss : 0.175623 model2 loss : 0.169474
iteration 2945 : model1 loss : 0.138027 model2 loss : 0.119246
iteration 2946 : model1 loss : 0.141309 model2 loss : 0.153524
iteration 2947 : model1 loss : 0.196606 model2 loss : 0.167991
iteration 2948 : model1 loss : 0.157782 model2 loss : 0.185096
iteration 2949 : model1 loss : 0.181909 model2 loss : 0.184206
iteration 2950 : model1 loss : 0.201998 model2 loss : 0.182083
iteration 2951 : model1 loss : 0.190165 model2 loss : 0.213061
iteration 2952 : model1 loss : 0.171911 model2 loss : 0.185590
iteration 2953 : model1 loss : 0.180108 model2 loss : 0.180897
iteration 2954 : model1 loss : 0.180662 model2 loss : 0.200560
iteration 2955 : model1 loss : 0.184085 model2 loss : 0.146417
iteration 2956 : model1 loss : 0.163571 model2 loss : 0.183046
iteration 2957 : model1 loss : 0.150508 model2 loss : 0.157641
iteration 2958 : model1 loss : 0.213472 model2 loss : 0.205226
 30%|███████▉                   | 174/589 [1:10:45<2:39:10, 23.01s/it]iteration 2959 : model1 loss : 0.148317 model2 loss : 0.168274
iteration 2960 : model1 loss : 0.163465 model2 loss : 0.193006
iteration 2961 : model1 loss : 0.193260 model2 loss : 0.198499
iteration 2962 : model1 loss : 0.189892 model2 loss : 0.187361
iteration 2963 : model1 loss : 0.139367 model2 loss : 0.151392
iteration 2964 : model1 loss : 0.147530 model2 loss : 0.187260
iteration 2965 : model1 loss : 0.225097 model2 loss : 0.232155
iteration 2966 : model1 loss : 0.156755 model2 loss : 0.170957
iteration 2967 : model1 loss : 0.178320 model2 loss : 0.182093
iteration 2968 : model1 loss : 0.187423 model2 loss : 0.152210
iteration 2969 : model1 loss : 0.205165 model2 loss : 0.175162
iteration 2970 : model1 loss : 0.210628 model2 loss : 0.161050
iteration 2971 : model1 loss : 0.155607 model2 loss : 0.153444
iteration 2972 : model1 loss : 0.145434 model2 loss : 0.213353
iteration 2973 : model1 loss : 0.176744 model2 loss : 0.179921
iteration 2974 : model1 loss : 0.165414 model2 loss : 0.208933
iteration 2975 : model1 loss : 0.224492 model2 loss : 0.236291
 30%|████████                   | 175/589 [1:11:08<2:38:17, 22.94s/it]iteration 2976 : model1 loss : 0.166071 model2 loss : 0.167443
iteration 2977 : model1 loss : 0.154443 model2 loss : 0.202002
iteration 2978 : model1 loss : 0.196225 model2 loss : 0.266685
iteration 2979 : model1 loss : 0.144412 model2 loss : 0.139116
iteration 2980 : model1 loss : 0.177528 model2 loss : 0.178527
iteration 2981 : model1 loss : 0.171735 model2 loss : 0.168312
iteration 2982 : model1 loss : 0.183748 model2 loss : 0.182675
iteration 2983 : model1 loss : 0.172037 model2 loss : 0.180873
iteration 2984 : model1 loss : 0.199571 model2 loss : 0.212302
iteration 2985 : model1 loss : 0.155851 model2 loss : 0.197682
iteration 2986 : model1 loss : 0.175459 model2 loss : 0.164271
iteration 2987 : model1 loss : 0.161883 model2 loss : 0.202376
iteration 2988 : model1 loss : 0.248475 model2 loss : 0.250978
iteration 2989 : model1 loss : 0.125687 model2 loss : 0.143144
iteration 2990 : model1 loss : 0.218209 model2 loss : 0.177416
iteration 2991 : model1 loss : 0.158633 model2 loss : 0.195506
iteration 2992 : model1 loss : 0.201006 model2 loss : 0.203297
 30%|████████                   | 176/589 [1:11:30<2:37:18, 22.85s/it]iteration 2993 : model1 loss : 0.209811 model2 loss : 0.183862
iteration 2994 : model1 loss : 0.168456 model2 loss : 0.133909
iteration 2995 : model1 loss : 0.156109 model2 loss : 0.181229
iteration 2996 : model1 loss : 0.169835 model2 loss : 0.158423
iteration 2997 : model1 loss : 0.155915 model2 loss : 0.180493
iteration 2998 : model1 loss : 0.140775 model2 loss : 0.172682
iteration 2999 : model1 loss : 0.180145 model2 loss : 0.187538
iteration 3000 : model1 loss : 0.153537 model2 loss : 0.194554
iteration 3000 : model1_mean_dice : 0.633031 model1_mean_hd95 : 85.128848 model1_mean_iou : 0.508751
iteration 3000 : model2_mean_dice : 0.784786 model2_mean_hd95 : 65.550748 model2_mean_iou : 0.676032
save model1 to ../model/Dermofit/Cross_Teaching_Between_CNN_Transformer_7/unet/model1_iter_3000.pth
save model2 to ../model/Dermofit/Cross_Teaching_Between_CNN_Transformer_7/unet/model2_iter_3000.pth
iteration 3001 : model1 loss : 0.142164 model2 loss : 0.156482
iteration 3002 : model1 loss : 0.197363 model2 loss : 0.223577
iteration 3003 : model1 loss : 0.178514 model2 loss : 0.220208
iteration 3004 : model1 loss : 0.209536 model2 loss : 0.177440
iteration 3005 : model1 loss : 0.199979 model2 loss : 0.197051
iteration 3006 : model1 loss : 0.149582 model2 loss : 0.175784
iteration 3007 : model1 loss : 0.172325 model2 loss : 0.240950
iteration 3008 : model1 loss : 0.163756 model2 loss : 0.201957
iteration 3009 : model1 loss : 0.166734 model2 loss : 0.158158
Traceback (most recent call last):
  File "/ext3/miniconda3/envs/ssl/lib/python3.9/multiprocessing/queues.py", line 251, in _feed
    send_bytes(obj)
  File "/ext3/miniconda3/envs/ssl/lib/python3.9/multiprocessing/connection.py", line 205, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/ext3/miniconda3/envs/ssl/lib/python3.9/multiprocessing/connection.py", line 416, in _send_bytes
    self._send(header + buf)
  File "/ext3/miniconda3/envs/ssl/lib/python3.9/multiprocessing/connection.py", line 373, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
 30%|████████                   | 177/589 [1:12:13<3:18:16, 28.87s/it]iteration 3010 : model1 loss : 0.195229 model2 loss : 0.172787
iteration 3011 : model1 loss : 0.173136 model2 loss : 0.167688
iteration 3012 : model1 loss : 0.147622 model2 loss : 0.214899
iteration 3013 : model1 loss : 0.162377 model2 loss : 0.211339
iteration 3014 : model1 loss : 0.145200 model2 loss : 0.154022
iteration 3015 : model1 loss : 0.153480 model2 loss : 0.152713
iteration 3016 : model1 loss : 0.173759 model2 loss : 0.187894
iteration 3017 : model1 loss : 0.161311 model2 loss : 0.196681
iteration 3018 : model1 loss : 0.153593 model2 loss : 0.145098
iteration 3019 : model1 loss : 0.244661 model2 loss : 0.202522
iteration 3020 : model1 loss : 0.139269 model2 loss : 0.110857
iteration 3021 : model1 loss : 0.173602 model2 loss : 0.231489
iteration 3022 : model1 loss : 0.200174 model2 loss : 0.194489
iteration 3023 : model1 loss : 0.214029 model2 loss : 0.171570
iteration 3024 : model1 loss : 0.160663 model2 loss : 0.145729
iteration 3025 : model1 loss : 0.196553 model2 loss : 0.158410
iteration 3026 : model1 loss : 0.205672 model2 loss : 0.145756
 30%|████████▏                  | 178/589 [1:12:36<3:05:03, 27.02s/it]iteration 3027 : model1 loss : 0.142374 model2 loss : 0.201000
iteration 3028 : model1 loss : 0.156433 model2 loss : 0.187463
iteration 3029 : model1 loss : 0.191857 model2 loss : 0.228935
iteration 3030 : model1 loss : 0.180366 model2 loss : 0.164787
iteration 3031 : model1 loss : 0.169680 model2 loss : 0.162120
iteration 3032 : model1 loss : 0.133440 model2 loss : 0.165790
iteration 3033 : model1 loss : 0.185725 model2 loss : 0.160007
iteration 3034 : model1 loss : 0.216033 model2 loss : 0.249497
iteration 3035 : model1 loss : 0.172736 model2 loss : 0.178199
iteration 3036 : model1 loss : 0.165988 model2 loss : 0.185989
iteration 3037 : model1 loss : 0.187135 model2 loss : 0.211726
iteration 3038 : model1 loss : 0.184368 model2 loss : 0.257109
iteration 3039 : model1 loss : 0.187069 model2 loss : 0.192251
iteration 3040 : model1 loss : 0.157286 model2 loss : 0.173625
iteration 3041 : model1 loss : 0.144574 model2 loss : 0.141215
iteration 3042 : model1 loss : 0.189616 model2 loss : 0.207159
iteration 3043 : model1 loss : 0.156820 model2 loss : 0.213049
 30%|████████▏                  | 179/589 [1:12:59<2:55:41, 25.71s/it]iteration 3044 : model1 loss : 0.198383 model2 loss : 0.228004
iteration 3045 : model1 loss : 0.204758 model2 loss : 0.228020
iteration 3046 : model1 loss : 0.188632 model2 loss : 0.180426
iteration 3047 : model1 loss : 0.133975 model2 loss : 0.157509
iteration 3048 : model1 loss : 0.173488 model2 loss : 0.194493
iteration 3049 : model1 loss : 0.147272 model2 loss : 0.158273
iteration 3050 : model1 loss : 0.176230 model2 loss : 0.204239
iteration 3051 : model1 loss : 0.153224 model2 loss : 0.205478
iteration 3052 : model1 loss : 0.184653 model2 loss : 0.178100
iteration 3053 : model1 loss : 0.149715 model2 loss : 0.177344
iteration 3054 : model1 loss : 0.187316 model2 loss : 0.203315
iteration 3055 : model1 loss : 0.151996 model2 loss : 0.180833
iteration 3056 : model1 loss : 0.187455 model2 loss : 0.211785
iteration 3057 : model1 loss : 0.164525 model2 loss : 0.170912
iteration 3058 : model1 loss : 0.178435 model2 loss : 0.174773
iteration 3059 : model1 loss : 0.160296 model2 loss : 0.168436
iteration 3060 : model1 loss : 0.190372 model2 loss : 0.266383
 31%|████████▎                  | 180/589 [1:13:21<2:49:16, 24.83s/it]iteration 3061 : model1 loss : 0.175344 model2 loss : 0.162126
iteration 3062 : model1 loss : 0.188221 model2 loss : 0.194064
iteration 3063 : model1 loss : 0.151664 model2 loss : 0.176092
iteration 3064 : model1 loss : 0.152692 model2 loss : 0.194716
iteration 3065 : model1 loss : 0.168153 model2 loss : 0.147942
iteration 3066 : model1 loss : 0.169398 model2 loss : 0.145796
iteration 3067 : model1 loss : 0.141397 model2 loss : 0.169732
iteration 3068 : model1 loss : 0.202019 model2 loss : 0.211748
iteration 3069 : model1 loss : 0.154609 model2 loss : 0.222881
iteration 3070 : model1 loss : 0.151394 model2 loss : 0.156219
iteration 3071 : model1 loss : 0.172573 model2 loss : 0.200721
iteration 3072 : model1 loss : 0.201203 model2 loss : 0.171983
iteration 3073 : model1 loss : 0.133196 model2 loss : 0.152769
iteration 3074 : model1 loss : 0.171874 model2 loss : 0.212026
iteration 3075 : model1 loss : 0.192844 model2 loss : 0.220817
iteration 3076 : model1 loss : 0.174823 model2 loss : 0.147202
iteration 3077 : model1 loss : 0.133599 model2 loss : 0.152590
 31%|████████▎                  | 181/589 [1:13:44<2:44:39, 24.21s/it]iteration 3078 : model1 loss : 0.179505 model2 loss : 0.156233
iteration 3079 : model1 loss : 0.196939 model2 loss : 0.244788
iteration 3080 : model1 loss : 0.135121 model2 loss : 0.149367
iteration 3081 : model1 loss : 0.179831 model2 loss : 0.228754
iteration 3082 : model1 loss : 0.126095 model2 loss : 0.129011
iteration 3083 : model1 loss : 0.166621 model2 loss : 0.179984
iteration 3084 : model1 loss : 0.204502 model2 loss : 0.206794
iteration 3085 : model1 loss : 0.180348 model2 loss : 0.152671
iteration 3086 : model1 loss : 0.175604 model2 loss : 0.139328
iteration 3087 : model1 loss : 0.210957 model2 loss : 0.196215
iteration 3088 : model1 loss : 0.187539 model2 loss : 0.162384
iteration 3089 : model1 loss : 0.199129 model2 loss : 0.220158
iteration 3090 : model1 loss : 0.143912 model2 loss : 0.140501
iteration 3091 : model1 loss : 0.164987 model2 loss : 0.214717
iteration 3092 : model1 loss : 0.169362 model2 loss : 0.232379
iteration 3093 : model1 loss : 0.176139 model2 loss : 0.174758
iteration 3094 : model1 loss : 0.168680 model2 loss : 0.173984
 31%|████████▎                  | 182/589 [1:14:07<2:41:15, 23.77s/it]iteration 3095 : model1 loss : 0.149214 model2 loss : 0.137800
iteration 3096 : model1 loss : 0.117066 model2 loss : 0.155153
iteration 3097 : model1 loss : 0.175119 model2 loss : 0.220322
iteration 3098 : model1 loss : 0.139336 model2 loss : 0.168373
iteration 3099 : model1 loss : 0.161239 model2 loss : 0.153945
iteration 3100 : model1 loss : 0.184920 model2 loss : 0.178952
iteration 3101 : model1 loss : 0.237135 model2 loss : 0.190229
iteration 3102 : model1 loss : 0.165155 model2 loss : 0.235628
iteration 3103 : model1 loss : 0.146138 model2 loss : 0.167775
iteration 3104 : model1 loss : 0.200711 model2 loss : 0.180264
iteration 3105 : model1 loss : 0.156675 model2 loss : 0.186366
iteration 3106 : model1 loss : 0.174116 model2 loss : 0.186323
iteration 3107 : model1 loss : 0.179751 model2 loss : 0.180400
iteration 3108 : model1 loss : 0.207307 model2 loss : 0.231929
iteration 3109 : model1 loss : 0.198888 model2 loss : 0.269272
iteration 3110 : model1 loss : 0.151687 model2 loss : 0.138068
iteration 3111 : model1 loss : 0.190410 model2 loss : 0.219786
 31%|████████▍                  | 183/589 [1:14:30<2:38:54, 23.49s/it]iteration 3112 : model1 loss : 0.220091 model2 loss : 0.234480
iteration 3113 : model1 loss : 0.160986 model2 loss : 0.194594
iteration 3114 : model1 loss : 0.146040 model2 loss : 0.168378
iteration 3115 : model1 loss : 0.198519 model2 loss : 0.176712
iteration 3116 : model1 loss : 0.147690 model2 loss : 0.181163
iteration 3117 : model1 loss : 0.146812 model2 loss : 0.147275
iteration 3118 : model1 loss : 0.180712 model2 loss : 0.203829
iteration 3119 : model1 loss : 0.161153 model2 loss : 0.156122
iteration 3120 : model1 loss : 0.151346 model2 loss : 0.153249
iteration 3121 : model1 loss : 0.153748 model2 loss : 0.226881
iteration 3122 : model1 loss : 0.153830 model2 loss : 0.156773
iteration 3123 : model1 loss : 0.206547 model2 loss : 0.206723
iteration 3124 : model1 loss : 0.138987 model2 loss : 0.117292
iteration 3125 : model1 loss : 0.169115 model2 loss : 0.183757
iteration 3126 : model1 loss : 0.207392 model2 loss : 0.197501
iteration 3127 : model1 loss : 0.177492 model2 loss : 0.214743
iteration 3128 : model1 loss : 0.180148 model2 loss : 0.198994
 31%|████████▍                  | 184/589 [1:14:52<2:36:55, 23.25s/it]iteration 3129 : model1 loss : 0.155186 model2 loss : 0.120522
iteration 3130 : model1 loss : 0.192359 model2 loss : 0.193771
iteration 3131 : model1 loss : 0.155128 model2 loss : 0.151322
iteration 3132 : model1 loss : 0.154170 model2 loss : 0.181846
iteration 3133 : model1 loss : 0.176445 model2 loss : 0.195893
iteration 3134 : model1 loss : 0.157322 model2 loss : 0.168749
iteration 3135 : model1 loss : 0.235501 model2 loss : 0.289342
iteration 3136 : model1 loss : 0.185478 model2 loss : 0.204918
iteration 3137 : model1 loss : 0.163184 model2 loss : 0.149884
iteration 3138 : model1 loss : 0.199178 model2 loss : 0.159085
iteration 3139 : model1 loss : 0.175484 model2 loss : 0.256708
iteration 3140 : model1 loss : 0.162173 model2 loss : 0.201869
iteration 3141 : model1 loss : 0.199842 model2 loss : 0.169171
iteration 3142 : model1 loss : 0.148898 model2 loss : 0.175307
iteration 3143 : model1 loss : 0.127449 model2 loss : 0.127897
iteration 3144 : model1 loss : 0.197382 model2 loss : 0.206732
iteration 3145 : model1 loss : 0.158029 model2 loss : 0.180280
 31%|████████▍                  | 185/589 [1:15:15<2:35:29, 23.09s/it]iteration 3146 : model1 loss : 0.170073 model2 loss : 0.141464
iteration 3147 : model1 loss : 0.151391 model2 loss : 0.145893
iteration 3148 : model1 loss : 0.181671 model2 loss : 0.178374
iteration 3149 : model1 loss : 0.199622 model2 loss : 0.200381
iteration 3150 : model1 loss : 0.172145 model2 loss : 0.185580
iteration 3151 : model1 loss : 0.177147 model2 loss : 0.170842
iteration 3152 : model1 loss : 0.174582 model2 loss : 0.203791
iteration 3153 : model1 loss : 0.205922 model2 loss : 0.181757
iteration 3154 : model1 loss : 0.142221 model2 loss : 0.229881
iteration 3155 : model1 loss : 0.168187 model2 loss : 0.169970
iteration 3156 : model1 loss : 0.181946 model2 loss : 0.152991
iteration 3157 : model1 loss : 0.156263 model2 loss : 0.178249
iteration 3158 : model1 loss : 0.186980 model2 loss : 0.180851
iteration 3159 : model1 loss : 0.157505 model2 loss : 0.177079
iteration 3160 : model1 loss : 0.179548 model2 loss : 0.146338
iteration 3161 : model1 loss : 0.152941 model2 loss : 0.209991
iteration 3162 : model1 loss : 0.140814 model2 loss : 0.178215
 32%|████████▌                  | 186/589 [1:15:38<2:34:28, 23.00s/it]iteration 3163 : model1 loss : 0.217900 model2 loss : 0.219808
iteration 3164 : model1 loss : 0.162185 model2 loss : 0.154911
iteration 3165 : model1 loss : 0.136851 model2 loss : 0.130269
iteration 3166 : model1 loss : 0.152323 model2 loss : 0.199720
iteration 3167 : model1 loss : 0.137340 model2 loss : 0.118531
iteration 3168 : model1 loss : 0.170793 model2 loss : 0.200465
iteration 3169 : model1 loss : 0.185095 model2 loss : 0.168773
iteration 3170 : model1 loss : 0.196717 model2 loss : 0.202504
iteration 3171 : model1 loss : 0.162775 model2 loss : 0.163781
iteration 3172 : model1 loss : 0.156326 model2 loss : 0.163173
iteration 3173 : model1 loss : 0.186665 model2 loss : 0.214390
iteration 3174 : model1 loss : 0.128846 model2 loss : 0.131020
iteration 3175 : model1 loss : 0.258678 model2 loss : 0.201993
iteration 3176 : model1 loss : 0.184126 model2 loss : 0.194092
iteration 3177 : model1 loss : 0.147284 model2 loss : 0.143776
iteration 3178 : model1 loss : 0.137465 model2 loss : 0.161005
iteration 3179 : model1 loss : 0.177223 model2 loss : 0.159496
 32%|████████▌                  | 187/589 [1:16:01<2:33:36, 22.93s/it]iteration 3180 : model1 loss : 0.196363 model2 loss : 0.160635
iteration 3181 : model1 loss : 0.221309 model2 loss : 0.203178
iteration 3182 : model1 loss : 0.218460 model2 loss : 0.190206
iteration 3183 : model1 loss : 0.120517 model2 loss : 0.120785
iteration 3184 : model1 loss : 0.129046 model2 loss : 0.150375
iteration 3185 : model1 loss : 0.215998 model2 loss : 0.183953
iteration 3186 : model1 loss : 0.153008 model2 loss : 0.128944
iteration 3187 : model1 loss : 0.158280 model2 loss : 0.182565
iteration 3188 : model1 loss : 0.212685 model2 loss : 0.224822
iteration 3189 : model1 loss : 0.213089 model2 loss : 0.207011
iteration 3190 : model1 loss : 0.201175 model2 loss : 0.219003
iteration 3191 : model1 loss : 0.187598 model2 loss : 0.168028
iteration 3192 : model1 loss : 0.187011 model2 loss : 0.206374
iteration 3193 : model1 loss : 0.176913 model2 loss : 0.150964
iteration 3194 : model1 loss : 0.157460 model2 loss : 0.156175
iteration 3195 : model1 loss : 0.168269 model2 loss : 0.191407
iteration 3196 : model1 loss : 0.132914 model2 loss : 0.120167
 32%|████████▌                  | 188/589 [1:16:23<2:32:47, 22.86s/it]iteration 3197 : model1 loss : 0.126945 model2 loss : 0.141210
iteration 3198 : model1 loss : 0.175786 model2 loss : 0.191723
iteration 3199 : model1 loss : 0.146154 model2 loss : 0.157508
iteration 3200 : model1 loss : 0.159178 model2 loss : 0.172493
iteration 3200 : model1_mean_dice : 0.656725 model1_mean_hd95 : 87.462215 model1_mean_iou : 0.532265
iteration 3200 : model2_mean_dice : 0.759232 model2_mean_hd95 : 70.062638 model2_mean_iou : 0.647730
iteration 3201 : model1 loss : 0.172966 model2 loss : 0.172663
iteration 3202 : model1 loss : 0.168919 model2 loss : 0.151279
iteration 3203 : model1 loss : 0.146855 model2 loss : 0.146350
iteration 3204 : model1 loss : 0.163738 model2 loss : 0.140616
iteration 3205 : model1 loss : 0.153086 model2 loss : 0.163151
iteration 3206 : model1 loss : 0.174335 model2 loss : 0.201250
iteration 3207 : model1 loss : 0.198705 model2 loss : 0.185711
iteration 3208 : model1 loss : 0.218726 model2 loss : 0.250414
iteration 3209 : model1 loss : 0.207583 model2 loss : 0.196672
iteration 3210 : model1 loss : 0.235891 model2 loss : 0.243271
iteration 3211 : model1 loss : 0.144921 model2 loss : 0.124321
iteration 3212 : model1 loss : 0.202069 model2 loss : 0.193090
iteration 3213 : model1 loss : 0.167848 model2 loss : 0.146651
 32%|████████▋                  | 189/589 [1:17:06<3:11:54, 28.79s/it]iteration 3214 : model1 loss : 0.165789 model2 loss : 0.156585
iteration 3215 : model1 loss : 0.161278 model2 loss : 0.186602
iteration 3216 : model1 loss : 0.197124 model2 loss : 0.183035
iteration 3217 : model1 loss : 0.169660 model2 loss : 0.166052
iteration 3218 : model1 loss : 0.183464 model2 loss : 0.173578
iteration 3219 : model1 loss : 0.178501 model2 loss : 0.206771
iteration 3220 : model1 loss : 0.160467 model2 loss : 0.180239
iteration 3221 : model1 loss : 0.165252 model2 loss : 0.171742
iteration 3222 : model1 loss : 0.175222 model2 loss : 0.141856
iteration 3223 : model1 loss : 0.154356 model2 loss : 0.240816
iteration 3224 : model1 loss : 0.148295 model2 loss : 0.118367
iteration 3225 : model1 loss : 0.193714 model2 loss : 0.194160
iteration 3226 : model1 loss : 0.157928 model2 loss : 0.134565
iteration 3227 : model1 loss : 0.207377 model2 loss : 0.147105
iteration 3228 : model1 loss : 0.175132 model2 loss : 0.147487
iteration 3229 : model1 loss : 0.165286 model2 loss : 0.146456
iteration 3230 : model1 loss : 0.160710 model2 loss : 0.135433
 32%|████████▋                  | 190/589 [1:17:29<2:59:19, 26.97s/it]iteration 3231 : model1 loss : 0.192347 model2 loss : 0.178738
iteration 3232 : model1 loss : 0.174106 model2 loss : 0.151382
iteration 3233 : model1 loss : 0.184581 model2 loss : 0.181794
iteration 3234 : model1 loss : 0.170924 model2 loss : 0.176475
iteration 3235 : model1 loss : 0.153955 model2 loss : 0.149648
iteration 3236 : model1 loss : 0.184631 model2 loss : 0.211532
iteration 3237 : model1 loss : 0.187963 model2 loss : 0.135870
iteration 3238 : model1 loss : 0.174897 model2 loss : 0.176721
iteration 3239 : model1 loss : 0.117901 model2 loss : 0.100989
iteration 3240 : model1 loss : 0.178041 model2 loss : 0.194377
iteration 3241 : model1 loss : 0.177956 model2 loss : 0.199667
iteration 3242 : model1 loss : 0.158716 model2 loss : 0.155965
iteration 3243 : model1 loss : 0.171440 model2 loss : 0.145734
iteration 3244 : model1 loss : 0.189825 model2 loss : 0.191302
iteration 3245 : model1 loss : 0.230169 model2 loss : 0.206291
iteration 3246 : model1 loss : 0.158788 model2 loss : 0.152213
iteration 3247 : model1 loss : 0.141402 model2 loss : 0.149579
 32%|████████▊                  | 191/589 [1:17:51<2:50:11, 25.66s/it]iteration 3248 : model1 loss : 0.172503 model2 loss : 0.184312
iteration 3249 : model1 loss : 0.198112 model2 loss : 0.152357
iteration 3250 : model1 loss : 0.153134 model2 loss : 0.177346
iteration 3251 : model1 loss : 0.156349 model2 loss : 0.158426
iteration 3252 : model1 loss : 0.227373 model2 loss : 0.196092
iteration 3253 : model1 loss : 0.186084 model2 loss : 0.164840
iteration 3254 : model1 loss : 0.191420 model2 loss : 0.201239
iteration 3255 : model1 loss : 0.176983 model2 loss : 0.166413
iteration 3256 : model1 loss : 0.164102 model2 loss : 0.143699
iteration 3257 : model1 loss : 0.199187 model2 loss : 0.197918
iteration 3258 : model1 loss : 0.140549 model2 loss : 0.135020
iteration 3259 : model1 loss : 0.192942 model2 loss : 0.266544
iteration 3260 : model1 loss : 0.141455 model2 loss : 0.163054
iteration 3261 : model1 loss : 0.155140 model2 loss : 0.189260
iteration 3262 : model1 loss : 0.160492 model2 loss : 0.144161
iteration 3263 : model1 loss : 0.155975 model2 loss : 0.139086
iteration 3264 : model1 loss : 0.153541 model2 loss : 0.160334
 33%|████████▊                  | 192/589 [1:18:14<2:44:10, 24.81s/it]iteration 3265 : model1 loss : 0.167935 model2 loss : 0.135018
iteration 3266 : model1 loss : 0.187823 model2 loss : 0.199741
iteration 3267 : model1 loss : 0.182483 model2 loss : 0.214813
iteration 3268 : model1 loss : 0.207585 model2 loss : 0.178430
iteration 3269 : model1 loss : 0.116285 model2 loss : 0.099708
iteration 3270 : model1 loss : 0.203820 model2 loss : 0.161324
iteration 3271 : model1 loss : 0.227410 model2 loss : 0.190642
iteration 3272 : model1 loss : 0.149257 model2 loss : 0.157841
iteration 3273 : model1 loss : 0.141893 model2 loss : 0.130570
iteration 3274 : model1 loss : 0.173809 model2 loss : 0.231808
iteration 3275 : model1 loss : 0.174118 model2 loss : 0.150695
iteration 3276 : model1 loss : 0.150631 model2 loss : 0.115580
iteration 3277 : model1 loss : 0.135001 model2 loss : 0.126659
iteration 3278 : model1 loss : 0.223588 model2 loss : 0.220341
iteration 3279 : model1 loss : 0.158001 model2 loss : 0.122380
iteration 3280 : model1 loss : 0.155016 model2 loss : 0.127303
iteration 3281 : model1 loss : 0.164839 model2 loss : 0.148488
 33%|████████▊                  | 193/589 [1:18:37<2:39:38, 24.19s/it]iteration 3282 : model1 loss : 0.125313 model2 loss : 0.131507
iteration 3283 : model1 loss : 0.171919 model2 loss : 0.190958
iteration 3284 : model1 loss : 0.159893 model2 loss : 0.141279
iteration 3285 : model1 loss : 0.177783 model2 loss : 0.170985
iteration 3286 : model1 loss : 0.174467 model2 loss : 0.126287
iteration 3287 : model1 loss : 0.153369 model2 loss : 0.181598
iteration 3288 : model1 loss : 0.172273 model2 loss : 0.168192
iteration 3289 : model1 loss : 0.162114 model2 loss : 0.196257
iteration 3290 : model1 loss : 0.147262 model2 loss : 0.143380
iteration 3291 : model1 loss : 0.200112 model2 loss : 0.190310
iteration 3292 : model1 loss : 0.169576 model2 loss : 0.157084
iteration 3293 : model1 loss : 0.187250 model2 loss : 0.255537
iteration 3294 : model1 loss : 0.176923 model2 loss : 0.140501
iteration 3295 : model1 loss : 0.155421 model2 loss : 0.141357
iteration 3296 : model1 loss : 0.133377 model2 loss : 0.110362
iteration 3297 : model1 loss : 0.162130 model2 loss : 0.195011
iteration 3298 : model1 loss : 0.169046 model2 loss : 0.167404
 33%|████████▉                  | 194/589 [1:19:00<2:36:29, 23.77s/it]iteration 3299 : model1 loss : 0.138174 model2 loss : 0.136133
iteration 3300 : model1 loss : 0.163423 model2 loss : 0.178160
iteration 3301 : model1 loss : 0.212018 model2 loss : 0.243310
iteration 3302 : model1 loss : 0.143594 model2 loss : 0.161345
iteration 3303 : model1 loss : 0.147418 model2 loss : 0.147300
iteration 3304 : model1 loss : 0.176684 model2 loss : 0.193762
iteration 3305 : model1 loss : 0.221986 model2 loss : 0.152880
iteration 3306 : model1 loss : 0.151128 model2 loss : 0.149467
iteration 3307 : model1 loss : 0.185769 model2 loss : 0.176935
iteration 3308 : model1 loss : 0.156703 model2 loss : 0.148110
iteration 3309 : model1 loss : 0.149039 model2 loss : 0.147080
iteration 3310 : model1 loss : 0.169143 model2 loss : 0.164747
iteration 3311 : model1 loss : 0.125145 model2 loss : 0.138494
iteration 3312 : model1 loss : 0.194153 model2 loss : 0.202856
iteration 3313 : model1 loss : 0.140395 model2 loss : 0.134991
iteration 3314 : model1 loss : 0.136009 model2 loss : 0.128502
iteration 3315 : model1 loss : 0.176201 model2 loss : 0.171139
 33%|████████▉                  | 195/589 [1:19:23<2:34:19, 23.50s/it]iteration 3316 : model1 loss : 0.158220 model2 loss : 0.148101
iteration 3317 : model1 loss : 0.175505 model2 loss : 0.161818
iteration 3318 : model1 loss : 0.163114 model2 loss : 0.158294
iteration 3319 : model1 loss : 0.145127 model2 loss : 0.124296
iteration 3320 : model1 loss : 0.171498 model2 loss : 0.178099
iteration 3321 : model1 loss : 0.171587 model2 loss : 0.169621
iteration 3322 : model1 loss : 0.183745 model2 loss : 0.184043
iteration 3323 : model1 loss : 0.170940 model2 loss : 0.159525
iteration 3324 : model1 loss : 0.167838 model2 loss : 0.152179
iteration 3325 : model1 loss : 0.174092 model2 loss : 0.177993
iteration 3326 : model1 loss : 0.186887 model2 loss : 0.108107
iteration 3327 : model1 loss : 0.211567 model2 loss : 0.226556
iteration 3328 : model1 loss : 0.168835 model2 loss : 0.138594
iteration 3329 : model1 loss : 0.190535 model2 loss : 0.177583
iteration 3330 : model1 loss : 0.152729 model2 loss : 0.166168
iteration 3331 : model1 loss : 0.120790 model2 loss : 0.110911
iteration 3332 : model1 loss : 0.185928 model2 loss : 0.180543
 33%|████████▉                  | 196/589 [1:19:45<2:32:29, 23.28s/it]iteration 3333 : model1 loss : 0.162095 model2 loss : 0.154295
iteration 3334 : model1 loss : 0.175258 model2 loss : 0.152055
iteration 3335 : model1 loss : 0.155890 model2 loss : 0.125330
iteration 3336 : model1 loss : 0.161632 model2 loss : 0.111905
iteration 3337 : model1 loss : 0.173027 model2 loss : 0.160425
iteration 3338 : model1 loss : 0.179627 model2 loss : 0.200979
iteration 3339 : model1 loss : 0.176918 model2 loss : 0.145303
iteration 3340 : model1 loss : 0.142721 model2 loss : 0.123839
iteration 3341 : model1 loss : 0.211426 model2 loss : 0.197973
iteration 3342 : model1 loss : 0.190801 model2 loss : 0.179420
iteration 3343 : model1 loss : 0.190336 model2 loss : 0.180005
iteration 3344 : model1 loss : 0.181802 model2 loss : 0.178100
iteration 3345 : model1 loss : 0.140870 model2 loss : 0.138327
iteration 3346 : model1 loss : 0.205928 model2 loss : 0.182214
iteration 3347 : model1 loss : 0.168876 model2 loss : 0.186693
iteration 3348 : model1 loss : 0.126747 model2 loss : 0.126114
iteration 3349 : model1 loss : 0.186664 model2 loss : 0.150919
 33%|█████████                  | 197/589 [1:20:08<2:31:06, 23.13s/it]iteration 3350 : model1 loss : 0.167249 model2 loss : 0.180719
iteration 3351 : model1 loss : 0.198622 model2 loss : 0.178099
iteration 3352 : model1 loss : 0.154416 model2 loss : 0.135031
iteration 3353 : model1 loss : 0.154155 model2 loss : 0.151102
iteration 3354 : model1 loss : 0.177013 model2 loss : 0.186972
iteration 3355 : model1 loss : 0.162455 model2 loss : 0.133876
iteration 3356 : model1 loss : 0.159330 model2 loss : 0.147742
iteration 3357 : model1 loss : 0.187862 model2 loss : 0.185969
iteration 3358 : model1 loss : 0.189432 model2 loss : 0.174060
iteration 3359 : model1 loss : 0.154170 model2 loss : 0.134908
iteration 3360 : model1 loss : 0.185344 model2 loss : 0.200623
iteration 3361 : model1 loss : 0.176057 model2 loss : 0.133428
iteration 3362 : model1 loss : 0.157909 model2 loss : 0.163233
iteration 3363 : model1 loss : 0.171305 model2 loss : 0.189670
iteration 3364 : model1 loss : 0.130117 model2 loss : 0.152863
iteration 3365 : model1 loss : 0.171061 model2 loss : 0.149575
iteration 3366 : model1 loss : 0.172130 model2 loss : 0.148228
 34%|█████████                  | 198/589 [1:20:31<2:30:09, 23.04s/it]iteration 3367 : model1 loss : 0.210197 model2 loss : 0.152824
iteration 3368 : model1 loss : 0.177122 model2 loss : 0.160145
iteration 3369 : model1 loss : 0.215867 model2 loss : 0.239345
iteration 3370 : model1 loss : 0.126718 model2 loss : 0.145190
iteration 3371 : model1 loss : 0.139941 model2 loss : 0.148236
iteration 3372 : model1 loss : 0.140647 model2 loss : 0.120131
iteration 3373 : model1 loss : 0.155202 model2 loss : 0.118817
iteration 3374 : model1 loss : 0.189386 model2 loss : 0.155776
iteration 3375 : model1 loss : 0.122085 model2 loss : 0.165397
iteration 3376 : model1 loss : 0.144573 model2 loss : 0.157139
iteration 3377 : model1 loss : 0.184034 model2 loss : 0.137430
iteration 3378 : model1 loss : 0.146660 model2 loss : 0.149860
iteration 3379 : model1 loss : 0.151861 model2 loss : 0.212072
iteration 3380 : model1 loss : 0.198266 model2 loss : 0.205035
iteration 3381 : model1 loss : 0.140391 model2 loss : 0.171259
iteration 3382 : model1 loss : 0.164166 model2 loss : 0.144967
iteration 3383 : model1 loss : 0.221326 model2 loss : 0.172631
 34%|█████████                  | 199/589 [1:20:54<2:29:12, 22.95s/it]iteration 3384 : model1 loss : 0.165163 model2 loss : 0.151603
iteration 3385 : model1 loss : 0.140242 model2 loss : 0.134035
iteration 3386 : model1 loss : 0.139022 model2 loss : 0.135569
iteration 3387 : model1 loss : 0.198457 model2 loss : 0.189860
iteration 3388 : model1 loss : 0.186400 model2 loss : 0.111908
iteration 3389 : model1 loss : 0.147991 model2 loss : 0.147372
iteration 3390 : model1 loss : 0.205382 model2 loss : 0.185962
iteration 3391 : model1 loss : 0.127540 model2 loss : 0.127585
iteration 3392 : model1 loss : 0.168799 model2 loss : 0.253401
iteration 3393 : model1 loss : 0.164883 model2 loss : 0.180553
iteration 3394 : model1 loss : 0.206722 model2 loss : 0.194386
iteration 3395 : model1 loss : 0.168723 model2 loss : 0.200497
iteration 3396 : model1 loss : 0.182052 model2 loss : 0.196262
iteration 3397 : model1 loss : 0.221904 model2 loss : 0.233090
iteration 3398 : model1 loss : 0.215216 model2 loss : 0.224019
iteration 3399 : model1 loss : 0.174555 model2 loss : 0.162488
iteration 3400 : model1 loss : 0.169577 model2 loss : 0.195679
iteration 3400 : model1_mean_dice : 0.639833 model1_mean_hd95 : 90.544972 model1_mean_iou : 0.518627
iteration 3400 : model2_mean_dice : 0.761823 model2_mean_hd95 : 68.691646 model2_mean_iou : 0.649185
 34%|█████████▏                 | 200/589 [1:21:36<3:07:08, 28.87s/it]iteration 3401 : model1 loss : 0.212623 model2 loss : 0.207132
iteration 3402 : model1 loss : 0.175783 model2 loss : 0.178798
iteration 3403 : model1 loss : 0.184599 model2 loss : 0.152764
iteration 3404 : model1 loss : 0.157852 model2 loss : 0.176585
iteration 3405 : model1 loss : 0.174664 model2 loss : 0.154762
iteration 3406 : model1 loss : 0.158877 model2 loss : 0.156487
iteration 3407 : model1 loss : 0.151889 model2 loss : 0.183416
iteration 3408 : model1 loss : 0.200367 model2 loss : 0.185528
iteration 3409 : model1 loss : 0.193184 model2 loss : 0.180292
iteration 3410 : model1 loss : 0.217883 model2 loss : 0.227634
iteration 3411 : model1 loss : 0.146971 model2 loss : 0.148372
iteration 3412 : model1 loss : 0.175017 model2 loss : 0.168983
iteration 3413 : model1 loss : 0.239983 model2 loss : 0.262634
iteration 3414 : model1 loss : 0.153097 model2 loss : 0.136131
iteration 3415 : model1 loss : 0.127024 model2 loss : 0.127166
iteration 3416 : model1 loss : 0.148732 model2 loss : 0.115404
iteration 3417 : model1 loss : 0.178971 model2 loss : 0.211797
Traceback (most recent call last):
  File "/ext3/miniconda3/envs/ssl/lib/python3.9/multiprocessing/queues.py", line 251, in _feed
    send_bytes(obj)
  File "/ext3/miniconda3/envs/ssl/lib/python3.9/multiprocessing/connection.py", line 205, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/ext3/miniconda3/envs/ssl/lib/python3.9/multiprocessing/connection.py", line 416, in _send_bytes
    self._send(header + buf)
  File "/ext3/miniconda3/envs/ssl/lib/python3.9/multiprocessing/connection.py", line 373, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
 34%|█████████▏                 | 201/589 [1:21:59<2:54:40, 27.01s/it]iteration 3418 : model1 loss : 0.197875 model2 loss : 0.261126
iteration 3419 : model1 loss : 0.121713 model2 loss : 0.117646
iteration 3420 : model1 loss : 0.144212 model2 loss : 0.158163
iteration 3421 : model1 loss : 0.191377 model2 loss : 0.175619
iteration 3422 : model1 loss : 0.187783 model2 loss : 0.154932
iteration 3423 : model1 loss : 0.205722 model2 loss : 0.170827
iteration 3424 : model1 loss : 0.226399 model2 loss : 0.179208
iteration 3425 : model1 loss : 0.164774 model2 loss : 0.172877
iteration 3426 : model1 loss : 0.164118 model2 loss : 0.159308
iteration 3427 : model1 loss : 0.193083 model2 loss : 0.177330
iteration 3428 : model1 loss : 0.214843 model2 loss : 0.145204
iteration 3429 : model1 loss : 0.170804 model2 loss : 0.183740
iteration 3430 : model1 loss : 0.151584 model2 loss : 0.149461
iteration 3431 : model1 loss : 0.157811 model2 loss : 0.132607
iteration 3432 : model1 loss : 0.190967 model2 loss : 0.184530
iteration 3433 : model1 loss : 0.135058 model2 loss : 0.116430
iteration 3434 : model1 loss : 0.197613 model2 loss : 0.216654
 34%|█████████▎                 | 202/589 [1:22:22<2:45:55, 25.73s/it]iteration 3435 : model1 loss : 0.151418 model2 loss : 0.124618
iteration 3436 : model1 loss : 0.155975 model2 loss : 0.164971
iteration 3437 : model1 loss : 0.177059 model2 loss : 0.195526
iteration 3438 : model1 loss : 0.154877 model2 loss : 0.136295
iteration 3439 : model1 loss : 0.154423 model2 loss : 0.134465
iteration 3440 : model1 loss : 0.249952 model2 loss : 0.221889
iteration 3441 : model1 loss : 0.161539 model2 loss : 0.137874
iteration 3442 : model1 loss : 0.176448 model2 loss : 0.165925
iteration 3443 : model1 loss : 0.153846 model2 loss : 0.151961
iteration 3444 : model1 loss : 0.184428 model2 loss : 0.216772
iteration 3445 : model1 loss : 0.145837 model2 loss : 0.134822
iteration 3446 : model1 loss : 0.177672 model2 loss : 0.166729
iteration 3447 : model1 loss : 0.159632 model2 loss : 0.129371
iteration 3448 : model1 loss : 0.153040 model2 loss : 0.110645
iteration 3449 : model1 loss : 0.177371 model2 loss : 0.158648
iteration 3450 : model1 loss : 0.203086 model2 loss : 0.139782
iteration 3451 : model1 loss : 0.158301 model2 loss : 0.145422
 34%|█████████▎                 | 203/589 [1:22:45<2:39:51, 24.85s/it]iteration 3452 : model1 loss : 0.158888 model2 loss : 0.202201
iteration 3453 : model1 loss : 0.168109 model2 loss : 0.200864
iteration 3454 : model1 loss : 0.227796 model2 loss : 0.174848
iteration 3455 : model1 loss : 0.192485 model2 loss : 0.155304
iteration 3456 : model1 loss : 0.194623 model2 loss : 0.208967
iteration 3457 : model1 loss : 0.160794 model2 loss : 0.157465
iteration 3458 : model1 loss : 0.168926 model2 loss : 0.173299
iteration 3459 : model1 loss : 0.150813 model2 loss : 0.143756
iteration 3460 : model1 loss : 0.162300 model2 loss : 0.167062
iteration 3461 : model1 loss : 0.138505 model2 loss : 0.154882
iteration 3462 : model1 loss : 0.126251 model2 loss : 0.137197
iteration 3463 : model1 loss : 0.199051 model2 loss : 0.180693
iteration 3464 : model1 loss : 0.160701 model2 loss : 0.158880
iteration 3465 : model1 loss : 0.156676 model2 loss : 0.168317
iteration 3466 : model1 loss : 0.174944 model2 loss : 0.122468
iteration 3467 : model1 loss : 0.160609 model2 loss : 0.169549
iteration 3468 : model1 loss : 0.190854 model2 loss : 0.171600
 35%|█████████▎                 | 204/589 [1:23:07<2:35:15, 24.19s/it]iteration 3469 : model1 loss : 0.165904 model2 loss : 0.168488
iteration 3470 : model1 loss : 0.170220 model2 loss : 0.158389
iteration 3471 : model1 loss : 0.173827 model2 loss : 0.189475
iteration 3472 : model1 loss : 0.168077 model2 loss : 0.176623
iteration 3473 : model1 loss : 0.151390 model2 loss : 0.145350
iteration 3474 : model1 loss : 0.150286 model2 loss : 0.155081
iteration 3475 : model1 loss : 0.160033 model2 loss : 0.155473
iteration 3476 : model1 loss : 0.149782 model2 loss : 0.133068
iteration 3477 : model1 loss : 0.180888 model2 loss : 0.138336
iteration 3478 : model1 loss : 0.173004 model2 loss : 0.151010
iteration 3479 : model1 loss : 0.184211 model2 loss : 0.138390
iteration 3480 : model1 loss : 0.140012 model2 loss : 0.158271
iteration 3481 : model1 loss : 0.209464 model2 loss : 0.172131
iteration 3482 : model1 loss : 0.170042 model2 loss : 0.198619
iteration 3483 : model1 loss : 0.160505 model2 loss : 0.127299
iteration 3484 : model1 loss : 0.146671 model2 loss : 0.138097
iteration 3485 : model1 loss : 0.186271 model2 loss : 0.160559
 35%|█████████▍                 | 205/589 [1:23:30<2:31:53, 23.73s/it]iteration 3486 : model1 loss : 0.188017 model2 loss : 0.168124
iteration 3487 : model1 loss : 0.165387 model2 loss : 0.149638
iteration 3488 : model1 loss : 0.164810 model2 loss : 0.116441
iteration 3489 : model1 loss : 0.162879 model2 loss : 0.141775
iteration 3490 : model1 loss : 0.139287 model2 loss : 0.134502
iteration 3491 : model1 loss : 0.182073 model2 loss : 0.140156
iteration 3492 : model1 loss : 0.155346 model2 loss : 0.168962
iteration 3493 : model1 loss : 0.152394 model2 loss : 0.147589
iteration 3494 : model1 loss : 0.130682 model2 loss : 0.132938
iteration 3495 : model1 loss : 0.190849 model2 loss : 0.168190
iteration 3496 : model1 loss : 0.166123 model2 loss : 0.149284
iteration 3497 : model1 loss : 0.217878 model2 loss : 0.202692
iteration 3498 : model1 loss : 0.178449 model2 loss : 0.180721
iteration 3499 : model1 loss : 0.158317 model2 loss : 0.170471
iteration 3500 : model1 loss : 0.159314 model2 loss : 0.138944
iteration 3501 : model1 loss : 0.154162 model2 loss : 0.238576
iteration 3502 : model1 loss : 0.191332 model2 loss : 0.195145
 35%|█████████▍                 | 206/589 [1:23:53<2:29:48, 23.47s/it]iteration 3503 : model1 loss : 0.154120 model2 loss : 0.168606
iteration 3504 : model1 loss : 0.146582 model2 loss : 0.140731
iteration 3505 : model1 loss : 0.159472 model2 loss : 0.156415
iteration 3506 : model1 loss : 0.156131 model2 loss : 0.184171
iteration 3507 : model1 loss : 0.130269 model2 loss : 0.167112
iteration 3508 : model1 loss : 0.182559 model2 loss : 0.132136
iteration 3509 : model1 loss : 0.233572 model2 loss : 0.229303
iteration 3510 : model1 loss : 0.153710 model2 loss : 0.188809
iteration 3511 : model1 loss : 0.146633 model2 loss : 0.181387
iteration 3512 : model1 loss : 0.154658 model2 loss : 0.162813
iteration 3513 : model1 loss : 0.129924 model2 loss : 0.128093
iteration 3514 : model1 loss : 0.153922 model2 loss : 0.185813
iteration 3515 : model1 loss : 0.168618 model2 loss : 0.215298
iteration 3516 : model1 loss : 0.145971 model2 loss : 0.160274
iteration 3517 : model1 loss : 0.181912 model2 loss : 0.147511
iteration 3518 : model1 loss : 0.223206 model2 loss : 0.189593
iteration 3519 : model1 loss : 0.174549 model2 loss : 0.166865
 35%|█████████▍                 | 207/589 [1:24:16<2:28:01, 23.25s/it]iteration 3520 : model1 loss : 0.134169 model2 loss : 0.144574
iteration 3521 : model1 loss : 0.169432 model2 loss : 0.166322
iteration 3522 : model1 loss : 0.232362 model2 loss : 0.213703
iteration 3523 : model1 loss : 0.169145 model2 loss : 0.156286
iteration 3524 : model1 loss : 0.155058 model2 loss : 0.180211
iteration 3525 : model1 loss : 0.148401 model2 loss : 0.159670
iteration 3526 : model1 loss : 0.216446 model2 loss : 0.155799
iteration 3527 : model1 loss : 0.125355 model2 loss : 0.194885
iteration 3528 : model1 loss : 0.157214 model2 loss : 0.156070
iteration 3529 : model1 loss : 0.151447 model2 loss : 0.168161
iteration 3530 : model1 loss : 0.140779 model2 loss : 0.154899
iteration 3531 : model1 loss : 0.189468 model2 loss : 0.214120
iteration 3532 : model1 loss : 0.151557 model2 loss : 0.194222
iteration 3533 : model1 loss : 0.177869 model2 loss : 0.163589
iteration 3534 : model1 loss : 0.145809 model2 loss : 0.139011
iteration 3535 : model1 loss : 0.148190 model2 loss : 0.157513
iteration 3536 : model1 loss : 0.160510 model2 loss : 0.157865
 35%|█████████▌                 | 208/589 [1:24:38<2:26:40, 23.10s/it]iteration 3537 : model1 loss : 0.148714 model2 loss : 0.160548
iteration 3538 : model1 loss : 0.190356 model2 loss : 0.161205
iteration 3539 : model1 loss : 0.177195 model2 loss : 0.164381
iteration 3540 : model1 loss : 0.206361 model2 loss : 0.170316
iteration 3541 : model1 loss : 0.119208 model2 loss : 0.113460
iteration 3542 : model1 loss : 0.187188 model2 loss : 0.199984
iteration 3543 : model1 loss : 0.204456 model2 loss : 0.176191
iteration 3544 : model1 loss : 0.151198 model2 loss : 0.152214
iteration 3545 : model1 loss : 0.199378 model2 loss : 0.162320
iteration 3546 : model1 loss : 0.117142 model2 loss : 0.109384
iteration 3547 : model1 loss : 0.161748 model2 loss : 0.124466
iteration 3548 : model1 loss : 0.165125 model2 loss : 0.140312
iteration 3549 : model1 loss : 0.116829 model2 loss : 0.119394
iteration 3550 : model1 loss : 0.150308 model2 loss : 0.158688
iteration 3551 : model1 loss : 0.179570 model2 loss : 0.143145
iteration 3552 : model1 loss : 0.149843 model2 loss : 0.164203
iteration 3553 : model1 loss : 0.206554 model2 loss : 0.206145
 35%|█████████▌                 | 209/589 [1:25:01<2:25:45, 23.01s/it]iteration 3554 : model1 loss : 0.196190 model2 loss : 0.172587
iteration 3555 : model1 loss : 0.202659 model2 loss : 0.178701
iteration 3556 : model1 loss : 0.142594 model2 loss : 0.117874
iteration 3557 : model1 loss : 0.104520 model2 loss : 0.107979
iteration 3558 : model1 loss : 0.156453 model2 loss : 0.147541
iteration 3559 : model1 loss : 0.156519 model2 loss : 0.164245
iteration 3560 : model1 loss : 0.177340 model2 loss : 0.207638
iteration 3561 : model1 loss : 0.236089 model2 loss : 0.165877
iteration 3562 : model1 loss : 0.173946 model2 loss : 0.176125
iteration 3563 : model1 loss : 0.168704 model2 loss : 0.157937
iteration 3564 : model1 loss : 0.153270 model2 loss : 0.166873
iteration 3565 : model1 loss : 0.148390 model2 loss : 0.162200
iteration 3566 : model1 loss : 0.197585 model2 loss : 0.188634
iteration 3567 : model1 loss : 0.170355 model2 loss : 0.124189
iteration 3568 : model1 loss : 0.171255 model2 loss : 0.156711
iteration 3569 : model1 loss : 0.169855 model2 loss : 0.197636
iteration 3570 : model1 loss : 0.166012 model2 loss : 0.164749
 36%|█████████▋                 | 210/589 [1:25:24<2:24:47, 22.92s/it]iteration 3571 : model1 loss : 0.157383 model2 loss : 0.140931
iteration 3572 : model1 loss : 0.166273 model2 loss : 0.179730
iteration 3573 : model1 loss : 0.164158 model2 loss : 0.134150
iteration 3574 : model1 loss : 0.155505 model2 loss : 0.192352
iteration 3575 : model1 loss : 0.127910 model2 loss : 0.125615
iteration 3576 : model1 loss : 0.210234 model2 loss : 0.197206
iteration 3577 : model1 loss : 0.181863 model2 loss : 0.150068
iteration 3578 : model1 loss : 0.168684 model2 loss : 0.185096
iteration 3579 : model1 loss : 0.169235 model2 loss : 0.153659
iteration 3580 : model1 loss : 0.146148 model2 loss : 0.138223
iteration 3581 : model1 loss : 0.223049 model2 loss : 0.226309
iteration 3582 : model1 loss : 0.210636 model2 loss : 0.195107
iteration 3583 : model1 loss : 0.151456 model2 loss : 0.152911
iteration 3584 : model1 loss : 0.143089 model2 loss : 0.148754
iteration 3585 : model1 loss : 0.135275 model2 loss : 0.149269
iteration 3586 : model1 loss : 0.140904 model2 loss : 0.177848
iteration 3587 : model1 loss : 0.178245 model2 loss : 0.152707
 36%|█████████▋                 | 211/589 [1:25:47<2:24:00, 22.86s/it]iteration 3588 : model1 loss : 0.159388 model2 loss : 0.126954
iteration 3589 : model1 loss : 0.189052 model2 loss : 0.179889
iteration 3590 : model1 loss : 0.152122 model2 loss : 0.158512
iteration 3591 : model1 loss : 0.164821 model2 loss : 0.153621
iteration 3592 : model1 loss : 0.190964 model2 loss : 0.208417
iteration 3593 : model1 loss : 0.173391 model2 loss : 0.151424
iteration 3594 : model1 loss : 0.168824 model2 loss : 0.155102
iteration 3595 : model1 loss : 0.154989 model2 loss : 0.136721
iteration 3596 : model1 loss : 0.174797 model2 loss : 0.178938
iteration 3597 : model1 loss : 0.207768 model2 loss : 0.191573
iteration 3598 : model1 loss : 0.163001 model2 loss : 0.180069
iteration 3599 : model1 loss : 0.156722 model2 loss : 0.133102
iteration 3600 : model1 loss : 0.190436 model2 loss : 0.126978
iteration 3600 : model1_mean_dice : 0.678636 model1_mean_hd95 : 98.962193 model1_mean_iou : 0.551421
iteration 3600 : model2_mean_dice : 0.765084 model2_mean_hd95 : 70.372254 model2_mean_iou : 0.650716
iteration 3601 : model1 loss : 0.123485 model2 loss : 0.137538
iteration 3602 : model1 loss : 0.195058 model2 loss : 0.116168
iteration 3603 : model1 loss : 0.136634 model2 loss : 0.133966
iteration 3604 : model1 loss : 0.135162 model2 loss : 0.178012
 36%|█████████▋                 | 212/589 [1:26:29<3:01:21, 28.86s/it]iteration 3605 : model1 loss : 0.163967 model2 loss : 0.149562
iteration 3606 : model1 loss : 0.203339 model2 loss : 0.158422
iteration 3607 : model1 loss : 0.171197 model2 loss : 0.143995
iteration 3608 : model1 loss : 0.136250 model2 loss : 0.107890
iteration 3609 : model1 loss : 0.210951 model2 loss : 0.192962
iteration 3610 : model1 loss : 0.153248 model2 loss : 0.123561
iteration 3611 : model1 loss : 0.149270 model2 loss : 0.155978
iteration 3612 : model1 loss : 0.155856 model2 loss : 0.137920
iteration 3613 : model1 loss : 0.169974 model2 loss : 0.175064
iteration 3614 : model1 loss : 0.183305 model2 loss : 0.180581
iteration 3615 : model1 loss : 0.163132 model2 loss : 0.138196
iteration 3616 : model1 loss : 0.144658 model2 loss : 0.137866
iteration 3617 : model1 loss : 0.198810 model2 loss : 0.163811
iteration 3618 : model1 loss : 0.210185 model2 loss : 0.170023
iteration 3619 : model1 loss : 0.141185 model2 loss : 0.129040
iteration 3620 : model1 loss : 0.138729 model2 loss : 0.117018
iteration 3621 : model1 loss : 0.155882 model2 loss : 0.143784
 36%|█████████▊                 | 213/589 [1:26:52<2:49:05, 26.98s/it]iteration 3622 : model1 loss : 0.134325 model2 loss : 0.141750
iteration 3623 : model1 loss : 0.199486 model2 loss : 0.169699
iteration 3624 : model1 loss : 0.196269 model2 loss : 0.180715
iteration 3625 : model1 loss : 0.147496 model2 loss : 0.143397
iteration 3626 : model1 loss : 0.235746 model2 loss : 0.164451
iteration 3627 : model1 loss : 0.129177 model2 loss : 0.147921
iteration 3628 : model1 loss : 0.127275 model2 loss : 0.123464
iteration 3629 : model1 loss : 0.170468 model2 loss : 0.171540
iteration 3630 : model1 loss : 0.132531 model2 loss : 0.156457
iteration 3631 : model1 loss : 0.159854 model2 loss : 0.131706
iteration 3632 : model1 loss : 0.158900 model2 loss : 0.171207
iteration 3633 : model1 loss : 0.161737 model2 loss : 0.151461
iteration 3634 : model1 loss : 0.184151 model2 loss : 0.183511
iteration 3635 : model1 loss : 0.168090 model2 loss : 0.182632
iteration 3636 : model1 loss : 0.156897 model2 loss : 0.107166
iteration 3637 : model1 loss : 0.166340 model2 loss : 0.167712
iteration 3638 : model1 loss : 0.173663 model2 loss : 0.166921
 36%|█████████▊                 | 214/589 [1:27:15<2:40:30, 25.68s/it]iteration 3639 : model1 loss : 0.176302 model2 loss : 0.174427
iteration 3640 : model1 loss : 0.156798 model2 loss : 0.119081
iteration 3641 : model1 loss : 0.236060 model2 loss : 0.161082
iteration 3642 : model1 loss : 0.181796 model2 loss : 0.171444
iteration 3643 : model1 loss : 0.139422 model2 loss : 0.112115
iteration 3644 : model1 loss : 0.088440 model2 loss : 0.104351
iteration 3645 : model1 loss : 0.186806 model2 loss : 0.141135
iteration 3646 : model1 loss : 0.148448 model2 loss : 0.154601
iteration 3647 : model1 loss : 0.111683 model2 loss : 0.101584
iteration 3648 : model1 loss : 0.181917 model2 loss : 0.176188
iteration 3649 : model1 loss : 0.173019 model2 loss : 0.162801
iteration 3650 : model1 loss : 0.148139 model2 loss : 0.136772
iteration 3651 : model1 loss : 0.208186 model2 loss : 0.172330
iteration 3652 : model1 loss : 0.159615 model2 loss : 0.146718
iteration 3653 : model1 loss : 0.178266 model2 loss : 0.208548
iteration 3654 : model1 loss : 0.164125 model2 loss : 0.131850
iteration 3655 : model1 loss : 0.188603 model2 loss : 0.163732
 37%|█████████▊                 | 215/589 [1:27:37<2:34:39, 24.81s/it]iteration 3656 : model1 loss : 0.157038 model2 loss : 0.161360
iteration 3657 : model1 loss : 0.160231 model2 loss : 0.177634
iteration 3658 : model1 loss : 0.178299 model2 loss : 0.154060
iteration 3659 : model1 loss : 0.222402 model2 loss : 0.194589
iteration 3660 : model1 loss : 0.140020 model2 loss : 0.112965
iteration 3661 : model1 loss : 0.204585 model2 loss : 0.195801
iteration 3662 : model1 loss : 0.159340 model2 loss : 0.155383
iteration 3663 : model1 loss : 0.175128 model2 loss : 0.196042
iteration 3664 : model1 loss : 0.172080 model2 loss : 0.149862
iteration 3665 : model1 loss : 0.166115 model2 loss : 0.159814
iteration 3666 : model1 loss : 0.165300 model2 loss : 0.161325
iteration 3667 : model1 loss : 0.138257 model2 loss : 0.155790
iteration 3668 : model1 loss : 0.159449 model2 loss : 0.123528
iteration 3669 : model1 loss : 0.204716 model2 loss : 0.174087
iteration 3670 : model1 loss : 0.153955 model2 loss : 0.182080
iteration 3671 : model1 loss : 0.147800 model2 loss : 0.129458
iteration 3672 : model1 loss : 0.110217 model2 loss : 0.098151
 37%|█████████▉                 | 216/589 [1:28:00<2:30:14, 24.17s/it]iteration 3673 : model1 loss : 0.162432 model2 loss : 0.175301
iteration 3674 : model1 loss : 0.147054 model2 loss : 0.124607
iteration 3675 : model1 loss : 0.154851 model2 loss : 0.159078
iteration 3676 : model1 loss : 0.223991 model2 loss : 0.224651
iteration 3677 : model1 loss : 0.142923 model2 loss : 0.129392
iteration 3678 : model1 loss : 0.152535 model2 loss : 0.168225
iteration 3679 : model1 loss : 0.180639 model2 loss : 0.164573
iteration 3680 : model1 loss : 0.164908 model2 loss : 0.166687
iteration 3681 : model1 loss : 0.129734 model2 loss : 0.121577
iteration 3682 : model1 loss : 0.133396 model2 loss : 0.188120
iteration 3683 : model1 loss : 0.173147 model2 loss : 0.153579
iteration 3684 : model1 loss : 0.203275 model2 loss : 0.176787
iteration 3685 : model1 loss : 0.119935 model2 loss : 0.142605
iteration 3686 : model1 loss : 0.133960 model2 loss : 0.135538
iteration 3687 : model1 loss : 0.169645 model2 loss : 0.163282
iteration 3688 : model1 loss : 0.184589 model2 loss : 0.178410
iteration 3689 : model1 loss : 0.186656 model2 loss : 0.145225
 37%|█████████▉                 | 217/589 [1:28:23<2:27:06, 23.73s/it]iteration 3690 : model1 loss : 0.134003 model2 loss : 0.119884
iteration 3691 : model1 loss : 0.170781 model2 loss : 0.161039
iteration 3692 : model1 loss : 0.152137 model2 loss : 0.163198
iteration 3693 : model1 loss : 0.150437 model2 loss : 0.120485
iteration 3694 : model1 loss : 0.151978 model2 loss : 0.177875
iteration 3695 : model1 loss : 0.151324 model2 loss : 0.167527
iteration 3696 : model1 loss : 0.218853 model2 loss : 0.151200
iteration 3697 : model1 loss : 0.225850 model2 loss : 0.160595
iteration 3698 : model1 loss : 0.226082 model2 loss : 0.214680
iteration 3699 : model1 loss : 0.140551 model2 loss : 0.111344
iteration 3700 : model1 loss : 0.141203 model2 loss : 0.138150
iteration 3701 : model1 loss : 0.122333 model2 loss : 0.147639
iteration 3702 : model1 loss : 0.177454 model2 loss : 0.179826
iteration 3703 : model1 loss : 0.222910 model2 loss : 0.213321
iteration 3704 : model1 loss : 0.165337 model2 loss : 0.176349
iteration 3705 : model1 loss : 0.167549 model2 loss : 0.151632
iteration 3706 : model1 loss : 0.186455 model2 loss : 0.185060
 37%|█████████▉                 | 218/589 [1:28:46<2:25:01, 23.45s/it]iteration 3707 : model1 loss : 0.145253 model2 loss : 0.136234
iteration 3708 : model1 loss : 0.176080 model2 loss : 0.151604
iteration 3709 : model1 loss : 0.145885 model2 loss : 0.189095
iteration 3710 : model1 loss : 0.212844 model2 loss : 0.173413
iteration 3711 : model1 loss : 0.154937 model2 loss : 0.182354
iteration 3712 : model1 loss : 0.215423 model2 loss : 0.156345
iteration 3713 : model1 loss : 0.135571 model2 loss : 0.124008
iteration 3714 : model1 loss : 0.141242 model2 loss : 0.155546
iteration 3715 : model1 loss : 0.207030 model2 loss : 0.131176
iteration 3716 : model1 loss : 0.208838 model2 loss : 0.192526
iteration 3717 : model1 loss : 0.175708 model2 loss : 0.136557
iteration 3718 : model1 loss : 0.160100 model2 loss : 0.137617
iteration 3719 : model1 loss : 0.147371 model2 loss : 0.179573
iteration 3720 : model1 loss : 0.175126 model2 loss : 0.191203
iteration 3721 : model1 loss : 0.186234 model2 loss : 0.203790
iteration 3722 : model1 loss : 0.151030 model2 loss : 0.125194
iteration 3723 : model1 loss : 0.162399 model2 loss : 0.193802
 37%|██████████                 | 219/589 [1:29:08<2:23:11, 23.22s/it]iteration 3724 : model1 loss : 0.163920 model2 loss : 0.120873
iteration 3725 : model1 loss : 0.159073 model2 loss : 0.152539
iteration 3726 : model1 loss : 0.183991 model2 loss : 0.176090
iteration 3727 : model1 loss : 0.128891 model2 loss : 0.137551
iteration 3728 : model1 loss : 0.173306 model2 loss : 0.159047
iteration 3729 : model1 loss : 0.162915 model2 loss : 0.175507
iteration 3730 : model1 loss : 0.154840 model2 loss : 0.169173
iteration 3731 : model1 loss : 0.216853 model2 loss : 0.193906
iteration 3732 : model1 loss : 0.196446 model2 loss : 0.217630
iteration 3733 : model1 loss : 0.201762 model2 loss : 0.176682
iteration 3734 : model1 loss : 0.160044 model2 loss : 0.174230
iteration 3735 : model1 loss : 0.194875 model2 loss : 0.170935
iteration 3736 : model1 loss : 0.155492 model2 loss : 0.185401
iteration 3737 : model1 loss : 0.156923 model2 loss : 0.150355
iteration 3738 : model1 loss : 0.126194 model2 loss : 0.146117
iteration 3739 : model1 loss : 0.197223 model2 loss : 0.183183
iteration 3740 : model1 loss : 0.193853 model2 loss : 0.122022
 37%|██████████                 | 220/589 [1:29:31<2:21:52, 23.07s/it]iteration 3741 : model1 loss : 0.150911 model2 loss : 0.205845
iteration 3742 : model1 loss : 0.197698 model2 loss : 0.194817
iteration 3743 : model1 loss : 0.196210 model2 loss : 0.129695
iteration 3744 : model1 loss : 0.154025 model2 loss : 0.170577
iteration 3745 : model1 loss : 0.157890 model2 loss : 0.136680
iteration 3746 : model1 loss : 0.136637 model2 loss : 0.130424
iteration 3747 : model1 loss : 0.182996 model2 loss : 0.135637
iteration 3748 : model1 loss : 0.148387 model2 loss : 0.119956
iteration 3749 : model1 loss : 0.161755 model2 loss : 0.161254
iteration 3750 : model1 loss : 0.150511 model2 loss : 0.151654
iteration 3751 : model1 loss : 0.206452 model2 loss : 0.157390
iteration 3752 : model1 loss : 0.159433 model2 loss : 0.141070
iteration 3753 : model1 loss : 0.194818 model2 loss : 0.210241
iteration 3754 : model1 loss : 0.232785 model2 loss : 0.182826
iteration 3755 : model1 loss : 0.146336 model2 loss : 0.158170
iteration 3756 : model1 loss : 0.194562 model2 loss : 0.156942
iteration 3757 : model1 loss : 0.207022 model2 loss : 0.171831
 38%|██████████▏                | 221/589 [1:29:54<2:21:03, 23.00s/it]iteration 3758 : model1 loss : 0.190612 model2 loss : 0.183599
iteration 3759 : model1 loss : 0.233658 model2 loss : 0.223094
iteration 3760 : model1 loss : 0.151281 model2 loss : 0.109917
iteration 3761 : model1 loss : 0.209150 model2 loss : 0.172108
iteration 3762 : model1 loss : 0.141311 model2 loss : 0.129758
iteration 3763 : model1 loss : 0.146304 model2 loss : 0.116354
iteration 3764 : model1 loss : 0.174625 model2 loss : 0.156900
iteration 3765 : model1 loss : 0.113861 model2 loss : 0.092171
iteration 3766 : model1 loss : 0.182859 model2 loss : 0.139080
iteration 3767 : model1 loss : 0.169928 model2 loss : 0.154114
iteration 3768 : model1 loss : 0.144111 model2 loss : 0.135174
iteration 3769 : model1 loss : 0.158496 model2 loss : 0.152233
iteration 3770 : model1 loss : 0.171793 model2 loss : 0.162919
iteration 3771 : model1 loss : 0.137120 model2 loss : 0.147350
iteration 3772 : model1 loss : 0.136468 model2 loss : 0.167422
iteration 3773 : model1 loss : 0.188225 model2 loss : 0.160264
iteration 3774 : model1 loss : 0.172431 model2 loss : 0.187125
 38%|██████████▏                | 222/589 [1:30:17<2:20:08, 22.91s/it]iteration 3775 : model1 loss : 0.213780 model2 loss : 0.213578
iteration 3776 : model1 loss : 0.203864 model2 loss : 0.165340
iteration 3777 : model1 loss : 0.168240 model2 loss : 0.178706
iteration 3778 : model1 loss : 0.170792 model2 loss : 0.189495
iteration 3779 : model1 loss : 0.172419 model2 loss : 0.164425
iteration 3780 : model1 loss : 0.205085 model2 loss : 0.177395
iteration 3781 : model1 loss : 0.156432 model2 loss : 0.189995
iteration 3782 : model1 loss : 0.151999 model2 loss : 0.163876
iteration 3783 : model1 loss : 0.130803 model2 loss : 0.144405
iteration 3784 : model1 loss : 0.131403 model2 loss : 0.169378
iteration 3785 : model1 loss : 0.128531 model2 loss : 0.152936
iteration 3786 : model1 loss : 0.159191 model2 loss : 0.171595
iteration 3787 : model1 loss : 0.138829 model2 loss : 0.134965
iteration 3788 : model1 loss : 0.178576 model2 loss : 0.203991
iteration 3789 : model1 loss : 0.162516 model2 loss : 0.132156
iteration 3790 : model1 loss : 0.150066 model2 loss : 0.159755
iteration 3791 : model1 loss : 0.151058 model2 loss : 0.134724
 38%|██████████▏                | 223/589 [1:30:39<2:19:25, 22.86s/it]iteration 3792 : model1 loss : 0.194359 model2 loss : 0.134670
iteration 3793 : model1 loss : 0.160720 model2 loss : 0.132385
iteration 3794 : model1 loss : 0.172740 model2 loss : 0.134713
iteration 3795 : model1 loss : 0.206740 model2 loss : 0.178543
iteration 3796 : model1 loss : 0.175978 model2 loss : 0.162842
iteration 3797 : model1 loss : 0.122096 model2 loss : 0.120548
iteration 3798 : model1 loss : 0.208285 model2 loss : 0.151666
iteration 3799 : model1 loss : 0.154290 model2 loss : 0.189739
iteration 3800 : model1 loss : 0.157885 model2 loss : 0.131686
iteration 3800 : model1_mean_dice : 0.685576 model1_mean_hd95 : 89.959285 model1_mean_iou : 0.557506
iteration 3800 : model2_mean_dice : 0.764025 model2_mean_hd95 : 69.102793 model2_mean_iou : 0.653683
iteration 3801 : model1 loss : 0.189351 model2 loss : 0.159508
iteration 3802 : model1 loss : 0.157704 model2 loss : 0.178093
iteration 3803 : model1 loss : 0.160621 model2 loss : 0.155360
iteration 3804 : model1 loss : 0.138311 model2 loss : 0.116813
iteration 3805 : model1 loss : 0.179810 model2 loss : 0.141956
iteration 3806 : model1 loss : 0.143879 model2 loss : 0.142339
iteration 3807 : model1 loss : 0.143583 model2 loss : 0.145701
iteration 3808 : model1 loss : 0.191019 model2 loss : 0.173910
 38%|██████████▎                | 224/589 [1:31:22<2:55:22, 28.83s/it]iteration 3809 : model1 loss : 0.158923 model2 loss : 0.147797
iteration 3810 : model1 loss : 0.138602 model2 loss : 0.132281
iteration 3811 : model1 loss : 0.173574 model2 loss : 0.120553
iteration 3812 : model1 loss : 0.220112 model2 loss : 0.159056
iteration 3813 : model1 loss : 0.160464 model2 loss : 0.136013
iteration 3814 : model1 loss : 0.126057 model2 loss : 0.114675
iteration 3815 : model1 loss : 0.153189 model2 loss : 0.146153
iteration 3816 : model1 loss : 0.172982 model2 loss : 0.174428
iteration 3817 : model1 loss : 0.130767 model2 loss : 0.141595
iteration 3818 : model1 loss : 0.149238 model2 loss : 0.157133
iteration 3819 : model1 loss : 0.223378 model2 loss : 0.177653
iteration 3820 : model1 loss : 0.170682 model2 loss : 0.204040
iteration 3821 : model1 loss : 0.190673 model2 loss : 0.173650
iteration 3822 : model1 loss : 0.139454 model2 loss : 0.164390
iteration 3823 : model1 loss : 0.163887 model2 loss : 0.150896
iteration 3824 : model1 loss : 0.174921 model2 loss : 0.172021
iteration 3825 : model1 loss : 0.192399 model2 loss : 0.137466
 38%|██████████▎                | 225/589 [1:31:45<2:43:43, 26.99s/it]iteration 3826 : model1 loss : 0.159730 model2 loss : 0.179693
iteration 3827 : model1 loss : 0.183881 model2 loss : 0.193432
iteration 3828 : model1 loss : 0.147631 model2 loss : 0.152627
iteration 3829 : model1 loss : 0.154380 model2 loss : 0.146991
iteration 3830 : model1 loss : 0.184690 model2 loss : 0.155147
iteration 3831 : model1 loss : 0.146787 model2 loss : 0.145007
iteration 3832 : model1 loss : 0.135341 model2 loss : 0.178795
iteration 3833 : model1 loss : 0.158876 model2 loss : 0.153682
iteration 3834 : model1 loss : 0.200014 model2 loss : 0.165000
iteration 3835 : model1 loss : 0.138114 model2 loss : 0.149474
iteration 3836 : model1 loss : 0.134122 model2 loss : 0.142998
iteration 3837 : model1 loss : 0.191759 model2 loss : 0.205801
iteration 3838 : model1 loss : 0.159554 model2 loss : 0.154117
iteration 3839 : model1 loss : 0.132884 model2 loss : 0.164283
iteration 3840 : model1 loss : 0.176672 model2 loss : 0.151421
iteration 3841 : model1 loss : 0.190562 model2 loss : 0.210236
iteration 3842 : model1 loss : 0.221696 model2 loss : 0.194721
 38%|██████████▎                | 226/589 [1:32:07<2:35:24, 25.69s/it]iteration 3843 : model1 loss : 0.182162 model2 loss : 0.162137
iteration 3844 : model1 loss : 0.152033 model2 loss : 0.139761
iteration 3845 : model1 loss : 0.159542 model2 loss : 0.174713
iteration 3846 : model1 loss : 0.184127 model2 loss : 0.193360
iteration 3847 : model1 loss : 0.154415 model2 loss : 0.161904
iteration 3848 : model1 loss : 0.144871 model2 loss : 0.126820
iteration 3849 : model1 loss : 0.148282 model2 loss : 0.137079
iteration 3850 : model1 loss : 0.142892 model2 loss : 0.118717
iteration 3851 : model1 loss : 0.172152 model2 loss : 0.211591
iteration 3852 : model1 loss : 0.193662 model2 loss : 0.190083
iteration 3853 : model1 loss : 0.159790 model2 loss : 0.156762
iteration 3854 : model1 loss : 0.147353 model2 loss : 0.155053
iteration 3855 : model1 loss : 0.188130 model2 loss : 0.185867
iteration 3856 : model1 loss : 0.172159 model2 loss : 0.148752
iteration 3857 : model1 loss : 0.143218 model2 loss : 0.131903
iteration 3858 : model1 loss : 0.134076 model2 loss : 0.107983
iteration 3859 : model1 loss : 0.197131 model2 loss : 0.180707
 39%|██████████▍                | 227/589 [1:32:30<2:29:49, 24.83s/it]iteration 3860 : model1 loss : 0.169781 model2 loss : 0.143188
iteration 3861 : model1 loss : 0.140399 model2 loss : 0.125337
iteration 3862 : model1 loss : 0.146191 model2 loss : 0.131413
iteration 3863 : model1 loss : 0.152736 model2 loss : 0.159553
iteration 3864 : model1 loss : 0.141494 model2 loss : 0.115442
iteration 3865 : model1 loss : 0.192976 model2 loss : 0.224556
iteration 3866 : model1 loss : 0.191831 model2 loss : 0.170030
iteration 3867 : model1 loss : 0.175470 model2 loss : 0.182855
iteration 3868 : model1 loss : 0.121164 model2 loss : 0.124789
iteration 3869 : model1 loss : 0.185275 model2 loss : 0.159233
iteration 3870 : model1 loss : 0.181403 model2 loss : 0.158319
iteration 3871 : model1 loss : 0.193802 model2 loss : 0.221041
iteration 3872 : model1 loss : 0.222643 model2 loss : 0.175631
iteration 3873 : model1 loss : 0.175969 model2 loss : 0.166293
iteration 3874 : model1 loss : 0.152840 model2 loss : 0.148652
iteration 3875 : model1 loss : 0.152043 model2 loss : 0.146310
iteration 3876 : model1 loss : 0.159760 model2 loss : 0.127291
 39%|██████████▍                | 228/589 [1:32:53<2:25:37, 24.20s/it]iteration 3877 : model1 loss : 0.152304 model2 loss : 0.147206
iteration 3878 : model1 loss : 0.158864 model2 loss : 0.134182
iteration 3879 : model1 loss : 0.156511 model2 loss : 0.134335
iteration 3880 : model1 loss : 0.105793 model2 loss : 0.126332
iteration 3881 : model1 loss : 0.117488 model2 loss : 0.136793
iteration 3882 : model1 loss : 0.207799 model2 loss : 0.214822
iteration 3883 : model1 loss : 0.154530 model2 loss : 0.148690
iteration 3884 : model1 loss : 0.163850 model2 loss : 0.169553
iteration 3885 : model1 loss : 0.150733 model2 loss : 0.155865
iteration 3886 : model1 loss : 0.167613 model2 loss : 0.149133
iteration 3887 : model1 loss : 0.171068 model2 loss : 0.183969
iteration 3888 : model1 loss : 0.145197 model2 loss : 0.143668
iteration 3889 : model1 loss : 0.152838 model2 loss : 0.136659
iteration 3890 : model1 loss : 0.189360 model2 loss : 0.107439
iteration 3891 : model1 loss : 0.216722 model2 loss : 0.197347
iteration 3892 : model1 loss : 0.153322 model2 loss : 0.153935
iteration 3893 : model1 loss : 0.174801 model2 loss : 0.239419
 39%|██████████▍                | 229/589 [1:33:16<2:22:27, 23.74s/it]iteration 3894 : model1 loss : 0.182826 model2 loss : 0.149799
iteration 3895 : model1 loss : 0.117370 model2 loss : 0.117494
iteration 3896 : model1 loss : 0.116729 model2 loss : 0.104066
iteration 3897 : model1 loss : 0.137291 model2 loss : 0.141785
iteration 3898 : model1 loss : 0.168055 model2 loss : 0.163635
iteration 3899 : model1 loss : 0.174567 model2 loss : 0.149556
iteration 3900 : model1 loss : 0.162730 model2 loss : 0.151383
iteration 3901 : model1 loss : 0.151211 model2 loss : 0.118713
iteration 3902 : model1 loss : 0.177342 model2 loss : 0.226408
iteration 3903 : model1 loss : 0.206244 model2 loss : 0.200362
iteration 3904 : model1 loss : 0.127481 model2 loss : 0.122636
iteration 3905 : model1 loss : 0.157423 model2 loss : 0.184441
iteration 3906 : model1 loss : 0.191013 model2 loss : 0.171361
iteration 3907 : model1 loss : 0.180775 model2 loss : 0.170805
iteration 3908 : model1 loss : 0.173689 model2 loss : 0.134549
iteration 3909 : model1 loss : 0.167711 model2 loss : 0.186341
iteration 3910 : model1 loss : 0.164541 model2 loss : 0.155198
 39%|██████████▌                | 230/589 [1:33:38<2:20:20, 23.45s/it]iteration 3911 : model1 loss : 0.170420 model2 loss : 0.208589
iteration 3912 : model1 loss : 0.172169 model2 loss : 0.195235
iteration 3913 : model1 loss : 0.174486 model2 loss : 0.181004
iteration 3914 : model1 loss : 0.143342 model2 loss : 0.137791
iteration 3915 : model1 loss : 0.124566 model2 loss : 0.120412
iteration 3916 : model1 loss : 0.191609 model2 loss : 0.190640
iteration 3917 : model1 loss : 0.179410 model2 loss : 0.130985
iteration 3918 : model1 loss : 0.153050 model2 loss : 0.114272
iteration 3919 : model1 loss : 0.180268 model2 loss : 0.127923
iteration 3920 : model1 loss : 0.185806 model2 loss : 0.198778
iteration 3921 : model1 loss : 0.186809 model2 loss : 0.199173
iteration 3922 : model1 loss : 0.140424 model2 loss : 0.153892
iteration 3923 : model1 loss : 0.157812 model2 loss : 0.157116
iteration 3924 : model1 loss : 0.173288 model2 loss : 0.211381
iteration 3925 : model1 loss : 0.128480 model2 loss : 0.118144
iteration 3926 : model1 loss : 0.116379 model2 loss : 0.133984
iteration 3927 : model1 loss : 0.150208 model2 loss : 0.118764
 39%|██████████▌                | 231/589 [1:34:01<2:18:36, 23.23s/it]iteration 3928 : model1 loss : 0.208276 model2 loss : 0.170991
iteration 3929 : model1 loss : 0.157661 model2 loss : 0.136301
iteration 3930 : model1 loss : 0.181671 model2 loss : 0.138843
iteration 3931 : model1 loss : 0.178404 model2 loss : 0.149928
iteration 3932 : model1 loss : 0.111248 model2 loss : 0.128201
iteration 3933 : model1 loss : 0.146192 model2 loss : 0.176460
iteration 3934 : model1 loss : 0.156782 model2 loss : 0.178636
iteration 3935 : model1 loss : 0.208031 model2 loss : 0.151573
iteration 3936 : model1 loss : 0.155867 model2 loss : 0.117161
iteration 3937 : model1 loss : 0.176314 model2 loss : 0.154494
iteration 3938 : model1 loss : 0.133952 model2 loss : 0.119862
iteration 3939 : model1 loss : 0.160243 model2 loss : 0.137804
iteration 3940 : model1 loss : 0.122002 model2 loss : 0.113961
iteration 3941 : model1 loss : 0.185193 model2 loss : 0.241123
iteration 3942 : model1 loss : 0.114792 model2 loss : 0.118458
iteration 3943 : model1 loss : 0.180075 model2 loss : 0.160663
iteration 3944 : model1 loss : 0.223925 model2 loss : 0.214987
 39%|██████████▋                | 232/589 [1:34:24<2:17:13, 23.06s/it]iteration 3945 : model1 loss : 0.121345 model2 loss : 0.107417
iteration 3946 : model1 loss : 0.160772 model2 loss : 0.176146
iteration 3947 : model1 loss : 0.156739 model2 loss : 0.148398
iteration 3948 : model1 loss : 0.198546 model2 loss : 0.174365
iteration 3949 : model1 loss : 0.148874 model2 loss : 0.127842
iteration 3950 : model1 loss : 0.182766 model2 loss : 0.170030
iteration 3951 : model1 loss : 0.171401 model2 loss : 0.143907
iteration 3952 : model1 loss : 0.142468 model2 loss : 0.132886
iteration 3953 : model1 loss : 0.151053 model2 loss : 0.166036
iteration 3954 : model1 loss : 0.145400 model2 loss : 0.166148
iteration 3955 : model1 loss : 0.133647 model2 loss : 0.143781
iteration 3956 : model1 loss : 0.151096 model2 loss : 0.101145
iteration 3957 : model1 loss : 0.182851 model2 loss : 0.169863
iteration 3958 : model1 loss : 0.168171 model2 loss : 0.157776
iteration 3959 : model1 loss : 0.179121 model2 loss : 0.172841
iteration 3960 : model1 loss : 0.187771 model2 loss : 0.182832
iteration 3961 : model1 loss : 0.152855 model2 loss : 0.144461
 40%|██████████▋                | 233/589 [1:34:47<2:16:22, 22.98s/it]iteration 3962 : model1 loss : 0.141805 model2 loss : 0.120438
iteration 3963 : model1 loss : 0.147277 model2 loss : 0.136886
iteration 3964 : model1 loss : 0.233252 model2 loss : 0.210449
iteration 3965 : model1 loss : 0.165705 model2 loss : 0.158581
iteration 3966 : model1 loss : 0.169264 model2 loss : 0.182472
iteration 3967 : model1 loss : 0.149415 model2 loss : 0.110477
iteration 3968 : model1 loss : 0.152557 model2 loss : 0.140117
iteration 3969 : model1 loss : 0.144897 model2 loss : 0.134068
iteration 3970 : model1 loss : 0.148105 model2 loss : 0.118268
iteration 3971 : model1 loss : 0.170913 model2 loss : 0.145562
iteration 3972 : model1 loss : 0.182975 model2 loss : 0.153193
iteration 3973 : model1 loss : 0.205231 model2 loss : 0.166619
iteration 3974 : model1 loss : 0.185650 model2 loss : 0.172450
iteration 3975 : model1 loss : 0.180972 model2 loss : 0.176845
iteration 3976 : model1 loss : 0.154981 model2 loss : 0.151233
iteration 3977 : model1 loss : 0.134250 model2 loss : 0.133296
iteration 3978 : model1 loss : 0.149367 model2 loss : 0.143432
 40%|██████████▋                | 234/589 [1:35:09<2:15:34, 22.91s/it]iteration 3979 : model1 loss : 0.156824 model2 loss : 0.161022
iteration 3980 : model1 loss : 0.180895 model2 loss : 0.173270
iteration 3981 : model1 loss : 0.190834 model2 loss : 0.143700
iteration 3982 : model1 loss : 0.154265 model2 loss : 0.134266
iteration 3983 : model1 loss : 0.175074 model2 loss : 0.138735
iteration 3984 : model1 loss : 0.148545 model2 loss : 0.140616
iteration 3985 : model1 loss : 0.117548 model2 loss : 0.108277
iteration 3986 : model1 loss : 0.142467 model2 loss : 0.123733
iteration 3987 : model1 loss : 0.164462 model2 loss : 0.111262
iteration 3988 : model1 loss : 0.168614 model2 loss : 0.133024
iteration 3989 : model1 loss : 0.179315 model2 loss : 0.143828
iteration 3990 : model1 loss : 0.148158 model2 loss : 0.105872
iteration 3991 : model1 loss : 0.146675 model2 loss : 0.117128
iteration 3992 : model1 loss : 0.147606 model2 loss : 0.140003
iteration 3993 : model1 loss : 0.191253 model2 loss : 0.159448
iteration 3994 : model1 loss : 0.195555 model2 loss : 0.169912
iteration 3995 : model1 loss : 0.149948 model2 loss : 0.135326
 40%|██████████▊                | 235/589 [1:35:32<2:14:47, 22.85s/it]iteration 3996 : model1 loss : 0.175026 model2 loss : 0.118663
iteration 3997 : model1 loss : 0.260155 model2 loss : 0.175595
iteration 3998 : model1 loss : 0.156382 model2 loss : 0.156068
iteration 3999 : model1 loss : 0.138983 model2 loss : 0.110550
iteration 4000 : model1 loss : 0.159991 model2 loss : 0.132103
iteration 4000 : model1_mean_dice : 0.676247 model1_mean_hd95 : 84.895149 model1_mean_iou : 0.548548
iteration 4000 : model2_mean_dice : 0.765665 model2_mean_hd95 : 65.834825 model2_mean_iou : 0.654419
iteration 4001 : model1 loss : 0.143776 model2 loss : 0.140882
iteration 4002 : model1 loss : 0.150505 model2 loss : 0.146224
iteration 4003 : model1 loss : 0.174055 model2 loss : 0.148964
iteration 4004 : model1 loss : 0.179989 model2 loss : 0.179583
iteration 4005 : model1 loss : 0.128269 model2 loss : 0.185498
iteration 4006 : model1 loss : 0.162281 model2 loss : 0.195254
iteration 4007 : model1 loss : 0.169377 model2 loss : 0.144340
iteration 4008 : model1 loss : 0.148558 model2 loss : 0.154711
iteration 4009 : model1 loss : 0.178006 model2 loss : 0.154441
iteration 4010 : model1 loss : 0.148725 model2 loss : 0.123203
iteration 4011 : model1 loss : 0.216067 model2 loss : 0.162953
iteration 4012 : model1 loss : 0.165357 model2 loss : 0.149252
 40%|██████████▊                | 236/589 [1:36:15<2:49:25, 28.80s/it]iteration 4013 : model1 loss : 0.139171 model2 loss : 0.114087
iteration 4014 : model1 loss : 0.160309 model2 loss : 0.162010
iteration 4015 : model1 loss : 0.134918 model2 loss : 0.127243
iteration 4016 : model1 loss : 0.142831 model2 loss : 0.114783
iteration 4017 : model1 loss : 0.147735 model2 loss : 0.137786
iteration 4018 : model1 loss : 0.221103 model2 loss : 0.147312
iteration 4019 : model1 loss : 0.201771 model2 loss : 0.204989
iteration 4020 : model1 loss : 0.176746 model2 loss : 0.141789
iteration 4021 : model1 loss : 0.185607 model2 loss : 0.150657
iteration 4022 : model1 loss : 0.148118 model2 loss : 0.146531
iteration 4023 : model1 loss : 0.176694 model2 loss : 0.167625
iteration 4024 : model1 loss : 0.116315 model2 loss : 0.102466
iteration 4025 : model1 loss : 0.168217 model2 loss : 0.161240
iteration 4026 : model1 loss : 0.184574 model2 loss : 0.186428
iteration 4027 : model1 loss : 0.136623 model2 loss : 0.123242
iteration 4028 : model1 loss : 0.200916 model2 loss : 0.260374
iteration 4029 : model1 loss : 0.178930 model2 loss : 0.133243
 40%|██████████▊                | 237/589 [1:36:37<2:38:12, 26.97s/it]iteration 4030 : model1 loss : 0.147558 model2 loss : 0.107153
iteration 4031 : model1 loss : 0.167578 model2 loss : 0.166486
iteration 4032 : model1 loss : 0.199758 model2 loss : 0.163687
iteration 4033 : model1 loss : 0.173393 model2 loss : 0.169425
iteration 4034 : model1 loss : 0.174871 model2 loss : 0.174157
iteration 4035 : model1 loss : 0.158309 model2 loss : 0.126587
iteration 4036 : model1 loss : 0.197077 model2 loss : 0.197190
iteration 4037 : model1 loss : 0.151419 model2 loss : 0.146036
iteration 4038 : model1 loss : 0.161114 model2 loss : 0.153587
iteration 4039 : model1 loss : 0.138962 model2 loss : 0.114318
iteration 4040 : model1 loss : 0.159681 model2 loss : 0.122291
iteration 4041 : model1 loss : 0.163029 model2 loss : 0.138679
iteration 4042 : model1 loss : 0.167798 model2 loss : 0.154928
iteration 4043 : model1 loss : 0.189239 model2 loss : 0.132265
iteration 4044 : model1 loss : 0.153872 model2 loss : 0.162275
iteration 4045 : model1 loss : 0.123022 model2 loss : 0.103455
iteration 4046 : model1 loss : 0.166379 model2 loss : 0.165222
 40%|██████████▉                | 238/589 [1:37:00<2:30:16, 25.69s/it]iteration 4047 : model1 loss : 0.194428 model2 loss : 0.158214
iteration 4048 : model1 loss : 0.103299 model2 loss : 0.106157
iteration 4049 : model1 loss : 0.176944 model2 loss : 0.197163
iteration 4050 : model1 loss : 0.201456 model2 loss : 0.149439
iteration 4051 : model1 loss : 0.154206 model2 loss : 0.118985
iteration 4052 : model1 loss : 0.172615 model2 loss : 0.196618
iteration 4053 : model1 loss : 0.172952 model2 loss : 0.146449
iteration 4054 : model1 loss : 0.229672 model2 loss : 0.157210
iteration 4055 : model1 loss : 0.197996 model2 loss : 0.186297
iteration 4056 : model1 loss : 0.159607 model2 loss : 0.126744
iteration 4057 : model1 loss : 0.147134 model2 loss : 0.119755
iteration 4058 : model1 loss : 0.145088 model2 loss : 0.134086
iteration 4059 : model1 loss : 0.143848 model2 loss : 0.153546
iteration 4060 : model1 loss : 0.124394 model2 loss : 0.101756
iteration 4061 : model1 loss : 0.181897 model2 loss : 0.177261
iteration 4062 : model1 loss : 0.146982 model2 loss : 0.177771
iteration 4063 : model1 loss : 0.177023 model2 loss : 0.162782
 41%|██████████▉                | 239/589 [1:37:23<2:24:49, 24.83s/it]iteration 4064 : model1 loss : 0.176981 model2 loss : 0.136530
iteration 4065 : model1 loss : 0.180932 model2 loss : 0.164030
iteration 4066 : model1 loss : 0.166741 model2 loss : 0.186314
iteration 4067 : model1 loss : 0.132021 model2 loss : 0.132832
iteration 4068 : model1 loss : 0.153348 model2 loss : 0.135997
iteration 4069 : model1 loss : 0.179723 model2 loss : 0.132079
iteration 4070 : model1 loss : 0.183721 model2 loss : 0.193832
iteration 4071 : model1 loss : 0.150289 model2 loss : 0.122471
iteration 4072 : model1 loss : 0.144684 model2 loss : 0.151521
iteration 4073 : model1 loss : 0.196743 model2 loss : 0.168997
iteration 4074 : model1 loss : 0.142398 model2 loss : 0.129589
iteration 4075 : model1 loss : 0.167754 model2 loss : 0.132347
iteration 4076 : model1 loss : 0.161863 model2 loss : 0.151276
iteration 4077 : model1 loss : 0.123730 model2 loss : 0.086903
iteration 4078 : model1 loss : 0.192774 model2 loss : 0.176365
iteration 4079 : model1 loss : 0.171146 model2 loss : 0.123938
iteration 4080 : model1 loss : 0.144642 model2 loss : 0.118475
 41%|███████████                | 240/589 [1:37:46<2:20:41, 24.19s/it]iteration 4081 : model1 loss : 0.191057 model2 loss : 0.145049
iteration 4082 : model1 loss : 0.152709 model2 loss : 0.135382
iteration 4083 : model1 loss : 0.147816 model2 loss : 0.115419
iteration 4084 : model1 loss : 0.150029 model2 loss : 0.140798
iteration 4085 : model1 loss : 0.111454 model2 loss : 0.109139
iteration 4086 : model1 loss : 0.187177 model2 loss : 0.166436
iteration 4087 : model1 loss : 0.159186 model2 loss : 0.202077
iteration 4088 : model1 loss : 0.133031 model2 loss : 0.123937
iteration 4089 : model1 loss : 0.172015 model2 loss : 0.157117
iteration 4090 : model1 loss : 0.137832 model2 loss : 0.137512
iteration 4091 : model1 loss : 0.163174 model2 loss : 0.123609
iteration 4092 : model1 loss : 0.196110 model2 loss : 0.211389
iteration 4093 : model1 loss : 0.132019 model2 loss : 0.131428
iteration 4094 : model1 loss : 0.144684 model2 loss : 0.155000
iteration 4095 : model1 loss : 0.142600 model2 loss : 0.131766
iteration 4096 : model1 loss : 0.139828 model2 loss : 0.145937
iteration 4097 : model1 loss : 0.156510 model2 loss : 0.129717
 41%|███████████                | 241/589 [1:38:08<2:17:50, 23.77s/it]iteration 4098 : model1 loss : 0.218356 model2 loss : 0.182921
iteration 4099 : model1 loss : 0.176392 model2 loss : 0.170635
iteration 4100 : model1 loss : 0.163962 model2 loss : 0.134274
iteration 4101 : model1 loss : 0.185069 model2 loss : 0.122467
iteration 4102 : model1 loss : 0.198532 model2 loss : 0.177678
iteration 4103 : model1 loss : 0.132328 model2 loss : 0.143241
iteration 4104 : model1 loss : 0.154691 model2 loss : 0.171338
iteration 4105 : model1 loss : 0.142537 model2 loss : 0.152884
iteration 4106 : model1 loss : 0.124163 model2 loss : 0.153651
iteration 4107 : model1 loss : 0.169133 model2 loss : 0.228547
iteration 4108 : model1 loss : 0.134846 model2 loss : 0.135697
iteration 4109 : model1 loss : 0.144769 model2 loss : 0.174885
iteration 4110 : model1 loss : 0.179979 model2 loss : 0.144832
iteration 4111 : model1 loss : 0.168185 model2 loss : 0.173175
iteration 4112 : model1 loss : 0.149049 model2 loss : 0.164146
iteration 4113 : model1 loss : 0.154320 model2 loss : 0.129225
iteration 4114 : model1 loss : 0.157000 model2 loss : 0.142973
 41%|███████████                | 242/589 [1:38:31<2:15:49, 23.49s/it]iteration 4115 : model1 loss : 0.131059 model2 loss : 0.116591
iteration 4116 : model1 loss : 0.149242 model2 loss : 0.191246
iteration 4117 : model1 loss : 0.159999 model2 loss : 0.143556
iteration 4118 : model1 loss : 0.132161 model2 loss : 0.105239
iteration 4119 : model1 loss : 0.131174 model2 loss : 0.158203
iteration 4120 : model1 loss : 0.157393 model2 loss : 0.201751
iteration 4121 : model1 loss : 0.215731 model2 loss : 0.167322
iteration 4122 : model1 loss : 0.175531 model2 loss : 0.166460
iteration 4123 : model1 loss : 0.165123 model2 loss : 0.141043
iteration 4124 : model1 loss : 0.175336 model2 loss : 0.164729
iteration 4125 : model1 loss : 0.166537 model2 loss : 0.134512
iteration 4126 : model1 loss : 0.155469 model2 loss : 0.133752
iteration 4127 : model1 loss : 0.151615 model2 loss : 0.190564
iteration 4128 : model1 loss : 0.187360 model2 loss : 0.175390
iteration 4129 : model1 loss : 0.168640 model2 loss : 0.163450
iteration 4130 : model1 loss : 0.152512 model2 loss : 0.127615
iteration 4131 : model1 loss : 0.178308 model2 loss : 0.115156
 41%|███████████▏               | 243/589 [1:38:54<2:14:03, 23.25s/it]iteration 4132 : model1 loss : 0.140073 model2 loss : 0.145770
iteration 4133 : model1 loss : 0.122353 model2 loss : 0.120679
iteration 4134 : model1 loss : 0.197415 model2 loss : 0.171700
iteration 4135 : model1 loss : 0.153229 model2 loss : 0.169246
iteration 4136 : model1 loss : 0.134692 model2 loss : 0.115381
iteration 4137 : model1 loss : 0.155396 model2 loss : 0.134575
iteration 4138 : model1 loss : 0.188137 model2 loss : 0.118302
iteration 4139 : model1 loss : 0.167397 model2 loss : 0.131845
iteration 4140 : model1 loss : 0.203811 model2 loss : 0.155108
iteration 4141 : model1 loss : 0.147759 model2 loss : 0.162018
iteration 4142 : model1 loss : 0.178666 model2 loss : 0.172732
iteration 4143 : model1 loss : 0.198165 model2 loss : 0.177056
iteration 4144 : model1 loss : 0.157035 model2 loss : 0.129842
iteration 4145 : model1 loss : 0.195910 model2 loss : 0.134009
iteration 4146 : model1 loss : 0.148432 model2 loss : 0.168031
iteration 4147 : model1 loss : 0.167592 model2 loss : 0.160396
iteration 4148 : model1 loss : 0.128300 model2 loss : 0.137641
 41%|███████████▏               | 244/589 [1:39:17<2:12:43, 23.08s/it]iteration 4149 : model1 loss : 0.161099 model2 loss : 0.155910
iteration 4150 : model1 loss : 0.174038 model2 loss : 0.164407
iteration 4151 : model1 loss : 0.123045 model2 loss : 0.105876
iteration 4152 : model1 loss : 0.179149 model2 loss : 0.145020
iteration 4153 : model1 loss : 0.183630 model2 loss : 0.199110
iteration 4154 : model1 loss : 0.207913 model2 loss : 0.136013
iteration 4155 : model1 loss : 0.127842 model2 loss : 0.112348
iteration 4156 : model1 loss : 0.168267 model2 loss : 0.183078
iteration 4157 : model1 loss : 0.156151 model2 loss : 0.177282
iteration 4158 : model1 loss : 0.177730 model2 loss : 0.165697
iteration 4159 : model1 loss : 0.158138 model2 loss : 0.146310
iteration 4160 : model1 loss : 0.155542 model2 loss : 0.174214
iteration 4161 : model1 loss : 0.172246 model2 loss : 0.205296
iteration 4162 : model1 loss : 0.151446 model2 loss : 0.139893
iteration 4163 : model1 loss : 0.158021 model2 loss : 0.112451
iteration 4164 : model1 loss : 0.144844 model2 loss : 0.148175
iteration 4165 : model1 loss : 0.154925 model2 loss : 0.160722
 42%|███████████▏               | 245/589 [1:39:40<2:11:59, 23.02s/it]iteration 4166 : model1 loss : 0.129594 model2 loss : 0.141786
iteration 4167 : model1 loss : 0.149080 model2 loss : 0.120690
iteration 4168 : model1 loss : 0.181734 model2 loss : 0.208479
iteration 4169 : model1 loss : 0.175476 model2 loss : 0.145082
iteration 4170 : model1 loss : 0.135736 model2 loss : 0.142278
iteration 4171 : model1 loss : 0.188609 model2 loss : 0.129873
iteration 4172 : model1 loss : 0.211299 model2 loss : 0.156522
iteration 4173 : model1 loss : 0.137353 model2 loss : 0.133244
iteration 4174 : model1 loss : 0.175760 model2 loss : 0.199834
iteration 4175 : model1 loss : 0.155971 model2 loss : 0.211925
iteration 4176 : model1 loss : 0.182139 model2 loss : 0.179232
iteration 4177 : model1 loss : 0.191560 model2 loss : 0.176047
iteration 4178 : model1 loss : 0.160658 model2 loss : 0.121782
iteration 4179 : model1 loss : 0.134904 model2 loss : 0.137676
iteration 4180 : model1 loss : 0.129662 model2 loss : 0.100035
iteration 4181 : model1 loss : 0.134373 model2 loss : 0.110973
iteration 4182 : model1 loss : 0.166238 model2 loss : 0.130557
 42%|███████████▎               | 246/589 [1:40:02<2:11:03, 22.93s/it]iteration 4183 : model1 loss : 0.145467 model2 loss : 0.114690
iteration 4184 : model1 loss : 0.143882 model2 loss : 0.121211
iteration 4185 : model1 loss : 0.173491 model2 loss : 0.240802
iteration 4186 : model1 loss : 0.206060 model2 loss : 0.128279
iteration 4187 : model1 loss : 0.159143 model2 loss : 0.163395
iteration 4188 : model1 loss : 0.137658 model2 loss : 0.127805
iteration 4189 : model1 loss : 0.167226 model2 loss : 0.178534
iteration 4190 : model1 loss : 0.137375 model2 loss : 0.134573
iteration 4191 : model1 loss : 0.151683 model2 loss : 0.148177
iteration 4192 : model1 loss : 0.127351 model2 loss : 0.114202
iteration 4193 : model1 loss : 0.162327 model2 loss : 0.141294
iteration 4194 : model1 loss : 0.182040 model2 loss : 0.146661
iteration 4195 : model1 loss : 0.138630 model2 loss : 0.139552
iteration 4196 : model1 loss : 0.133656 model2 loss : 0.122847
iteration 4197 : model1 loss : 0.171204 model2 loss : 0.215489
iteration 4198 : model1 loss : 0.172426 model2 loss : 0.214354
iteration 4199 : model1 loss : 0.184078 model2 loss : 0.132562
 42%|███████████▎               | 247/589 [1:40:25<2:10:18, 22.86s/it]iteration 4200 : model1 loss : 0.120585 model2 loss : 0.110240
iteration 4200 : model1_mean_dice : 0.641874 model1_mean_hd95 : 94.992982 model1_mean_iou : 0.514052
iteration 4200 : model2_mean_dice : 0.780541 model2_mean_hd95 : 64.770923 model2_mean_iou : 0.665235
iteration 4201 : model1 loss : 0.170719 model2 loss : 0.124061
iteration 4202 : model1 loss : 0.165476 model2 loss : 0.154281
iteration 4203 : model1 loss : 0.156291 model2 loss : 0.101982
iteration 4204 : model1 loss : 0.138905 model2 loss : 0.144775
iteration 4205 : model1 loss : 0.155303 model2 loss : 0.161080
iteration 4206 : model1 loss : 0.218373 model2 loss : 0.198448
iteration 4207 : model1 loss : 0.171598 model2 loss : 0.152107
iteration 4208 : model1 loss : 0.162964 model2 loss : 0.150489
iteration 4209 : model1 loss : 0.145671 model2 loss : 0.130038
iteration 4210 : model1 loss : 0.139872 model2 loss : 0.126235
iteration 4211 : model1 loss : 0.148004 model2 loss : 0.140767
iteration 4212 : model1 loss : 0.117134 model2 loss : 0.124895
iteration 4213 : model1 loss : 0.143717 model2 loss : 0.158547
iteration 4214 : model1 loss : 0.152093 model2 loss : 0.127532
iteration 4215 : model1 loss : 0.162311 model2 loss : 0.148940
iteration 4216 : model1 loss : 0.177418 model2 loss : 0.174195
 42%|███████████▎               | 248/589 [1:41:08<2:44:24, 28.93s/it]iteration 4217 : model1 loss : 0.125461 model2 loss : 0.081969
iteration 4218 : model1 loss : 0.137188 model2 loss : 0.135661
iteration 4219 : model1 loss : 0.189195 model2 loss : 0.145998
iteration 4220 : model1 loss : 0.148324 model2 loss : 0.138774
iteration 4221 : model1 loss : 0.124993 model2 loss : 0.152399
iteration 4222 : model1 loss : 0.180934 model2 loss : 0.158812
iteration 4223 : model1 loss : 0.191597 model2 loss : 0.163886
iteration 4224 : model1 loss : 0.164363 model2 loss : 0.132020
iteration 4225 : model1 loss : 0.152126 model2 loss : 0.129828
iteration 4226 : model1 loss : 0.192151 model2 loss : 0.151780
iteration 4227 : model1 loss : 0.182076 model2 loss : 0.158163
iteration 4228 : model1 loss : 0.183013 model2 loss : 0.172039
iteration 4229 : model1 loss : 0.179365 model2 loss : 0.142173
iteration 4230 : model1 loss : 0.137897 model2 loss : 0.128494
iteration 4231 : model1 loss : 0.144659 model2 loss : 0.150199
iteration 4232 : model1 loss : 0.174059 model2 loss : 0.141758
iteration 4233 : model1 loss : 0.149867 model2 loss : 0.135918
 42%|███████████▍               | 249/589 [1:41:31<2:33:15, 27.05s/it]iteration 4234 : model1 loss : 0.155801 model2 loss : 0.157520
iteration 4235 : model1 loss : 0.163050 model2 loss : 0.125536
iteration 4236 : model1 loss : 0.173661 model2 loss : 0.165908
iteration 4237 : model1 loss : 0.158005 model2 loss : 0.119330
iteration 4238 : model1 loss : 0.128775 model2 loss : 0.107094
iteration 4239 : model1 loss : 0.172279 model2 loss : 0.160529
iteration 4240 : model1 loss : 0.202211 model2 loss : 0.248080
iteration 4241 : model1 loss : 0.176132 model2 loss : 0.136696
iteration 4242 : model1 loss : 0.126183 model2 loss : 0.092816
iteration 4243 : model1 loss : 0.139110 model2 loss : 0.112462
iteration 4244 : model1 loss : 0.154695 model2 loss : 0.100310
iteration 4245 : model1 loss : 0.155563 model2 loss : 0.141769
iteration 4246 : model1 loss : 0.146757 model2 loss : 0.143971
iteration 4247 : model1 loss : 0.149336 model2 loss : 0.156675
iteration 4248 : model1 loss : 0.188512 model2 loss : 0.132890
iteration 4249 : model1 loss : 0.162225 model2 loss : 0.128721
iteration 4250 : model1 loss : 0.217758 model2 loss : 0.182167
 42%|███████████▍               | 250/589 [1:41:53<2:25:39, 25.78s/it]iteration 4251 : model1 loss : 0.160651 model2 loss : 0.135099
iteration 4252 : model1 loss : 0.158724 model2 loss : 0.181896
iteration 4253 : model1 loss : 0.148947 model2 loss : 0.154817
iteration 4254 : model1 loss : 0.161583 model2 loss : 0.130916
iteration 4255 : model1 loss : 0.176734 model2 loss : 0.144513
iteration 4256 : model1 loss : 0.137892 model2 loss : 0.145107
iteration 4257 : model1 loss : 0.129433 model2 loss : 0.108924
iteration 4258 : model1 loss : 0.169313 model2 loss : 0.157371
iteration 4259 : model1 loss : 0.137886 model2 loss : 0.152603
iteration 4260 : model1 loss : 0.160267 model2 loss : 0.156383
iteration 4261 : model1 loss : 0.162442 model2 loss : 0.152636
iteration 4262 : model1 loss : 0.162648 model2 loss : 0.136896
iteration 4263 : model1 loss : 0.159789 model2 loss : 0.148185
iteration 4264 : model1 loss : 0.140380 model2 loss : 0.119668
iteration 4265 : model1 loss : 0.132481 model2 loss : 0.119811
iteration 4266 : model1 loss : 0.149939 model2 loss : 0.115807
iteration 4267 : model1 loss : 0.170068 model2 loss : 0.148271
 43%|███████████▌               | 251/589 [1:42:16<2:19:59, 24.85s/it]iteration 4268 : model1 loss : 0.138498 model2 loss : 0.178686
iteration 4269 : model1 loss : 0.133487 model2 loss : 0.130400
iteration 4270 : model1 loss : 0.188555 model2 loss : 0.132544
iteration 4271 : model1 loss : 0.172448 model2 loss : 0.154261
iteration 4272 : model1 loss : 0.140013 model2 loss : 0.141531
iteration 4273 : model1 loss : 0.164303 model2 loss : 0.113096
iteration 4274 : model1 loss : 0.148260 model2 loss : 0.123424
iteration 4275 : model1 loss : 0.175229 model2 loss : 0.163373
iteration 4276 : model1 loss : 0.122011 model2 loss : 0.098955
iteration 4277 : model1 loss : 0.132696 model2 loss : 0.132628
iteration 4278 : model1 loss : 0.222392 model2 loss : 0.186768
iteration 4279 : model1 loss : 0.176886 model2 loss : 0.146290
iteration 4280 : model1 loss : 0.116695 model2 loss : 0.088848
iteration 4281 : model1 loss : 0.151910 model2 loss : 0.150338
iteration 4282 : model1 loss : 0.148627 model2 loss : 0.149098
iteration 4283 : model1 loss : 0.187511 model2 loss : 0.164883
iteration 4284 : model1 loss : 0.164967 model2 loss : 0.148949
 43%|███████████▌               | 252/589 [1:42:39<2:15:59, 24.21s/it]iteration 4285 : model1 loss : 0.156951 model2 loss : 0.123171
iteration 4286 : model1 loss : 0.135484 model2 loss : 0.114396
iteration 4287 : model1 loss : 0.137335 model2 loss : 0.114954
iteration 4288 : model1 loss : 0.154112 model2 loss : 0.130565
iteration 4289 : model1 loss : 0.155148 model2 loss : 0.131254
iteration 4290 : model1 loss : 0.163751 model2 loss : 0.154729
iteration 4291 : model1 loss : 0.186072 model2 loss : 0.169577
iteration 4292 : model1 loss : 0.149337 model2 loss : 0.172242
iteration 4293 : model1 loss : 0.203319 model2 loss : 0.193542
iteration 4294 : model1 loss : 0.168132 model2 loss : 0.169014
iteration 4295 : model1 loss : 0.149463 model2 loss : 0.164785
iteration 4296 : model1 loss : 0.138353 model2 loss : 0.121904
iteration 4297 : model1 loss : 0.140485 model2 loss : 0.158734
iteration 4298 : model1 loss : 0.188987 model2 loss : 0.170939
iteration 4299 : model1 loss : 0.168408 model2 loss : 0.184571
iteration 4300 : model1 loss : 0.122118 model2 loss : 0.134231
iteration 4301 : model1 loss : 0.169945 model2 loss : 0.140677
 43%|███████████▌               | 253/589 [1:43:02<2:13:18, 23.80s/it]iteration 4302 : model1 loss : 0.184021 model2 loss : 0.167399
iteration 4303 : model1 loss : 0.158953 model2 loss : 0.123812
iteration 4304 : model1 loss : 0.156853 model2 loss : 0.142704
iteration 4305 : model1 loss : 0.157983 model2 loss : 0.161327
iteration 4306 : model1 loss : 0.161230 model2 loss : 0.117213
iteration 4307 : model1 loss : 0.122256 model2 loss : 0.115783
iteration 4308 : model1 loss : 0.118604 model2 loss : 0.136744
iteration 4309 : model1 loss : 0.193184 model2 loss : 0.164809
iteration 4310 : model1 loss : 0.176959 model2 loss : 0.146687
iteration 4311 : model1 loss : 0.178202 model2 loss : 0.171050
iteration 4312 : model1 loss : 0.172934 model2 loss : 0.176511
iteration 4313 : model1 loss : 0.161212 model2 loss : 0.167011
iteration 4314 : model1 loss : 0.125969 model2 loss : 0.137248
iteration 4315 : model1 loss : 0.137596 model2 loss : 0.136158
iteration 4316 : model1 loss : 0.143014 model2 loss : 0.129606
iteration 4317 : model1 loss : 0.132655 model2 loss : 0.144732
iteration 4318 : model1 loss : 0.151715 model2 loss : 0.127813
 43%|███████████▋               | 254/589 [1:43:24<2:11:07, 23.49s/it]iteration 4319 : model1 loss : 0.187517 model2 loss : 0.196900
iteration 4320 : model1 loss : 0.160805 model2 loss : 0.149327
iteration 4321 : model1 loss : 0.144941 model2 loss : 0.130157
iteration 4322 : model1 loss : 0.216903 model2 loss : 0.182873
iteration 4323 : model1 loss : 0.155830 model2 loss : 0.129354
iteration 4324 : model1 loss : 0.154194 model2 loss : 0.126889
iteration 4325 : model1 loss : 0.144082 model2 loss : 0.124835
iteration 4326 : model1 loss : 0.162500 model2 loss : 0.125245
iteration 4327 : model1 loss : 0.211358 model2 loss : 0.162958
iteration 4328 : model1 loss : 0.163374 model2 loss : 0.135803
iteration 4329 : model1 loss : 0.137914 model2 loss : 0.098536
iteration 4330 : model1 loss : 0.108711 model2 loss : 0.110395
iteration 4331 : model1 loss : 0.162633 model2 loss : 0.158962
iteration 4332 : model1 loss : 0.160192 model2 loss : 0.158006
iteration 4333 : model1 loss : 0.139986 model2 loss : 0.154737
iteration 4334 : model1 loss : 0.168354 model2 loss : 0.144363
iteration 4335 : model1 loss : 0.152735 model2 loss : 0.105379
 43%|███████████▋               | 255/589 [1:43:47<2:09:34, 23.28s/it]iteration 4336 : model1 loss : 0.178123 model2 loss : 0.121395
iteration 4337 : model1 loss : 0.170375 model2 loss : 0.116146
iteration 4338 : model1 loss : 0.116759 model2 loss : 0.118660
iteration 4339 : model1 loss : 0.169584 model2 loss : 0.127724
iteration 4340 : model1 loss : 0.103735 model2 loss : 0.099962
iteration 4341 : model1 loss : 0.167908 model2 loss : 0.170593
iteration 4342 : model1 loss : 0.183050 model2 loss : 0.157497
iteration 4343 : model1 loss : 0.093475 model2 loss : 0.083266
iteration 4344 : model1 loss : 0.210367 model2 loss : 0.149705
iteration 4345 : model1 loss : 0.152724 model2 loss : 0.124188
iteration 4346 : model1 loss : 0.191466 model2 loss : 0.147989
iteration 4347 : model1 loss : 0.179849 model2 loss : 0.170986
iteration 4348 : model1 loss : 0.173638 model2 loss : 0.223269
iteration 4349 : model1 loss : 0.188429 model2 loss : 0.228296
iteration 4350 : model1 loss : 0.175913 model2 loss : 0.158955
iteration 4351 : model1 loss : 0.127096 model2 loss : 0.133296
iteration 4352 : model1 loss : 0.155753 model2 loss : 0.107882
 43%|███████████▋               | 256/589 [1:44:10<2:08:23, 23.13s/it]iteration 4353 : model1 loss : 0.162689 model2 loss : 0.195297
iteration 4354 : model1 loss : 0.167905 model2 loss : 0.155546
iteration 4355 : model1 loss : 0.213390 model2 loss : 0.160117
iteration 4356 : model1 loss : 0.161263 model2 loss : 0.130494
iteration 4357 : model1 loss : 0.130149 model2 loss : 0.136031
iteration 4358 : model1 loss : 0.204170 model2 loss : 0.180765
iteration 4359 : model1 loss : 0.179583 model2 loss : 0.152769
iteration 4360 : model1 loss : 0.138852 model2 loss : 0.127986
iteration 4361 : model1 loss : 0.124312 model2 loss : 0.108453
iteration 4362 : model1 loss : 0.169383 model2 loss : 0.146517
iteration 4363 : model1 loss : 0.162018 model2 loss : 0.245687
iteration 4364 : model1 loss : 0.161333 model2 loss : 0.178588
iteration 4365 : model1 loss : 0.166358 model2 loss : 0.129044
iteration 4366 : model1 loss : 0.134619 model2 loss : 0.135277
iteration 4367 : model1 loss : 0.187136 model2 loss : 0.175021
iteration 4368 : model1 loss : 0.124864 model2 loss : 0.125908
iteration 4369 : model1 loss : 0.124143 model2 loss : 0.129185
 44%|███████████▊               | 257/589 [1:44:33<2:07:21, 23.02s/it]iteration 4370 : model1 loss : 0.182208 model2 loss : 0.207276
iteration 4371 : model1 loss : 0.136886 model2 loss : 0.145371
iteration 4372 : model1 loss : 0.163473 model2 loss : 0.171381
iteration 4373 : model1 loss : 0.133983 model2 loss : 0.105309
iteration 4374 : model1 loss : 0.128695 model2 loss : 0.107094
iteration 4375 : model1 loss : 0.170239 model2 loss : 0.146705
iteration 4376 : model1 loss : 0.142752 model2 loss : 0.101825
iteration 4377 : model1 loss : 0.142256 model2 loss : 0.115740
iteration 4378 : model1 loss : 0.168366 model2 loss : 0.134447
iteration 4379 : model1 loss : 0.222649 model2 loss : 0.186524
iteration 4380 : model1 loss : 0.117612 model2 loss : 0.103177
iteration 4381 : model1 loss : 0.163175 model2 loss : 0.148975
iteration 4382 : model1 loss : 0.133097 model2 loss : 0.121124
iteration 4383 : model1 loss : 0.139927 model2 loss : 0.131693
iteration 4384 : model1 loss : 0.159264 model2 loss : 0.136657
iteration 4385 : model1 loss : 0.172073 model2 loss : 0.149998
iteration 4386 : model1 loss : 0.185853 model2 loss : 0.152476
 44%|███████████▊               | 258/589 [1:44:56<2:06:28, 22.93s/it]iteration 4387 : model1 loss : 0.192411 model2 loss : 0.139595
iteration 4388 : model1 loss : 0.133464 model2 loss : 0.107477
iteration 4389 : model1 loss : 0.180037 model2 loss : 0.152661
iteration 4390 : model1 loss : 0.173905 model2 loss : 0.153539
iteration 4391 : model1 loss : 0.157758 model2 loss : 0.156771
iteration 4392 : model1 loss : 0.145372 model2 loss : 0.129534
iteration 4393 : model1 loss : 0.227642 model2 loss : 0.176990
iteration 4394 : model1 loss : 0.145372 model2 loss : 0.120690
iteration 4395 : model1 loss : 0.116766 model2 loss : 0.123927
iteration 4396 : model1 loss : 0.208121 model2 loss : 0.214493
iteration 4397 : model1 loss : 0.163912 model2 loss : 0.105927
iteration 4398 : model1 loss : 0.128215 model2 loss : 0.108609
iteration 4399 : model1 loss : 0.150735 model2 loss : 0.132713
iteration 4400 : model1 loss : 0.151113 model2 loss : 0.116421
iteration 4400 : model1_mean_dice : 0.656708 model1_mean_hd95 : 91.640344 model1_mean_iou : 0.530633
iteration 4400 : model2_mean_dice : 0.767258 model2_mean_hd95 : 63.933526 model2_mean_iou : 0.660482
iteration 4401 : model1 loss : 0.161908 model2 loss : 0.122798
iteration 4402 : model1 loss : 0.191208 model2 loss : 0.176119
iteration 4403 : model1 loss : 0.167107 model2 loss : 0.151346
 44%|███████████▊               | 259/589 [1:45:38<2:38:21, 28.79s/it]iteration 4404 : model1 loss : 0.194524 model2 loss : 0.129539
iteration 4405 : model1 loss : 0.162934 model2 loss : 0.168853
iteration 4406 : model1 loss : 0.213764 model2 loss : 0.167685
iteration 4407 : model1 loss : 0.155213 model2 loss : 0.146625
iteration 4408 : model1 loss : 0.156206 model2 loss : 0.158365
iteration 4409 : model1 loss : 0.171449 model2 loss : 0.191755
iteration 4410 : model1 loss : 0.136138 model2 loss : 0.118454
iteration 4411 : model1 loss : 0.153005 model2 loss : 0.125891
iteration 4412 : model1 loss : 0.129536 model2 loss : 0.110630
iteration 4413 : model1 loss : 0.155965 model2 loss : 0.164679
iteration 4414 : model1 loss : 0.182464 model2 loss : 0.190186
iteration 4415 : model1 loss : 0.123809 model2 loss : 0.133783
iteration 4416 : model1 loss : 0.184893 model2 loss : 0.153863
iteration 4417 : model1 loss : 0.140663 model2 loss : 0.118911
iteration 4418 : model1 loss : 0.145088 model2 loss : 0.109493
iteration 4419 : model1 loss : 0.174056 model2 loss : 0.162094
iteration 4420 : model1 loss : 0.117612 model2 loss : 0.112292
 44%|███████████▉               | 260/589 [1:46:01<2:27:57, 26.98s/it]iteration 4421 : model1 loss : 0.134751 model2 loss : 0.099332
iteration 4422 : model1 loss : 0.174808 model2 loss : 0.145936
iteration 4423 : model1 loss : 0.136075 model2 loss : 0.156336
iteration 4424 : model1 loss : 0.160919 model2 loss : 0.135538
iteration 4425 : model1 loss : 0.139518 model2 loss : 0.165010
iteration 4426 : model1 loss : 0.157312 model2 loss : 0.176137
iteration 4427 : model1 loss : 0.202366 model2 loss : 0.146660
iteration 4428 : model1 loss : 0.184562 model2 loss : 0.134803
iteration 4429 : model1 loss : 0.159802 model2 loss : 0.133912
iteration 4430 : model1 loss : 0.156956 model2 loss : 0.134433
iteration 4431 : model1 loss : 0.139342 model2 loss : 0.133285
iteration 4432 : model1 loss : 0.137809 model2 loss : 0.110074
iteration 4433 : model1 loss : 0.142519 model2 loss : 0.147282
iteration 4434 : model1 loss : 0.185948 model2 loss : 0.183698
iteration 4435 : model1 loss : 0.142492 model2 loss : 0.095160
iteration 4436 : model1 loss : 0.146727 model2 loss : 0.125118
iteration 4437 : model1 loss : 0.171643 model2 loss : 0.152669
 44%|███████████▉               | 261/589 [1:46:24<2:20:32, 25.71s/it]iteration 4438 : model1 loss : 0.148658 model2 loss : 0.126245
iteration 4439 : model1 loss : 0.133892 model2 loss : 0.116134
iteration 4440 : model1 loss : 0.187967 model2 loss : 0.139673
iteration 4441 : model1 loss : 0.224287 model2 loss : 0.179893
iteration 4442 : model1 loss : 0.134229 model2 loss : 0.102622
iteration 4443 : model1 loss : 0.131403 model2 loss : 0.127196
iteration 4444 : model1 loss : 0.206369 model2 loss : 0.149827
iteration 4445 : model1 loss : 0.196971 model2 loss : 0.180093
iteration 4446 : model1 loss : 0.144039 model2 loss : 0.114793
iteration 4447 : model1 loss : 0.216786 model2 loss : 0.154794
iteration 4448 : model1 loss : 0.152271 model2 loss : 0.142270
iteration 4449 : model1 loss : 0.133906 model2 loss : 0.129780
iteration 4450 : model1 loss : 0.147114 model2 loss : 0.121382
iteration 4451 : model1 loss : 0.160037 model2 loss : 0.155929
iteration 4452 : model1 loss : 0.175740 model2 loss : 0.122945
iteration 4453 : model1 loss : 0.140344 model2 loss : 0.173852
iteration 4454 : model1 loss : 0.168361 model2 loss : 0.138873
 44%|████████████               | 262/589 [1:46:46<2:15:29, 24.86s/it]iteration 4455 : model1 loss : 0.155524 model2 loss : 0.103273
iteration 4456 : model1 loss : 0.155755 model2 loss : 0.146154
iteration 4457 : model1 loss : 0.204349 model2 loss : 0.174818
iteration 4458 : model1 loss : 0.172052 model2 loss : 0.137935
iteration 4459 : model1 loss : 0.152045 model2 loss : 0.127739
iteration 4460 : model1 loss : 0.147086 model2 loss : 0.113977
iteration 4461 : model1 loss : 0.148020 model2 loss : 0.121333
iteration 4462 : model1 loss : 0.148863 model2 loss : 0.130384
iteration 4463 : model1 loss : 0.198916 model2 loss : 0.143480
iteration 4464 : model1 loss : 0.144077 model2 loss : 0.136316
iteration 4465 : model1 loss : 0.193193 model2 loss : 0.125856
iteration 4466 : model1 loss : 0.175058 model2 loss : 0.181545
iteration 4467 : model1 loss : 0.130701 model2 loss : 0.130662
iteration 4468 : model1 loss : 0.159474 model2 loss : 0.147837
iteration 4469 : model1 loss : 0.163751 model2 loss : 0.131852
iteration 4470 : model1 loss : 0.158361 model2 loss : 0.148940
iteration 4471 : model1 loss : 0.145193 model2 loss : 0.144141
 45%|████████████               | 263/589 [1:47:09<2:11:29, 24.20s/it]iteration 4472 : model1 loss : 0.179126 model2 loss : 0.122082
iteration 4473 : model1 loss : 0.147880 model2 loss : 0.113830
iteration 4474 : model1 loss : 0.193710 model2 loss : 0.139143
iteration 4475 : model1 loss : 0.166162 model2 loss : 0.129795
iteration 4476 : model1 loss : 0.118143 model2 loss : 0.096974
iteration 4477 : model1 loss : 0.153351 model2 loss : 0.125324
iteration 4478 : model1 loss : 0.186188 model2 loss : 0.141773
iteration 4479 : model1 loss : 0.184394 model2 loss : 0.200518
iteration 4480 : model1 loss : 0.128293 model2 loss : 0.150690
iteration 4481 : model1 loss : 0.196298 model2 loss : 0.180725
iteration 4482 : model1 loss : 0.127336 model2 loss : 0.117929
iteration 4483 : model1 loss : 0.147071 model2 loss : 0.143684
iteration 4484 : model1 loss : 0.135118 model2 loss : 0.094724
iteration 4485 : model1 loss : 0.129063 model2 loss : 0.115893
iteration 4486 : model1 loss : 0.172012 model2 loss : 0.131677
iteration 4487 : model1 loss : 0.200156 model2 loss : 0.174070
iteration 4488 : model1 loss : 0.215224 model2 loss : 0.187607
 45%|████████████               | 264/589 [1:47:32<2:08:45, 23.77s/it]iteration 4489 : model1 loss : 0.165540 model2 loss : 0.181369
iteration 4490 : model1 loss : 0.199951 model2 loss : 0.158964
iteration 4491 : model1 loss : 0.137295 model2 loss : 0.162385
iteration 4492 : model1 loss : 0.145912 model2 loss : 0.104596
iteration 4493 : model1 loss : 0.152084 model2 loss : 0.169606
iteration 4494 : model1 loss : 0.125881 model2 loss : 0.136617
iteration 4495 : model1 loss : 0.178510 model2 loss : 0.149060
iteration 4496 : model1 loss : 0.120102 model2 loss : 0.105656
iteration 4497 : model1 loss : 0.121109 model2 loss : 0.124305
iteration 4498 : model1 loss : 0.165004 model2 loss : 0.166425
iteration 4499 : model1 loss : 0.220448 model2 loss : 0.153028
iteration 4500 : model1 loss : 0.166862 model2 loss : 0.102770
iteration 4501 : model1 loss : 0.136049 model2 loss : 0.110918
iteration 4502 : model1 loss : 0.201909 model2 loss : 0.175552
iteration 4503 : model1 loss : 0.147330 model2 loss : 0.136325
iteration 4504 : model1 loss : 0.179409 model2 loss : 0.169184
iteration 4505 : model1 loss : 0.198669 model2 loss : 0.111341
 45%|████████████▏              | 265/589 [1:47:55<2:06:50, 23.49s/it]iteration 4506 : model1 loss : 0.151998 model2 loss : 0.141182
iteration 4507 : model1 loss : 0.146731 model2 loss : 0.123276
iteration 4508 : model1 loss : 0.185814 model2 loss : 0.167253
iteration 4509 : model1 loss : 0.206137 model2 loss : 0.136962
iteration 4510 : model1 loss : 0.170315 model2 loss : 0.130826
iteration 4511 : model1 loss : 0.133662 model2 loss : 0.127391
iteration 4512 : model1 loss : 0.179975 model2 loss : 0.139266
iteration 4513 : model1 loss : 0.155525 model2 loss : 0.137420
iteration 4514 : model1 loss : 0.161153 model2 loss : 0.147700
iteration 4515 : model1 loss : 0.111270 model2 loss : 0.090224
iteration 4516 : model1 loss : 0.154608 model2 loss : 0.141287
iteration 4517 : model1 loss : 0.169193 model2 loss : 0.192260
iteration 4518 : model1 loss : 0.130330 model2 loss : 0.107917
iteration 4519 : model1 loss : 0.186593 model2 loss : 0.173129
iteration 4520 : model1 loss : 0.151567 model2 loss : 0.115672
iteration 4521 : model1 loss : 0.152903 model2 loss : 0.188964
iteration 4522 : model1 loss : 0.124493 model2 loss : 0.119660
 45%|████████████▏              | 266/589 [1:48:17<2:05:14, 23.26s/it]iteration 4523 : model1 loss : 0.152485 model2 loss : 0.131469
iteration 4524 : model1 loss : 0.141495 model2 loss : 0.139302
iteration 4525 : model1 loss : 0.161628 model2 loss : 0.143317
iteration 4526 : model1 loss : 0.141974 model2 loss : 0.095393
iteration 4527 : model1 loss : 0.187817 model2 loss : 0.139615
iteration 4528 : model1 loss : 0.131797 model2 loss : 0.128693
iteration 4529 : model1 loss : 0.152644 model2 loss : 0.138365
iteration 4530 : model1 loss : 0.174692 model2 loss : 0.163832
iteration 4531 : model1 loss : 0.160723 model2 loss : 0.141203
iteration 4532 : model1 loss : 0.161229 model2 loss : 0.170471
iteration 4533 : model1 loss : 0.179324 model2 loss : 0.128071
iteration 4534 : model1 loss : 0.161331 model2 loss : 0.127975
iteration 4535 : model1 loss : 0.152205 model2 loss : 0.112293
iteration 4536 : model1 loss : 0.179602 model2 loss : 0.110150
iteration 4537 : model1 loss : 0.201605 model2 loss : 0.155904
iteration 4538 : model1 loss : 0.165187 model2 loss : 0.123372
iteration 4539 : model1 loss : 0.125259 model2 loss : 0.095659
 45%|████████████▏              | 267/589 [1:48:40<2:03:57, 23.10s/it]iteration 4540 : model1 loss : 0.200233 model2 loss : 0.194751
iteration 4541 : model1 loss : 0.157365 model2 loss : 0.114810
iteration 4542 : model1 loss : 0.188159 model2 loss : 0.142827
iteration 4543 : model1 loss : 0.116817 model2 loss : 0.143228
iteration 4544 : model1 loss : 0.137708 model2 loss : 0.124547
iteration 4545 : model1 loss : 0.156374 model2 loss : 0.162112
iteration 4546 : model1 loss : 0.162420 model2 loss : 0.151675
iteration 4547 : model1 loss : 0.169705 model2 loss : 0.128426
iteration 4548 : model1 loss : 0.205060 model2 loss : 0.175999
iteration 4549 : model1 loss : 0.140155 model2 loss : 0.161408
iteration 4550 : model1 loss : 0.136924 model2 loss : 0.132854
iteration 4551 : model1 loss : 0.158892 model2 loss : 0.108845
iteration 4552 : model1 loss : 0.185638 model2 loss : 0.124979
iteration 4553 : model1 loss : 0.140255 model2 loss : 0.113558
iteration 4554 : model1 loss : 0.172147 model2 loss : 0.127393
iteration 4555 : model1 loss : 0.152460 model2 loss : 0.142977
iteration 4556 : model1 loss : 0.140736 model2 loss : 0.104002
 46%|████████████▎              | 268/589 [1:49:03<2:03:08, 23.02s/it]iteration 4557 : model1 loss : 0.163245 model2 loss : 0.184758
iteration 4558 : model1 loss : 0.119592 model2 loss : 0.096565
iteration 4559 : model1 loss : 0.221099 model2 loss : 0.146590
iteration 4560 : model1 loss : 0.143731 model2 loss : 0.126329
iteration 4561 : model1 loss : 0.137201 model2 loss : 0.139735
iteration 4562 : model1 loss : 0.139188 model2 loss : 0.157433
iteration 4563 : model1 loss : 0.195998 model2 loss : 0.136085
iteration 4564 : model1 loss : 0.161010 model2 loss : 0.153932
iteration 4565 : model1 loss : 0.128648 model2 loss : 0.096442
iteration 4566 : model1 loss : 0.158550 model2 loss : 0.123354
iteration 4567 : model1 loss : 0.172829 model2 loss : 0.167299
iteration 4568 : model1 loss : 0.130316 model2 loss : 0.142042
iteration 4569 : model1 loss : 0.137641 model2 loss : 0.119160
iteration 4570 : model1 loss : 0.195023 model2 loss : 0.206741
iteration 4571 : model1 loss : 0.179675 model2 loss : 0.176475
iteration 4572 : model1 loss : 0.148464 model2 loss : 0.117772
iteration 4573 : model1 loss : 0.167159 model2 loss : 0.129741
 46%|████████████▎              | 269/589 [1:49:26<2:02:26, 22.96s/it]iteration 4574 : model1 loss : 0.154750 model2 loss : 0.156741
iteration 4575 : model1 loss : 0.184648 model2 loss : 0.148964
iteration 4576 : model1 loss : 0.195115 model2 loss : 0.244136
iteration 4577 : model1 loss : 0.112687 model2 loss : 0.108085
iteration 4578 : model1 loss : 0.143374 model2 loss : 0.122387
iteration 4579 : model1 loss : 0.212858 model2 loss : 0.130961
iteration 4580 : model1 loss : 0.147684 model2 loss : 0.122513
iteration 4581 : model1 loss : 0.165185 model2 loss : 0.158053
iteration 4582 : model1 loss : 0.149417 model2 loss : 0.141226
iteration 4583 : model1 loss : 0.142105 model2 loss : 0.145099
iteration 4584 : model1 loss : 0.168537 model2 loss : 0.129509
iteration 4585 : model1 loss : 0.136728 model2 loss : 0.164321
iteration 4586 : model1 loss : 0.199174 model2 loss : 0.133962
iteration 4587 : model1 loss : 0.174568 model2 loss : 0.148294
iteration 4588 : model1 loss : 0.127545 model2 loss : 0.101170
iteration 4589 : model1 loss : 0.114436 model2 loss : 0.114376
iteration 4590 : model1 loss : 0.184186 model2 loss : 0.179673
 46%|████████████▍              | 270/589 [1:49:49<2:01:45, 22.90s/it]iteration 4591 : model1 loss : 0.127717 model2 loss : 0.112765
iteration 4592 : model1 loss : 0.148477 model2 loss : 0.168208
iteration 4593 : model1 loss : 0.120069 model2 loss : 0.108341
iteration 4594 : model1 loss : 0.179852 model2 loss : 0.145515
iteration 4595 : model1 loss : 0.160662 model2 loss : 0.238701
iteration 4596 : model1 loss : 0.163607 model2 loss : 0.139445
iteration 4597 : model1 loss : 0.216480 model2 loss : 0.246105
iteration 4598 : model1 loss : 0.159498 model2 loss : 0.128207
iteration 4599 : model1 loss : 0.192108 model2 loss : 0.155734
iteration 4600 : model1 loss : 0.126687 model2 loss : 0.119659
iteration 4600 : model1_mean_dice : 0.696064 model1_mean_hd95 : 92.722883 model1_mean_iou : 0.570247
iteration 4600 : model2_mean_dice : 0.769664 model2_mean_hd95 : 76.613377 model2_mean_iou : 0.659401
iteration 4601 : model1 loss : 0.206265 model2 loss : 0.231030
iteration 4602 : model1 loss : 0.113443 model2 loss : 0.141104
iteration 4603 : model1 loss : 0.160158 model2 loss : 0.209316
iteration 4604 : model1 loss : 0.160353 model2 loss : 0.128661
iteration 4605 : model1 loss : 0.184242 model2 loss : 0.186417
iteration 4606 : model1 loss : 0.189508 model2 loss : 0.152742
iteration 4607 : model1 loss : 0.136811 model2 loss : 0.130230
 46%|████████████▍              | 271/589 [1:50:32<2:33:27, 28.96s/it]iteration 4608 : model1 loss : 0.129350 model2 loss : 0.135965
iteration 4609 : model1 loss : 0.139192 model2 loss : 0.125436
iteration 4610 : model1 loss : 0.192811 model2 loss : 0.169711
iteration 4611 : model1 loss : 0.143185 model2 loss : 0.113682
iteration 4612 : model1 loss : 0.225719 model2 loss : 0.148425
iteration 4613 : model1 loss : 0.203952 model2 loss : 0.146570
iteration 4614 : model1 loss : 0.157560 model2 loss : 0.171581
iteration 4615 : model1 loss : 0.134055 model2 loss : 0.103737
iteration 4616 : model1 loss : 0.152581 model2 loss : 0.115270
iteration 4617 : model1 loss : 0.142337 model2 loss : 0.124554
iteration 4618 : model1 loss : 0.159392 model2 loss : 0.155814
iteration 4619 : model1 loss : 0.156361 model2 loss : 0.139239
iteration 4620 : model1 loss : 0.138452 model2 loss : 0.122096
iteration 4621 : model1 loss : 0.154821 model2 loss : 0.133383
iteration 4622 : model1 loss : 0.131577 model2 loss : 0.137359
iteration 4623 : model1 loss : 0.158638 model2 loss : 0.133909
iteration 4624 : model1 loss : 0.179947 model2 loss : 0.127914
 46%|████████████▍              | 272/589 [1:50:54<2:23:09, 27.10s/it]iteration 4625 : model1 loss : 0.148262 model2 loss : 0.188456
iteration 4626 : model1 loss : 0.168167 model2 loss : 0.128121
iteration 4627 : model1 loss : 0.102182 model2 loss : 0.083533
iteration 4628 : model1 loss : 0.199443 model2 loss : 0.170451
iteration 4629 : model1 loss : 0.153437 model2 loss : 0.137683
iteration 4630 : model1 loss : 0.160867 model2 loss : 0.135077
iteration 4631 : model1 loss : 0.178123 model2 loss : 0.175702
iteration 4632 : model1 loss : 0.137292 model2 loss : 0.101039
iteration 4633 : model1 loss : 0.160176 model2 loss : 0.170267
iteration 4634 : model1 loss : 0.102190 model2 loss : 0.119412
iteration 4635 : model1 loss : 0.173553 model2 loss : 0.177047
iteration 4636 : model1 loss : 0.151953 model2 loss : 0.131318
iteration 4637 : model1 loss : 0.167519 model2 loss : 0.144708
iteration 4638 : model1 loss : 0.150154 model2 loss : 0.172803
iteration 4639 : model1 loss : 0.153024 model2 loss : 0.110781
iteration 4640 : model1 loss : 0.140731 model2 loss : 0.163796
iteration 4641 : model1 loss : 0.147638 model2 loss : 0.110167
 46%|████████████▌              | 273/589 [1:51:17<2:15:49, 25.79s/it]iteration 4642 : model1 loss : 0.122910 model2 loss : 0.106946
iteration 4643 : model1 loss : 0.126339 model2 loss : 0.145991
iteration 4644 : model1 loss : 0.177162 model2 loss : 0.119471
iteration 4645 : model1 loss : 0.165671 model2 loss : 0.183827
iteration 4646 : model1 loss : 0.125844 model2 loss : 0.110812
iteration 4647 : model1 loss : 0.168190 model2 loss : 0.168768
iteration 4648 : model1 loss : 0.135741 model2 loss : 0.116673
iteration 4649 : model1 loss : 0.184219 model2 loss : 0.171530
iteration 4650 : model1 loss : 0.178059 model2 loss : 0.161058
iteration 4651 : model1 loss : 0.150516 model2 loss : 0.159901
iteration 4652 : model1 loss : 0.201830 model2 loss : 0.170841
iteration 4653 : model1 loss : 0.158846 model2 loss : 0.138027
iteration 4654 : model1 loss : 0.115055 model2 loss : 0.143944
iteration 4655 : model1 loss : 0.136079 model2 loss : 0.125597
iteration 4656 : model1 loss : 0.154956 model2 loss : 0.114062
iteration 4657 : model1 loss : 0.156885 model2 loss : 0.181498
iteration 4658 : model1 loss : 0.167928 model2 loss : 0.111316
 47%|████████████▌              | 274/589 [1:51:40<2:10:45, 24.91s/it]iteration 4659 : model1 loss : 0.141212 model2 loss : 0.119060
iteration 4660 : model1 loss : 0.176462 model2 loss : 0.154103
iteration 4661 : model1 loss : 0.138252 model2 loss : 0.105197
iteration 4662 : model1 loss : 0.135777 model2 loss : 0.100662
iteration 4663 : model1 loss : 0.129263 model2 loss : 0.134217
iteration 4664 : model1 loss : 0.144748 model2 loss : 0.152827
iteration 4665 : model1 loss : 0.212445 model2 loss : 0.165435
iteration 4666 : model1 loss : 0.128467 model2 loss : 0.108675
iteration 4667 : model1 loss : 0.136375 model2 loss : 0.105754
iteration 4668 : model1 loss : 0.183595 model2 loss : 0.131303
iteration 4669 : model1 loss : 0.158145 model2 loss : 0.124589
iteration 4670 : model1 loss : 0.145287 model2 loss : 0.182540
iteration 4671 : model1 loss : 0.173279 model2 loss : 0.145265
iteration 4672 : model1 loss : 0.167308 model2 loss : 0.127919
iteration 4673 : model1 loss : 0.152670 model2 loss : 0.139319
iteration 4674 : model1 loss : 0.168204 model2 loss : 0.150576
iteration 4675 : model1 loss : 0.165223 model2 loss : 0.151222
 47%|████████████▌              | 275/589 [1:52:03<2:06:58, 24.26s/it]iteration 4676 : model1 loss : 0.155365 model2 loss : 0.108756
iteration 4677 : model1 loss : 0.223375 model2 loss : 0.168812
iteration 4678 : model1 loss : 0.116132 model2 loss : 0.129210
iteration 4679 : model1 loss : 0.158313 model2 loss : 0.112596
iteration 4680 : model1 loss : 0.142309 model2 loss : 0.130430
iteration 4681 : model1 loss : 0.160862 model2 loss : 0.155264
iteration 4682 : model1 loss : 0.144638 model2 loss : 0.134639
iteration 4683 : model1 loss : 0.144820 model2 loss : 0.131525
iteration 4684 : model1 loss : 0.136400 model2 loss : 0.126578
iteration 4685 : model1 loss : 0.140726 model2 loss : 0.118488
iteration 4686 : model1 loss : 0.177941 model2 loss : 0.145441
iteration 4687 : model1 loss : 0.181912 model2 loss : 0.170471
iteration 4688 : model1 loss : 0.158794 model2 loss : 0.163370
iteration 4689 : model1 loss : 0.181233 model2 loss : 0.107631
iteration 4690 : model1 loss : 0.154128 model2 loss : 0.123081
iteration 4691 : model1 loss : 0.164946 model2 loss : 0.152851
iteration 4692 : model1 loss : 0.131192 model2 loss : 0.124130
 47%|████████████▋              | 276/589 [1:52:25<2:04:12, 23.81s/it]iteration 4693 : model1 loss : 0.244638 model2 loss : 0.227649
iteration 4694 : model1 loss : 0.144907 model2 loss : 0.128599
iteration 4695 : model1 loss : 0.135124 model2 loss : 0.102161
iteration 4696 : model1 loss : 0.185267 model2 loss : 0.142839
iteration 4697 : model1 loss : 0.157516 model2 loss : 0.145207
iteration 4698 : model1 loss : 0.148313 model2 loss : 0.141178
iteration 4699 : model1 loss : 0.114808 model2 loss : 0.136969
iteration 4700 : model1 loss : 0.155784 model2 loss : 0.124468
iteration 4701 : model1 loss : 0.132271 model2 loss : 0.092749
iteration 4702 : model1 loss : 0.140335 model2 loss : 0.101887
iteration 4703 : model1 loss : 0.159216 model2 loss : 0.133926
iteration 4704 : model1 loss : 0.159775 model2 loss : 0.156889
iteration 4705 : model1 loss : 0.168336 model2 loss : 0.131041
iteration 4706 : model1 loss : 0.135400 model2 loss : 0.145207
iteration 4707 : model1 loss : 0.171092 model2 loss : 0.143551
iteration 4708 : model1 loss : 0.201686 model2 loss : 0.110607
iteration 4709 : model1 loss : 0.119868 model2 loss : 0.104732
 47%|████████████▋              | 277/589 [1:52:48<2:02:19, 23.53s/it]iteration 4710 : model1 loss : 0.178896 model2 loss : 0.177082
iteration 4711 : model1 loss : 0.127084 model2 loss : 0.120006
iteration 4712 : model1 loss : 0.155191 model2 loss : 0.155768
iteration 4713 : model1 loss : 0.154094 model2 loss : 0.153127
iteration 4714 : model1 loss : 0.158262 model2 loss : 0.158601
iteration 4715 : model1 loss : 0.160008 model2 loss : 0.135643
iteration 4716 : model1 loss : 0.200495 model2 loss : 0.130919
iteration 4717 : model1 loss : 0.130025 model2 loss : 0.154849
iteration 4718 : model1 loss : 0.171557 model2 loss : 0.144178
iteration 4719 : model1 loss : 0.144863 model2 loss : 0.138415
iteration 4720 : model1 loss : 0.177100 model2 loss : 0.122603
iteration 4721 : model1 loss : 0.155902 model2 loss : 0.118303
iteration 4722 : model1 loss : 0.150236 model2 loss : 0.126996
iteration 4723 : model1 loss : 0.152086 model2 loss : 0.160010
iteration 4724 : model1 loss : 0.117502 model2 loss : 0.087238
iteration 4725 : model1 loss : 0.161592 model2 loss : 0.118866
iteration 4726 : model1 loss : 0.149741 model2 loss : 0.148959
 47%|████████████▋              | 278/589 [1:53:11<2:00:48, 23.31s/it]iteration 4727 : model1 loss : 0.148917 model2 loss : 0.138832
iteration 4728 : model1 loss : 0.168260 model2 loss : 0.158269
iteration 4729 : model1 loss : 0.154121 model2 loss : 0.130278
iteration 4730 : model1 loss : 0.154132 model2 loss : 0.118002
iteration 4731 : model1 loss : 0.151211 model2 loss : 0.147368
iteration 4732 : model1 loss : 0.144698 model2 loss : 0.152724
iteration 4733 : model1 loss : 0.161463 model2 loss : 0.142752
iteration 4734 : model1 loss : 0.151577 model2 loss : 0.176013
iteration 4735 : model1 loss : 0.118442 model2 loss : 0.127021
iteration 4736 : model1 loss : 0.145248 model2 loss : 0.140574
iteration 4737 : model1 loss : 0.157255 model2 loss : 0.128230
iteration 4738 : model1 loss : 0.132752 model2 loss : 0.107393
iteration 4739 : model1 loss : 0.188063 model2 loss : 0.157296
iteration 4740 : model1 loss : 0.172629 model2 loss : 0.158641
iteration 4741 : model1 loss : 0.145851 model2 loss : 0.136411
iteration 4742 : model1 loss : 0.117289 model2 loss : 0.114777
iteration 4743 : model1 loss : 0.191308 model2 loss : 0.134888
 47%|████████████▊              | 279/589 [1:53:34<1:59:33, 23.14s/it]iteration 4744 : model1 loss : 0.142510 model2 loss : 0.142272
iteration 4745 : model1 loss : 0.142540 model2 loss : 0.129867
iteration 4746 : model1 loss : 0.135732 model2 loss : 0.125621
iteration 4747 : model1 loss : 0.149525 model2 loss : 0.115022
iteration 4748 : model1 loss : 0.112548 model2 loss : 0.100534
iteration 4749 : model1 loss : 0.141951 model2 loss : 0.179018
iteration 4750 : model1 loss : 0.173650 model2 loss : 0.113243
iteration 4751 : model1 loss : 0.180680 model2 loss : 0.150447
iteration 4752 : model1 loss : 0.186932 model2 loss : 0.137438
iteration 4753 : model1 loss : 0.171621 model2 loss : 0.174731
iteration 4754 : model1 loss : 0.164238 model2 loss : 0.129806
iteration 4755 : model1 loss : 0.183232 model2 loss : 0.162966
iteration 4756 : model1 loss : 0.151244 model2 loss : 0.127025
iteration 4757 : model1 loss : 0.136731 model2 loss : 0.117285
iteration 4758 : model1 loss : 0.142847 model2 loss : 0.177338
iteration 4759 : model1 loss : 0.174374 model2 loss : 0.202511
iteration 4760 : model1 loss : 0.167016 model2 loss : 0.134203
 48%|████████████▊              | 280/589 [1:53:57<1:58:44, 23.06s/it]iteration 4761 : model1 loss : 0.144484 model2 loss : 0.107464
iteration 4762 : model1 loss : 0.131608 model2 loss : 0.115091
iteration 4763 : model1 loss : 0.187952 model2 loss : 0.130569
iteration 4764 : model1 loss : 0.175062 model2 loss : 0.145784
iteration 4765 : model1 loss : 0.132991 model2 loss : 0.130692
iteration 4766 : model1 loss : 0.151178 model2 loss : 0.115697
iteration 4767 : model1 loss : 0.159340 model2 loss : 0.129933
iteration 4768 : model1 loss : 0.149562 model2 loss : 0.124214
iteration 4769 : model1 loss : 0.132544 model2 loss : 0.102625
iteration 4770 : model1 loss : 0.164563 model2 loss : 0.167768
iteration 4771 : model1 loss : 0.165220 model2 loss : 0.145263
iteration 4772 : model1 loss : 0.117886 model2 loss : 0.104153
iteration 4773 : model1 loss : 0.168836 model2 loss : 0.132933
iteration 4774 : model1 loss : 0.172214 model2 loss : 0.171185
iteration 4775 : model1 loss : 0.211768 model2 loss : 0.161285
iteration 4776 : model1 loss : 0.121278 model2 loss : 0.146611
iteration 4777 : model1 loss : 0.169622 model2 loss : 0.167946
 48%|████████████▉              | 281/589 [1:54:20<1:57:55, 22.97s/it]iteration 4778 : model1 loss : 0.153505 model2 loss : 0.145313
iteration 4779 : model1 loss : 0.185519 model2 loss : 0.159998
iteration 4780 : model1 loss : 0.155783 model2 loss : 0.156125
iteration 4781 : model1 loss : 0.153038 model2 loss : 0.108670
iteration 4782 : model1 loss : 0.136943 model2 loss : 0.108751
iteration 4783 : model1 loss : 0.152949 model2 loss : 0.126502
iteration 4784 : model1 loss : 0.196846 model2 loss : 0.137878
iteration 4785 : model1 loss : 0.151211 model2 loss : 0.138178
iteration 4786 : model1 loss : 0.189593 model2 loss : 0.180374
iteration 4787 : model1 loss : 0.158256 model2 loss : 0.162657
iteration 4788 : model1 loss : 0.175093 model2 loss : 0.168553
iteration 4789 : model1 loss : 0.166445 model2 loss : 0.103113
iteration 4790 : model1 loss : 0.126054 model2 loss : 0.123012
iteration 4791 : model1 loss : 0.118640 model2 loss : 0.127366
iteration 4792 : model1 loss : 0.159551 model2 loss : 0.124835
iteration 4793 : model1 loss : 0.181567 model2 loss : 0.116472
iteration 4794 : model1 loss : 0.151005 model2 loss : 0.133547
 48%|████████████▉              | 282/589 [1:54:42<1:57:12, 22.91s/it]iteration 4795 : model1 loss : 0.156155 model2 loss : 0.136793
iteration 4796 : model1 loss : 0.192019 model2 loss : 0.131207
iteration 4797 : model1 loss : 0.148394 model2 loss : 0.123313
iteration 4798 : model1 loss : 0.189988 model2 loss : 0.184908
iteration 4799 : model1 loss : 0.135745 model2 loss : 0.114502
iteration 4800 : model1 loss : 0.179156 model2 loss : 0.142678
iteration 4800 : model1_mean_dice : 0.720157 model1_mean_hd95 : 82.009773 model1_mean_iou : 0.597722
iteration 4800 : model2_mean_dice : 0.745578 model2_mean_hd95 : 66.038400 model2_mean_iou : 0.633811
iteration 4801 : model1 loss : 0.163962 model2 loss : 0.110578
iteration 4802 : model1 loss : 0.146847 model2 loss : 0.144012
iteration 4803 : model1 loss : 0.125334 model2 loss : 0.122078
iteration 4804 : model1 loss : 0.170148 model2 loss : 0.133091
iteration 4805 : model1 loss : 0.147839 model2 loss : 0.107633
iteration 4806 : model1 loss : 0.138843 model2 loss : 0.172695
iteration 4807 : model1 loss : 0.139495 model2 loss : 0.131232
iteration 4808 : model1 loss : 0.140373 model2 loss : 0.108017
iteration 4809 : model1 loss : 0.193794 model2 loss : 0.136516
iteration 4810 : model1 loss : 0.151724 model2 loss : 0.151500
iteration 4811 : model1 loss : 0.157135 model2 loss : 0.134681
 48%|████████████▉              | 283/589 [1:55:25<2:27:11, 28.86s/it]iteration 4812 : model1 loss : 0.155846 model2 loss : 0.165790
iteration 4813 : model1 loss : 0.148415 model2 loss : 0.154601
iteration 4814 : model1 loss : 0.170556 model2 loss : 0.151327
iteration 4815 : model1 loss : 0.150698 model2 loss : 0.149456
iteration 4816 : model1 loss : 0.156160 model2 loss : 0.133855
iteration 4817 : model1 loss : 0.115651 model2 loss : 0.116394
iteration 4818 : model1 loss : 0.139166 model2 loss : 0.138823
iteration 4819 : model1 loss : 0.124429 model2 loss : 0.147702
iteration 4820 : model1 loss : 0.192132 model2 loss : 0.173924
iteration 4821 : model1 loss : 0.142315 model2 loss : 0.115755
iteration 4822 : model1 loss : 0.118695 model2 loss : 0.112936
iteration 4823 : model1 loss : 0.166603 model2 loss : 0.164025
iteration 4824 : model1 loss : 0.153521 model2 loss : 0.144497
iteration 4825 : model1 loss : 0.194117 model2 loss : 0.134036
iteration 4826 : model1 loss : 0.175497 model2 loss : 0.188738
iteration 4827 : model1 loss : 0.171465 model2 loss : 0.147713
iteration 4828 : model1 loss : 0.150052 model2 loss : 0.115770
 48%|█████████████              | 284/589 [1:55:48<2:17:22, 27.02s/it]iteration 4829 : model1 loss : 0.118570 model2 loss : 0.095177
iteration 4830 : model1 loss : 0.191876 model2 loss : 0.126766
iteration 4831 : model1 loss : 0.140444 model2 loss : 0.098140
iteration 4832 : model1 loss : 0.161927 model2 loss : 0.116200
iteration 4833 : model1 loss : 0.154034 model2 loss : 0.147875
iteration 4834 : model1 loss : 0.177975 model2 loss : 0.132488
iteration 4835 : model1 loss : 0.151168 model2 loss : 0.133947
iteration 4836 : model1 loss : 0.166765 model2 loss : 0.124274
iteration 4837 : model1 loss : 0.131267 model2 loss : 0.141246
iteration 4838 : model1 loss : 0.180305 model2 loss : 0.145256
iteration 4839 : model1 loss : 0.156808 model2 loss : 0.106303
iteration 4840 : model1 loss : 0.189403 model2 loss : 0.187066
iteration 4841 : model1 loss : 0.160443 model2 loss : 0.129231
iteration 4842 : model1 loss : 0.183515 model2 loss : 0.127433
iteration 4843 : model1 loss : 0.131918 model2 loss : 0.091540
iteration 4844 : model1 loss : 0.135667 model2 loss : 0.145614
iteration 4845 : model1 loss : 0.128366 model2 loss : 0.119389
 48%|█████████████              | 285/589 [1:56:11<2:10:28, 25.75s/it]iteration 4846 : model1 loss : 0.126509 model2 loss : 0.101382
iteration 4847 : model1 loss : 0.150499 model2 loss : 0.106387
iteration 4848 : model1 loss : 0.155600 model2 loss : 0.142613
iteration 4849 : model1 loss : 0.158328 model2 loss : 0.167858
iteration 4850 : model1 loss : 0.164973 model2 loss : 0.123961
iteration 4851 : model1 loss : 0.156963 model2 loss : 0.145397
iteration 4852 : model1 loss : 0.145569 model2 loss : 0.121750
iteration 4853 : model1 loss : 0.118760 model2 loss : 0.101934
iteration 4854 : model1 loss : 0.144675 model2 loss : 0.140032
iteration 4855 : model1 loss : 0.134507 model2 loss : 0.131817
iteration 4856 : model1 loss : 0.163751 model2 loss : 0.156159
iteration 4857 : model1 loss : 0.147437 model2 loss : 0.115015
iteration 4858 : model1 loss : 0.145532 model2 loss : 0.155116
iteration 4859 : model1 loss : 0.151165 model2 loss : 0.107535
iteration 4860 : model1 loss : 0.182282 model2 loss : 0.166778
iteration 4861 : model1 loss : 0.199908 model2 loss : 0.149533
iteration 4862 : model1 loss : 0.151487 model2 loss : 0.127587
 49%|█████████████              | 286/589 [1:56:33<2:05:43, 24.90s/it]iteration 4863 : model1 loss : 0.168039 model2 loss : 0.140299
iteration 4864 : model1 loss : 0.142692 model2 loss : 0.175823
iteration 4865 : model1 loss : 0.160048 model2 loss : 0.142287
iteration 4866 : model1 loss : 0.165527 model2 loss : 0.105595
iteration 4867 : model1 loss : 0.130969 model2 loss : 0.127634
iteration 4868 : model1 loss : 0.157526 model2 loss : 0.160183
iteration 4869 : model1 loss : 0.173569 model2 loss : 0.132279
iteration 4870 : model1 loss : 0.144509 model2 loss : 0.128430
iteration 4871 : model1 loss : 0.117948 model2 loss : 0.104450
iteration 4872 : model1 loss : 0.132792 model2 loss : 0.115479
iteration 4873 : model1 loss : 0.134039 model2 loss : 0.096376
iteration 4874 : model1 loss : 0.150460 model2 loss : 0.108142
iteration 4875 : model1 loss : 0.183301 model2 loss : 0.138385
iteration 4876 : model1 loss : 0.181982 model2 loss : 0.128244
iteration 4877 : model1 loss : 0.196654 model2 loss : 0.170113
iteration 4878 : model1 loss : 0.120813 model2 loss : 0.135037
iteration 4879 : model1 loss : 0.126999 model2 loss : 0.102962
 49%|█████████████▏             | 287/589 [1:56:56<2:02:07, 24.26s/it]iteration 4880 : model1 loss : 0.172123 model2 loss : 0.144985
iteration 4881 : model1 loss : 0.128793 model2 loss : 0.132779
iteration 4882 : model1 loss : 0.214443 model2 loss : 0.173246
iteration 4883 : model1 loss : 0.150769 model2 loss : 0.102933
iteration 4884 : model1 loss : 0.130198 model2 loss : 0.139067
iteration 4885 : model1 loss : 0.135140 model2 loss : 0.116111
iteration 4886 : model1 loss : 0.179246 model2 loss : 0.152430
iteration 4887 : model1 loss : 0.154075 model2 loss : 0.167941
iteration 4888 : model1 loss : 0.126939 model2 loss : 0.128866
iteration 4889 : model1 loss : 0.140656 model2 loss : 0.126949
iteration 4890 : model1 loss : 0.129140 model2 loss : 0.108938
iteration 4891 : model1 loss : 0.138937 model2 loss : 0.140015
iteration 4892 : model1 loss : 0.130631 model2 loss : 0.094749
iteration 4893 : model1 loss : 0.194861 model2 loss : 0.127986
iteration 4894 : model1 loss : 0.150768 model2 loss : 0.131607
iteration 4895 : model1 loss : 0.195548 model2 loss : 0.108449
iteration 4896 : model1 loss : 0.164730 model2 loss : 0.149927
 49%|█████████████▏             | 288/589 [1:57:19<1:59:31, 23.82s/it]iteration 4897 : model1 loss : 0.147403 model2 loss : 0.125098
iteration 4898 : model1 loss : 0.137616 model2 loss : 0.121307
iteration 4899 : model1 loss : 0.137404 model2 loss : 0.125433
iteration 4900 : model1 loss : 0.169770 model2 loss : 0.126166
iteration 4901 : model1 loss : 0.153631 model2 loss : 0.127956
iteration 4902 : model1 loss : 0.111683 model2 loss : 0.115673
iteration 4903 : model1 loss : 0.155322 model2 loss : 0.120011
iteration 4904 : model1 loss : 0.149519 model2 loss : 0.129673
iteration 4905 : model1 loss : 0.139557 model2 loss : 0.099143
iteration 4906 : model1 loss : 0.155428 model2 loss : 0.113240
iteration 4907 : model1 loss : 0.124797 model2 loss : 0.127515
iteration 4908 : model1 loss : 0.189603 model2 loss : 0.145957
iteration 4909 : model1 loss : 0.198099 model2 loss : 0.163730
iteration 4910 : model1 loss : 0.189537 model2 loss : 0.126152
iteration 4911 : model1 loss : 0.170618 model2 loss : 0.149690
iteration 4912 : model1 loss : 0.138019 model2 loss : 0.104351
iteration 4913 : model1 loss : 0.118739 model2 loss : 0.115080
 49%|█████████████▏             | 289/589 [1:57:42<1:57:41, 23.54s/it]iteration 4914 : model1 loss : 0.149313 model2 loss : 0.137217
iteration 4915 : model1 loss : 0.146211 model2 loss : 0.111028
iteration 4916 : model1 loss : 0.175731 model2 loss : 0.144824
iteration 4917 : model1 loss : 0.124470 model2 loss : 0.114457
iteration 4918 : model1 loss : 0.132043 model2 loss : 0.181771
iteration 4919 : model1 loss : 0.141090 model2 loss : 0.120832
iteration 4920 : model1 loss : 0.180350 model2 loss : 0.138437
iteration 4921 : model1 loss : 0.173412 model2 loss : 0.138213
iteration 4922 : model1 loss : 0.150757 model2 loss : 0.119536
iteration 4923 : model1 loss : 0.115795 model2 loss : 0.121194
iteration 4924 : model1 loss : 0.191086 model2 loss : 0.175437
iteration 4925 : model1 loss : 0.125596 model2 loss : 0.119316
iteration 4926 : model1 loss : 0.134165 model2 loss : 0.145804
iteration 4927 : model1 loss : 0.176169 model2 loss : 0.114856
iteration 4928 : model1 loss : 0.122199 model2 loss : 0.108375
iteration 4929 : model1 loss : 0.150649 model2 loss : 0.149017
iteration 4930 : model1 loss : 0.160304 model2 loss : 0.117839
 49%|█████████████▎             | 290/589 [1:58:05<1:56:13, 23.32s/it]iteration 4931 : model1 loss : 0.170914 model2 loss : 0.126333
iteration 4932 : model1 loss : 0.145285 model2 loss : 0.116518
iteration 4933 : model1 loss : 0.160648 model2 loss : 0.135889
iteration 4934 : model1 loss : 0.121149 model2 loss : 0.146873
iteration 4935 : model1 loss : 0.143049 model2 loss : 0.120529
iteration 4936 : model1 loss : 0.216939 model2 loss : 0.139499
iteration 4937 : model1 loss : 0.130982 model2 loss : 0.111625
iteration 4938 : model1 loss : 0.123055 model2 loss : 0.112601
iteration 4939 : model1 loss : 0.146758 model2 loss : 0.116387
iteration 4940 : model1 loss : 0.125906 model2 loss : 0.117231
iteration 4941 : model1 loss : 0.174093 model2 loss : 0.159792
iteration 4942 : model1 loss : 0.185034 model2 loss : 0.159833
iteration 4943 : model1 loss : 0.134508 model2 loss : 0.138946
iteration 4944 : model1 loss : 0.175762 model2 loss : 0.210263
iteration 4945 : model1 loss : 0.148254 model2 loss : 0.126699
iteration 4946 : model1 loss : 0.146878 model2 loss : 0.157356
iteration 4947 : model1 loss : 0.152202 model2 loss : 0.121328
 49%|█████████████▎             | 291/589 [1:58:27<1:54:58, 23.15s/it]iteration 4948 : model1 loss : 0.100701 model2 loss : 0.117090
iteration 4949 : model1 loss : 0.167568 model2 loss : 0.123396
iteration 4950 : model1 loss : 0.215139 model2 loss : 0.155391
iteration 4951 : model1 loss : 0.148811 model2 loss : 0.119821
iteration 4952 : model1 loss : 0.104692 model2 loss : 0.099029
iteration 4953 : model1 loss : 0.146380 model2 loss : 0.104077
iteration 4954 : model1 loss : 0.165877 model2 loss : 0.148095
iteration 4955 : model1 loss : 0.165603 model2 loss : 0.101312
iteration 4956 : model1 loss : 0.145490 model2 loss : 0.113349
iteration 4957 : model1 loss : 0.142677 model2 loss : 0.133099
iteration 4958 : model1 loss : 0.168849 model2 loss : 0.112306
iteration 4959 : model1 loss : 0.154366 model2 loss : 0.123638
iteration 4960 : model1 loss : 0.155255 model2 loss : 0.104899
iteration 4961 : model1 loss : 0.171387 model2 loss : 0.133698
iteration 4962 : model1 loss : 0.191740 model2 loss : 0.195057
iteration 4963 : model1 loss : 0.145638 model2 loss : 0.115108
iteration 4964 : model1 loss : 0.169154 model2 loss : 0.161167
 50%|█████████████▍             | 292/589 [1:58:50<1:54:09, 23.06s/it]iteration 4965 : model1 loss : 0.143889 model2 loss : 0.132390
iteration 4966 : model1 loss : 0.156557 model2 loss : 0.119348
iteration 4967 : model1 loss : 0.194706 model2 loss : 0.166491
iteration 4968 : model1 loss : 0.173223 model2 loss : 0.104044
iteration 4969 : model1 loss : 0.143997 model2 loss : 0.131478
iteration 4970 : model1 loss : 0.181952 model2 loss : 0.163882
iteration 4971 : model1 loss : 0.112847 model2 loss : 0.108068
iteration 4972 : model1 loss : 0.166654 model2 loss : 0.126060
iteration 4973 : model1 loss : 0.174880 model2 loss : 0.115038
iteration 4974 : model1 loss : 0.205532 model2 loss : 0.167778
iteration 4975 : model1 loss : 0.152071 model2 loss : 0.127789
iteration 4976 : model1 loss : 0.130727 model2 loss : 0.103368
iteration 4977 : model1 loss : 0.123121 model2 loss : 0.133193
iteration 4978 : model1 loss : 0.172542 model2 loss : 0.124620
iteration 4979 : model1 loss : 0.147417 model2 loss : 0.127373
iteration 4980 : model1 loss : 0.124508 model2 loss : 0.137361
iteration 4981 : model1 loss : 0.146264 model2 loss : 0.131341
 50%|█████████████▍             | 293/589 [1:59:13<1:53:20, 22.98s/it]iteration 4982 : model1 loss : 0.167356 model2 loss : 0.164393
iteration 4983 : model1 loss : 0.231434 model2 loss : 0.162705
iteration 4984 : model1 loss : 0.127151 model2 loss : 0.105435
iteration 4985 : model1 loss : 0.113617 model2 loss : 0.117738
iteration 4986 : model1 loss : 0.120731 model2 loss : 0.126590
iteration 4987 : model1 loss : 0.172268 model2 loss : 0.138268
iteration 4988 : model1 loss : 0.159776 model2 loss : 0.116066
iteration 4989 : model1 loss : 0.150700 model2 loss : 0.115101
iteration 4990 : model1 loss : 0.135022 model2 loss : 0.114560
iteration 4991 : model1 loss : 0.116102 model2 loss : 0.106269
iteration 4992 : model1 loss : 0.137105 model2 loss : 0.116644
iteration 4993 : model1 loss : 0.143584 model2 loss : 0.152936
iteration 4994 : model1 loss : 0.137199 model2 loss : 0.130214
iteration 4995 : model1 loss : 0.152681 model2 loss : 0.129602
iteration 4996 : model1 loss : 0.158917 model2 loss : 0.158620
iteration 4997 : model1 loss : 0.180748 model2 loss : 0.129636
iteration 4998 : model1 loss : 0.196962 model2 loss : 0.132314
 50%|█████████████▍             | 294/589 [1:59:36<1:52:40, 22.92s/it]iteration 4999 : model1 loss : 0.147193 model2 loss : 0.106517
iteration 5000 : model1 loss : 0.159915 model2 loss : 0.210378
iteration 5000 : model1_mean_dice : 0.654604 model1_mean_hd95 : 92.580555 model1_mean_iou : 0.525880
iteration 5000 : model2_mean_dice : 0.750522 model2_mean_hd95 : 69.041032 model2_mean_iou : 0.638144
iteration 5001 : model1 loss : 0.208103 model2 loss : 0.177604
iteration 5002 : model1 loss : 0.118592 model2 loss : 0.114699
iteration 5003 : model1 loss : 0.131150 model2 loss : 0.108394
iteration 5004 : model1 loss : 0.204252 model2 loss : 0.182775
iteration 5005 : model1 loss : 0.164908 model2 loss : 0.156484
iteration 5006 : model1 loss : 0.177661 model2 loss : 0.151399
iteration 5007 : model1 loss : 0.180037 model2 loss : 0.156498
iteration 5008 : model1 loss : 0.136597 model2 loss : 0.104486
iteration 5009 : model1 loss : 0.113641 model2 loss : 0.111843
iteration 5010 : model1 loss : 0.171661 model2 loss : 0.128854
iteration 5011 : model1 loss : 0.153078 model2 loss : 0.179186
iteration 5012 : model1 loss : 0.126505 model2 loss : 0.121630
iteration 5013 : model1 loss : 0.156318 model2 loss : 0.156532
iteration 5014 : model1 loss : 0.169740 model2 loss : 0.138320
iteration 5015 : model1 loss : 0.173152 model2 loss : 0.139849
 50%|█████████████▌             | 295/589 [2:00:18<2:21:06, 28.80s/it]iteration 5016 : model1 loss : 0.153110 model2 loss : 0.198160
iteration 5017 : model1 loss : 0.140714 model2 loss : 0.126470
iteration 5018 : model1 loss : 0.148712 model2 loss : 0.126160
iteration 5019 : model1 loss : 0.195782 model2 loss : 0.157817
iteration 5020 : model1 loss : 0.142187 model2 loss : 0.149335
iteration 5021 : model1 loss : 0.109081 model2 loss : 0.141853
iteration 5022 : model1 loss : 0.147133 model2 loss : 0.135181
iteration 5023 : model1 loss : 0.147599 model2 loss : 0.154968
iteration 5024 : model1 loss : 0.158037 model2 loss : 0.134476
iteration 5025 : model1 loss : 0.153341 model2 loss : 0.132221
iteration 5026 : model1 loss : 0.136733 model2 loss : 0.140269
iteration 5027 : model1 loss : 0.169459 model2 loss : 0.142527
iteration 5028 : model1 loss : 0.227276 model2 loss : 0.190557
iteration 5029 : model1 loss : 0.149146 model2 loss : 0.119636
iteration 5030 : model1 loss : 0.134082 model2 loss : 0.107253
iteration 5031 : model1 loss : 0.204651 model2 loss : 0.195106
iteration 5032 : model1 loss : 0.123754 model2 loss : 0.125251
 50%|█████████████▌             | 296/589 [2:00:41<2:11:48, 26.99s/it]iteration 5033 : model1 loss : 0.122174 model2 loss : 0.130336
iteration 5034 : model1 loss : 0.137291 model2 loss : 0.108232
iteration 5035 : model1 loss : 0.179389 model2 loss : 0.122489
iteration 5036 : model1 loss : 0.166273 model2 loss : 0.154800
iteration 5037 : model1 loss : 0.152490 model2 loss : 0.142812
iteration 5038 : model1 loss : 0.162853 model2 loss : 0.148734
iteration 5039 : model1 loss : 0.164092 model2 loss : 0.122001
iteration 5040 : model1 loss : 0.185507 model2 loss : 0.154064
iteration 5041 : model1 loss : 0.150371 model2 loss : 0.116376
iteration 5042 : model1 loss : 0.123683 model2 loss : 0.112330
iteration 5043 : model1 loss : 0.150473 model2 loss : 0.167450
iteration 5044 : model1 loss : 0.152747 model2 loss : 0.124403
iteration 5045 : model1 loss : 0.165038 model2 loss : 0.162131
iteration 5046 : model1 loss : 0.136201 model2 loss : 0.108529
iteration 5047 : model1 loss : 0.139927 model2 loss : 0.126145
iteration 5048 : model1 loss : 0.158207 model2 loss : 0.195162
iteration 5049 : model1 loss : 0.144713 model2 loss : 0.129755
 50%|█████████████▌             | 297/589 [2:01:04<2:05:13, 25.73s/it]iteration 5050 : model1 loss : 0.098005 model2 loss : 0.119658
iteration 5051 : model1 loss : 0.140471 model2 loss : 0.134277
iteration 5052 : model1 loss : 0.153390 model2 loss : 0.108241
iteration 5053 : model1 loss : 0.187484 model2 loss : 0.138535
iteration 5054 : model1 loss : 0.100593 model2 loss : 0.132691
iteration 5055 : model1 loss : 0.144278 model2 loss : 0.139215
iteration 5056 : model1 loss : 0.121981 model2 loss : 0.115682
iteration 5057 : model1 loss : 0.160916 model2 loss : 0.130335
iteration 5058 : model1 loss : 0.189701 model2 loss : 0.148419
iteration 5059 : model1 loss : 0.171312 model2 loss : 0.145829
iteration 5060 : model1 loss : 0.146049 model2 loss : 0.113241
iteration 5061 : model1 loss : 0.152514 model2 loss : 0.158282
iteration 5062 : model1 loss : 0.143767 model2 loss : 0.129808
iteration 5063 : model1 loss : 0.188811 model2 loss : 0.146072
iteration 5064 : model1 loss : 0.160179 model2 loss : 0.131008
iteration 5065 : model1 loss : 0.112296 model2 loss : 0.118326
iteration 5066 : model1 loss : 0.155696 model2 loss : 0.134129
 51%|█████████████▋             | 298/589 [2:01:27<2:00:43, 24.89s/it]iteration 5067 : model1 loss : 0.178449 model2 loss : 0.157570
iteration 5068 : model1 loss : 0.182564 model2 loss : 0.132542
iteration 5069 : model1 loss : 0.167812 model2 loss : 0.129912
iteration 5070 : model1 loss : 0.150926 model2 loss : 0.116107
iteration 5071 : model1 loss : 0.169278 model2 loss : 0.132627
iteration 5072 : model1 loss : 0.113175 model2 loss : 0.117551
iteration 5073 : model1 loss : 0.147046 model2 loss : 0.158399
iteration 5074 : model1 loss : 0.147824 model2 loss : 0.115789
iteration 5075 : model1 loss : 0.133488 model2 loss : 0.151869
iteration 5076 : model1 loss : 0.151849 model2 loss : 0.121291
iteration 5077 : model1 loss : 0.126506 model2 loss : 0.118479
iteration 5078 : model1 loss : 0.106327 model2 loss : 0.113415
iteration 5079 : model1 loss : 0.160046 model2 loss : 0.171709
iteration 5080 : model1 loss : 0.170466 model2 loss : 0.121950
iteration 5081 : model1 loss : 0.187349 model2 loss : 0.114068
iteration 5082 : model1 loss : 0.165338 model2 loss : 0.130314
iteration 5083 : model1 loss : 0.133180 model2 loss : 0.107255
 51%|█████████████▋             | 299/589 [2:01:50<1:57:09, 24.24s/it]iteration 5084 : model1 loss : 0.140245 model2 loss : 0.120170
iteration 5085 : model1 loss : 0.135235 model2 loss : 0.122095
iteration 5086 : model1 loss : 0.145877 model2 loss : 0.118259
iteration 5087 : model1 loss : 0.161457 model2 loss : 0.130502
iteration 5088 : model1 loss : 0.168375 model2 loss : 0.134952
iteration 5089 : model1 loss : 0.147459 model2 loss : 0.132687
iteration 5090 : model1 loss : 0.150759 model2 loss : 0.121314
iteration 5091 : model1 loss : 0.126336 model2 loss : 0.089393
iteration 5092 : model1 loss : 0.154528 model2 loss : 0.122226
iteration 5093 : model1 loss : 0.166390 model2 loss : 0.141475
iteration 5094 : model1 loss : 0.205987 model2 loss : 0.107122
iteration 5095 : model1 loss : 0.167719 model2 loss : 0.136760
iteration 5096 : model1 loss : 0.145118 model2 loss : 0.118611
iteration 5097 : model1 loss : 0.132447 model2 loss : 0.115666
iteration 5098 : model1 loss : 0.139113 model2 loss : 0.119485
iteration 5099 : model1 loss : 0.149164 model2 loss : 0.130708
iteration 5100 : model1 loss : 0.143023 model2 loss : 0.153748
 51%|█████████████▊             | 300/589 [2:02:13<1:54:49, 23.84s/it]iteration 5101 : model1 loss : 0.163989 model2 loss : 0.144214
iteration 5102 : model1 loss : 0.131686 model2 loss : 0.115154
iteration 5103 : model1 loss : 0.182924 model2 loss : 0.151300
iteration 5104 : model1 loss : 0.146787 model2 loss : 0.242896
iteration 5105 : model1 loss : 0.100777 model2 loss : 0.086734
iteration 5106 : model1 loss : 0.146510 model2 loss : 0.131052
iteration 5107 : model1 loss : 0.164729 model2 loss : 0.108600
iteration 5108 : model1 loss : 0.122471 model2 loss : 0.120617
iteration 5109 : model1 loss : 0.195014 model2 loss : 0.160245
iteration 5110 : model1 loss : 0.132535 model2 loss : 0.117600
iteration 5111 : model1 loss : 0.157701 model2 loss : 0.142841
iteration 5112 : model1 loss : 0.182989 model2 loss : 0.126683
iteration 5113 : model1 loss : 0.195490 model2 loss : 0.145305
iteration 5114 : model1 loss : 0.116636 model2 loss : 0.083887
iteration 5115 : model1 loss : 0.146862 model2 loss : 0.115703
iteration 5116 : model1 loss : 0.167511 model2 loss : 0.146811
iteration 5117 : model1 loss : 0.197642 model2 loss : 0.177628
 51%|█████████████▊             | 301/589 [2:02:35<1:52:50, 23.51s/it]iteration 5118 : model1 loss : 0.142785 model2 loss : 0.127646
iteration 5119 : model1 loss : 0.115045 model2 loss : 0.095996
iteration 5120 : model1 loss : 0.148925 model2 loss : 0.138811
iteration 5121 : model1 loss : 0.196791 model2 loss : 0.130975
iteration 5122 : model1 loss : 0.146032 model2 loss : 0.110787
iteration 5123 : model1 loss : 0.156358 model2 loss : 0.127697
iteration 5124 : model1 loss : 0.152464 model2 loss : 0.122852
iteration 5125 : model1 loss : 0.184566 model2 loss : 0.131399
iteration 5126 : model1 loss : 0.153769 model2 loss : 0.098573
iteration 5127 : model1 loss : 0.144664 model2 loss : 0.136295
iteration 5128 : model1 loss : 0.201306 model2 loss : 0.187720
iteration 5129 : model1 loss : 0.144334 model2 loss : 0.118803
iteration 5130 : model1 loss : 0.148213 model2 loss : 0.130492
iteration 5131 : model1 loss : 0.167657 model2 loss : 0.178390
iteration 5132 : model1 loss : 0.145346 model2 loss : 0.107932
iteration 5133 : model1 loss : 0.123716 model2 loss : 0.123716
iteration 5134 : model1 loss : 0.156969 model2 loss : 0.104894
 51%|█████████████▊             | 302/589 [2:02:58<1:51:26, 23.30s/it]iteration 5135 : model1 loss : 0.167052 model2 loss : 0.096385
iteration 5136 : model1 loss : 0.152011 model2 loss : 0.137321
iteration 5137 : model1 loss : 0.161330 model2 loss : 0.132679
iteration 5138 : model1 loss : 0.191638 model2 loss : 0.140705
iteration 5139 : model1 loss : 0.127581 model2 loss : 0.117011
iteration 5140 : model1 loss : 0.138136 model2 loss : 0.116473
iteration 5141 : model1 loss : 0.141297 model2 loss : 0.111369
iteration 5142 : model1 loss : 0.129684 model2 loss : 0.105271
iteration 5143 : model1 loss : 0.165056 model2 loss : 0.130364
iteration 5144 : model1 loss : 0.150837 model2 loss : 0.138836
iteration 5145 : model1 loss : 0.133452 model2 loss : 0.137809
iteration 5146 : model1 loss : 0.195246 model2 loss : 0.136416
iteration 5147 : model1 loss : 0.189505 model2 loss : 0.158359
iteration 5148 : model1 loss : 0.163911 model2 loss : 0.112776
iteration 5149 : model1 loss : 0.150360 model2 loss : 0.128978
iteration 5150 : model1 loss : 0.154223 model2 loss : 0.113198
iteration 5151 : model1 loss : 0.122705 model2 loss : 0.084509
 51%|█████████████▉             | 303/589 [2:03:21<1:50:29, 23.18s/it]iteration 5152 : model1 loss : 0.143630 model2 loss : 0.103095
iteration 5153 : model1 loss : 0.186299 model2 loss : 0.132104
iteration 5154 : model1 loss : 0.141686 model2 loss : 0.123993
iteration 5155 : model1 loss : 0.172064 model2 loss : 0.113144
iteration 5156 : model1 loss : 0.139025 model2 loss : 0.130785
iteration 5157 : model1 loss : 0.132123 model2 loss : 0.121173
iteration 5158 : model1 loss : 0.131949 model2 loss : 0.102944
iteration 5159 : model1 loss : 0.162948 model2 loss : 0.179015
iteration 5160 : model1 loss : 0.159935 model2 loss : 0.191218
iteration 5161 : model1 loss : 0.102237 model2 loss : 0.108837
iteration 5162 : model1 loss : 0.204469 model2 loss : 0.152421
iteration 5163 : model1 loss : 0.132679 model2 loss : 0.101744
iteration 5164 : model1 loss : 0.153565 model2 loss : 0.107806
iteration 5165 : model1 loss : 0.164007 model2 loss : 0.137294
iteration 5166 : model1 loss : 0.166214 model2 loss : 0.150300
iteration 5167 : model1 loss : 0.173610 model2 loss : 0.111979
iteration 5168 : model1 loss : 0.153219 model2 loss : 0.185036
 52%|█████████████▉             | 304/589 [2:03:44<1:49:29, 23.05s/it]iteration 5169 : model1 loss : 0.191091 model2 loss : 0.158621
iteration 5170 : model1 loss : 0.184699 model2 loss : 0.122774
iteration 5171 : model1 loss : 0.158408 model2 loss : 0.161249
iteration 5172 : model1 loss : 0.185411 model2 loss : 0.175958
iteration 5173 : model1 loss : 0.138602 model2 loss : 0.093769
iteration 5174 : model1 loss : 0.123388 model2 loss : 0.108950
iteration 5175 : model1 loss : 0.138781 model2 loss : 0.127218
iteration 5176 : model1 loss : 0.141876 model2 loss : 0.138827
iteration 5177 : model1 loss : 0.120347 model2 loss : 0.087402
iteration 5178 : model1 loss : 0.174169 model2 loss : 0.139016
iteration 5179 : model1 loss : 0.186384 model2 loss : 0.186347
iteration 5180 : model1 loss : 0.157076 model2 loss : 0.134456
iteration 5181 : model1 loss : 0.154874 model2 loss : 0.140193
iteration 5182 : model1 loss : 0.119735 model2 loss : 0.126119
iteration 5183 : model1 loss : 0.154559 model2 loss : 0.141405
iteration 5184 : model1 loss : 0.142825 model2 loss : 0.135043
iteration 5185 : model1 loss : 0.154832 model2 loss : 0.131985
 52%|█████████████▉             | 305/589 [2:04:06<1:48:37, 22.95s/it]iteration 5186 : model1 loss : 0.201447 model2 loss : 0.144648
iteration 5187 : model1 loss : 0.141346 model2 loss : 0.120367
iteration 5188 : model1 loss : 0.144967 model2 loss : 0.137119
iteration 5189 : model1 loss : 0.140566 model2 loss : 0.133219
iteration 5190 : model1 loss : 0.142998 model2 loss : 0.111822
iteration 5191 : model1 loss : 0.147570 model2 loss : 0.162895
iteration 5192 : model1 loss : 0.149773 model2 loss : 0.115382
iteration 5193 : model1 loss : 0.123991 model2 loss : 0.115016
iteration 5194 : model1 loss : 0.151671 model2 loss : 0.127893
iteration 5195 : model1 loss : 0.153291 model2 loss : 0.138009
iteration 5196 : model1 loss : 0.180502 model2 loss : 0.129102
iteration 5197 : model1 loss : 0.153195 model2 loss : 0.123398
iteration 5198 : model1 loss : 0.155718 model2 loss : 0.126851
iteration 5199 : model1 loss : 0.163663 model2 loss : 0.144624
iteration 5200 : model1 loss : 0.123269 model2 loss : 0.099665
iteration 5200 : model1_mean_dice : 0.669081 model1_mean_hd95 : 85.894223 model1_mean_iou : 0.539894
iteration 5200 : model2_mean_dice : 0.771071 model2_mean_hd95 : 69.914882 model2_mean_iou : 0.657516
iteration 5201 : model1 loss : 0.131889 model2 loss : 0.112659
iteration 5202 : model1 loss : 0.124827 model2 loss : 0.117499
 52%|██████████████             | 306/589 [2:04:49<2:16:16, 28.89s/it]iteration 5203 : model1 loss : 0.174655 model2 loss : 0.141954
iteration 5204 : model1 loss : 0.137634 model2 loss : 0.111707
iteration 5205 : model1 loss : 0.198164 model2 loss : 0.107993
iteration 5206 : model1 loss : 0.164540 model2 loss : 0.119031
iteration 5207 : model1 loss : 0.153454 model2 loss : 0.135092
iteration 5208 : model1 loss : 0.153914 model2 loss : 0.130305
iteration 5209 : model1 loss : 0.158003 model2 loss : 0.148077
iteration 5210 : model1 loss : 0.175844 model2 loss : 0.161166
iteration 5211 : model1 loss : 0.214437 model2 loss : 0.184060
iteration 5212 : model1 loss : 0.172445 model2 loss : 0.139737
iteration 5213 : model1 loss : 0.164388 model2 loss : 0.110356
iteration 5214 : model1 loss : 0.133601 model2 loss : 0.136830
iteration 5215 : model1 loss : 0.184347 model2 loss : 0.157944
iteration 5216 : model1 loss : 0.142810 model2 loss : 0.112637
iteration 5217 : model1 loss : 0.120678 model2 loss : 0.127816
iteration 5218 : model1 loss : 0.138387 model2 loss : 0.107339
iteration 5219 : model1 loss : 0.123916 model2 loss : 0.094883
 52%|██████████████             | 307/589 [2:05:12<2:07:02, 27.03s/it]iteration 5220 : model1 loss : 0.143751 model2 loss : 0.118519
iteration 5221 : model1 loss : 0.188022 model2 loss : 0.184991
iteration 5222 : model1 loss : 0.151728 model2 loss : 0.159709
iteration 5223 : model1 loss : 0.167110 model2 loss : 0.120218
iteration 5224 : model1 loss : 0.159724 model2 loss : 0.138191
iteration 5225 : model1 loss : 0.161500 model2 loss : 0.153400
iteration 5226 : model1 loss : 0.151737 model2 loss : 0.143470
iteration 5227 : model1 loss : 0.118798 model2 loss : 0.166628
iteration 5228 : model1 loss : 0.133525 model2 loss : 0.134929
iteration 5229 : model1 loss : 0.131293 model2 loss : 0.130081
iteration 5230 : model1 loss : 0.112945 model2 loss : 0.115401
iteration 5231 : model1 loss : 0.147709 model2 loss : 0.192527
iteration 5232 : model1 loss : 0.165337 model2 loss : 0.139163
iteration 5233 : model1 loss : 0.180684 model2 loss : 0.172788
iteration 5234 : model1 loss : 0.158178 model2 loss : 0.146570
iteration 5235 : model1 loss : 0.138422 model2 loss : 0.104076
iteration 5236 : model1 loss : 0.149828 model2 loss : 0.119724
 52%|██████████████             | 308/589 [2:05:35<2:00:34, 25.74s/it]iteration 5237 : model1 loss : 0.168450 model2 loss : 0.140327
iteration 5238 : model1 loss : 0.145341 model2 loss : 0.145135
iteration 5239 : model1 loss : 0.139714 model2 loss : 0.124324
iteration 5240 : model1 loss : 0.144478 model2 loss : 0.099522
iteration 5241 : model1 loss : 0.099455 model2 loss : 0.092032
iteration 5242 : model1 loss : 0.178808 model2 loss : 0.123333
iteration 5243 : model1 loss : 0.147195 model2 loss : 0.136122
iteration 5244 : model1 loss : 0.144933 model2 loss : 0.129919
iteration 5245 : model1 loss : 0.139237 model2 loss : 0.117597
iteration 5246 : model1 loss : 0.128867 model2 loss : 0.107250
iteration 5247 : model1 loss : 0.129634 model2 loss : 0.098251
iteration 5248 : model1 loss : 0.121063 model2 loss : 0.104195
iteration 5249 : model1 loss : 0.121291 model2 loss : 0.143753
iteration 5250 : model1 loss : 0.169199 model2 loss : 0.121761
iteration 5251 : model1 loss : 0.193225 model2 loss : 0.146308
iteration 5252 : model1 loss : 0.143180 model2 loss : 0.095821
iteration 5253 : model1 loss : 0.148302 model2 loss : 0.127531
 52%|██████████████▏            | 309/589 [2:05:58<1:56:15, 24.91s/it]iteration 5254 : model1 loss : 0.165648 model2 loss : 0.122805
iteration 5255 : model1 loss : 0.176064 model2 loss : 0.119558
iteration 5256 : model1 loss : 0.140093 model2 loss : 0.135009
iteration 5257 : model1 loss : 0.128397 model2 loss : 0.106702
iteration 5258 : model1 loss : 0.141339 model2 loss : 0.123548
iteration 5259 : model1 loss : 0.143350 model2 loss : 0.148799
iteration 5260 : model1 loss : 0.202933 model2 loss : 0.124597
iteration 5261 : model1 loss : 0.162761 model2 loss : 0.114453
iteration 5262 : model1 loss : 0.114300 model2 loss : 0.095369
iteration 5263 : model1 loss : 0.146799 model2 loss : 0.119287
iteration 5264 : model1 loss : 0.172344 model2 loss : 0.102056
iteration 5265 : model1 loss : 0.181141 model2 loss : 0.180057
iteration 5266 : model1 loss : 0.141965 model2 loss : 0.143881
iteration 5267 : model1 loss : 0.169017 model2 loss : 0.128057
iteration 5268 : model1 loss : 0.186221 model2 loss : 0.138540
iteration 5269 : model1 loss : 0.127313 model2 loss : 0.110925
iteration 5270 : model1 loss : 0.153121 model2 loss : 0.128020
 53%|██████████████▏            | 310/589 [2:06:20<1:52:50, 24.27s/it]iteration 5271 : model1 loss : 0.151495 model2 loss : 0.148906
iteration 5272 : model1 loss : 0.135449 model2 loss : 0.174265
iteration 5273 : model1 loss : 0.127491 model2 loss : 0.122828
iteration 5274 : model1 loss : 0.145413 model2 loss : 0.129763
iteration 5275 : model1 loss : 0.133538 model2 loss : 0.165477
iteration 5276 : model1 loss : 0.153905 model2 loss : 0.141859
iteration 5277 : model1 loss : 0.209917 model2 loss : 0.115411
iteration 5278 : model1 loss : 0.140499 model2 loss : 0.105887
iteration 5279 : model1 loss : 0.175694 model2 loss : 0.151891
iteration 5280 : model1 loss : 0.180276 model2 loss : 0.139078
iteration 5281 : model1 loss : 0.147113 model2 loss : 0.113055
iteration 5282 : model1 loss : 0.159677 model2 loss : 0.137531
iteration 5283 : model1 loss : 0.157278 model2 loss : 0.153967
iteration 5284 : model1 loss : 0.145553 model2 loss : 0.150683
iteration 5285 : model1 loss : 0.136170 model2 loss : 0.125824
iteration 5286 : model1 loss : 0.125839 model2 loss : 0.109678
iteration 5287 : model1 loss : 0.169629 model2 loss : 0.166549
 53%|██████████████▎            | 311/589 [2:06:43<1:50:24, 23.83s/it]iteration 5288 : model1 loss : 0.167792 model2 loss : 0.147947
iteration 5289 : model1 loss : 0.178764 model2 loss : 0.126526
iteration 5290 : model1 loss : 0.147650 model2 loss : 0.110616
iteration 5291 : model1 loss : 0.139639 model2 loss : 0.115864
iteration 5292 : model1 loss : 0.158791 model2 loss : 0.151050
iteration 5293 : model1 loss : 0.133222 model2 loss : 0.143901
iteration 5294 : model1 loss : 0.145312 model2 loss : 0.125399
iteration 5295 : model1 loss : 0.144472 model2 loss : 0.129828
iteration 5296 : model1 loss : 0.135272 model2 loss : 0.104107
iteration 5297 : model1 loss : 0.157991 model2 loss : 0.107006
iteration 5298 : model1 loss : 0.160261 model2 loss : 0.117422
iteration 5299 : model1 loss : 0.152766 model2 loss : 0.094274
iteration 5300 : model1 loss : 0.133253 model2 loss : 0.115278
iteration 5301 : model1 loss : 0.124004 model2 loss : 0.120963
iteration 5302 : model1 loss : 0.158991 model2 loss : 0.129416
iteration 5303 : model1 loss : 0.165537 model2 loss : 0.104944
iteration 5304 : model1 loss : 0.192896 model2 loss : 0.121420
 53%|██████████████▎            | 312/589 [2:07:06<1:48:44, 23.56s/it]iteration 5305 : model1 loss : 0.135833 model2 loss : 0.122914
iteration 5306 : model1 loss : 0.165279 model2 loss : 0.128849
iteration 5307 : model1 loss : 0.122062 model2 loss : 0.090599
iteration 5308 : model1 loss : 0.125805 model2 loss : 0.137862
iteration 5309 : model1 loss : 0.137999 model2 loss : 0.119371
iteration 5310 : model1 loss : 0.199733 model2 loss : 0.166861
iteration 5311 : model1 loss : 0.130260 model2 loss : 0.109513
iteration 5312 : model1 loss : 0.139953 model2 loss : 0.128084
iteration 5313 : model1 loss : 0.140949 model2 loss : 0.110692
iteration 5314 : model1 loss : 0.154187 model2 loss : 0.204838
iteration 5315 : model1 loss : 0.122423 model2 loss : 0.120776
iteration 5316 : model1 loss : 0.150443 model2 loss : 0.116415
iteration 5317 : model1 loss : 0.207720 model2 loss : 0.127684
iteration 5318 : model1 loss : 0.149757 model2 loss : 0.118121
iteration 5319 : model1 loss : 0.151594 model2 loss : 0.128439
iteration 5320 : model1 loss : 0.129781 model2 loss : 0.121648
iteration 5321 : model1 loss : 0.129827 model2 loss : 0.114212
 53%|██████████████▎            | 313/589 [2:07:29<1:47:16, 23.32s/it]iteration 5322 : model1 loss : 0.142823 model2 loss : 0.118411
iteration 5323 : model1 loss : 0.131682 model2 loss : 0.098344
iteration 5324 : model1 loss : 0.169839 model2 loss : 0.137645
iteration 5325 : model1 loss : 0.152955 model2 loss : 0.108117
iteration 5326 : model1 loss : 0.137802 model2 loss : 0.110340
iteration 5327 : model1 loss : 0.159927 model2 loss : 0.163794
iteration 5328 : model1 loss : 0.183940 model2 loss : 0.137602
iteration 5329 : model1 loss : 0.129235 model2 loss : 0.111362
iteration 5330 : model1 loss : 0.105693 model2 loss : 0.083763
iteration 5331 : model1 loss : 0.169641 model2 loss : 0.126830
iteration 5332 : model1 loss : 0.123563 model2 loss : 0.106056
iteration 5333 : model1 loss : 0.126308 model2 loss : 0.117565
iteration 5334 : model1 loss : 0.131489 model2 loss : 0.119176
iteration 5335 : model1 loss : 0.203038 model2 loss : 0.145687
iteration 5336 : model1 loss : 0.144058 model2 loss : 0.131316
iteration 5337 : model1 loss : 0.138458 model2 loss : 0.123115
iteration 5338 : model1 loss : 0.169179 model2 loss : 0.136040
 53%|██████████████▍            | 314/589 [2:07:52<1:46:06, 23.15s/it]iteration 5339 : model1 loss : 0.100527 model2 loss : 0.092468
iteration 5340 : model1 loss : 0.108389 model2 loss : 0.110621
iteration 5341 : model1 loss : 0.132789 model2 loss : 0.101882
iteration 5342 : model1 loss : 0.139706 model2 loss : 0.147262
iteration 5343 : model1 loss : 0.129712 model2 loss : 0.104063
iteration 5344 : model1 loss : 0.144272 model2 loss : 0.103479
iteration 5345 : model1 loss : 0.128406 model2 loss : 0.112860
iteration 5346 : model1 loss : 0.190399 model2 loss : 0.187708
iteration 5347 : model1 loss : 0.158893 model2 loss : 0.161326
iteration 5348 : model1 loss : 0.154445 model2 loss : 0.124535
iteration 5349 : model1 loss : 0.190782 model2 loss : 0.142255
iteration 5350 : model1 loss : 0.173087 model2 loss : 0.126511
iteration 5351 : model1 loss : 0.151541 model2 loss : 0.128760
iteration 5352 : model1 loss : 0.183924 model2 loss : 0.159126
iteration 5353 : model1 loss : 0.171599 model2 loss : 0.166282
iteration 5354 : model1 loss : 0.138087 model2 loss : 0.123422
iteration 5355 : model1 loss : 0.162190 model2 loss : 0.122760
 53%|██████████████▍            | 315/589 [2:08:15<1:45:25, 23.09s/it]iteration 5356 : model1 loss : 0.133379 model2 loss : 0.110557
iteration 5357 : model1 loss : 0.132746 model2 loss : 0.114794
iteration 5358 : model1 loss : 0.130733 model2 loss : 0.099815
iteration 5359 : model1 loss : 0.133835 model2 loss : 0.121668
iteration 5360 : model1 loss : 0.121896 model2 loss : 0.115193
iteration 5361 : model1 loss : 0.200144 model2 loss : 0.188173
iteration 5362 : model1 loss : 0.225657 model2 loss : 0.171616
iteration 5363 : model1 loss : 0.132356 model2 loss : 0.128422
iteration 5364 : model1 loss : 0.136588 model2 loss : 0.119562
iteration 5365 : model1 loss : 0.144555 model2 loss : 0.145858
iteration 5366 : model1 loss : 0.128855 model2 loss : 0.131274
iteration 5367 : model1 loss : 0.140972 model2 loss : 0.098486
iteration 5368 : model1 loss : 0.124898 model2 loss : 0.146858
iteration 5369 : model1 loss : 0.141144 model2 loss : 0.104919
iteration 5370 : model1 loss : 0.140853 model2 loss : 0.127081
iteration 5371 : model1 loss : 0.139634 model2 loss : 0.160663
iteration 5372 : model1 loss : 0.172160 model2 loss : 0.129064
 54%|██████████████▍            | 316/589 [2:08:37<1:44:35, 22.99s/it]iteration 5373 : model1 loss : 0.150954 model2 loss : 0.127714
iteration 5374 : model1 loss : 0.157356 model2 loss : 0.139264
iteration 5375 : model1 loss : 0.180845 model2 loss : 0.114284
iteration 5376 : model1 loss : 0.167469 model2 loss : 0.131871
iteration 5377 : model1 loss : 0.128514 model2 loss : 0.167140
iteration 5378 : model1 loss : 0.122764 model2 loss : 0.121173
iteration 5379 : model1 loss : 0.155798 model2 loss : 0.121205
iteration 5380 : model1 loss : 0.131590 model2 loss : 0.086688
iteration 5381 : model1 loss : 0.165225 model2 loss : 0.118378
iteration 5382 : model1 loss : 0.145500 model2 loss : 0.111215
iteration 5383 : model1 loss : 0.126933 model2 loss : 0.127270
iteration 5384 : model1 loss : 0.169645 model2 loss : 0.085898
iteration 5385 : model1 loss : 0.152431 model2 loss : 0.145182
iteration 5386 : model1 loss : 0.157342 model2 loss : 0.145111
iteration 5387 : model1 loss : 0.119930 model2 loss : 0.095413
iteration 5388 : model1 loss : 0.161476 model2 loss : 0.123768
iteration 5389 : model1 loss : 0.156973 model2 loss : 0.094649
 54%|██████████████▌            | 317/589 [2:09:00<1:43:56, 22.93s/it]iteration 5390 : model1 loss : 0.178055 model2 loss : 0.138611
iteration 5391 : model1 loss : 0.137022 model2 loss : 0.123173
iteration 5392 : model1 loss : 0.138100 model2 loss : 0.102395
iteration 5393 : model1 loss : 0.137040 model2 loss : 0.125715
iteration 5394 : model1 loss : 0.138709 model2 loss : 0.130188
iteration 5395 : model1 loss : 0.135660 model2 loss : 0.125818
iteration 5396 : model1 loss : 0.160020 model2 loss : 0.142516
iteration 5397 : model1 loss : 0.170175 model2 loss : 0.113491
iteration 5398 : model1 loss : 0.138146 model2 loss : 0.118059
iteration 5399 : model1 loss : 0.127526 model2 loss : 0.116253
iteration 5400 : model1 loss : 0.184694 model2 loss : 0.141054
iteration 5400 : model1_mean_dice : 0.664600 model1_mean_hd95 : 92.596640 model1_mean_iou : 0.538625
iteration 5400 : model2_mean_dice : 0.748132 model2_mean_hd95 : 68.057939 model2_mean_iou : 0.637856
iteration 5401 : model1 loss : 0.189429 model2 loss : 0.111962
iteration 5402 : model1 loss : 0.119702 model2 loss : 0.120599
iteration 5403 : model1 loss : 0.148136 model2 loss : 0.135641
iteration 5404 : model1 loss : 0.146542 model2 loss : 0.127530
iteration 5405 : model1 loss : 0.139947 model2 loss : 0.138028
iteration 5406 : model1 loss : 0.121233 model2 loss : 0.102070
 54%|██████████████▌            | 318/589 [2:09:43<2:10:34, 28.91s/it]iteration 5407 : model1 loss : 0.123332 model2 loss : 0.102983
iteration 5408 : model1 loss : 0.197877 model2 loss : 0.186878
iteration 5409 : model1 loss : 0.180807 model2 loss : 0.116960
iteration 5410 : model1 loss : 0.185734 model2 loss : 0.131797
iteration 5411 : model1 loss : 0.137653 model2 loss : 0.087513
iteration 5412 : model1 loss : 0.164696 model2 loss : 0.170425
iteration 5413 : model1 loss : 0.121535 model2 loss : 0.107860
iteration 5414 : model1 loss : 0.121673 model2 loss : 0.108693
iteration 5415 : model1 loss : 0.128692 model2 loss : 0.118893
iteration 5416 : model1 loss : 0.155052 model2 loss : 0.131945
iteration 5417 : model1 loss : 0.152747 model2 loss : 0.151553
iteration 5418 : model1 loss : 0.157695 model2 loss : 0.151211
iteration 5419 : model1 loss : 0.159801 model2 loss : 0.123208
iteration 5420 : model1 loss : 0.153986 model2 loss : 0.090872
iteration 5421 : model1 loss : 0.110932 model2 loss : 0.097593
iteration 5422 : model1 loss : 0.144408 model2 loss : 0.158497
iteration 5423 : model1 loss : 0.144096 model2 loss : 0.096556
 54%|██████████████▌            | 319/589 [2:10:06<2:01:39, 27.04s/it]iteration 5424 : model1 loss : 0.142368 model2 loss : 0.109418
iteration 5425 : model1 loss : 0.154156 model2 loss : 0.130313
iteration 5426 : model1 loss : 0.119136 model2 loss : 0.099990
iteration 5427 : model1 loss : 0.206160 model2 loss : 0.117115
iteration 5428 : model1 loss : 0.138110 model2 loss : 0.124908
iteration 5429 : model1 loss : 0.107059 model2 loss : 0.087618
iteration 5430 : model1 loss : 0.195285 model2 loss : 0.140418
iteration 5431 : model1 loss : 0.105247 model2 loss : 0.084787
iteration 5432 : model1 loss : 0.192011 model2 loss : 0.150869
iteration 5433 : model1 loss : 0.125594 model2 loss : 0.102497
iteration 5434 : model1 loss : 0.141482 model2 loss : 0.108813
iteration 5435 : model1 loss : 0.108760 model2 loss : 0.100027
iteration 5436 : model1 loss : 0.127714 model2 loss : 0.127116
iteration 5437 : model1 loss : 0.171616 model2 loss : 0.125999
iteration 5438 : model1 loss : 0.214063 model2 loss : 0.171830
iteration 5439 : model1 loss : 0.149562 model2 loss : 0.162678
iteration 5440 : model1 loss : 0.170555 model2 loss : 0.129018
 54%|██████████████▋            | 320/589 [2:10:28<1:55:24, 25.74s/it]iteration 5441 : model1 loss : 0.198806 model2 loss : 0.171404
iteration 5442 : model1 loss : 0.153315 model2 loss : 0.134072
iteration 5443 : model1 loss : 0.171815 model2 loss : 0.205378
iteration 5444 : model1 loss : 0.186994 model2 loss : 0.169228
iteration 5445 : model1 loss : 0.148406 model2 loss : 0.121751
iteration 5446 : model1 loss : 0.131729 model2 loss : 0.140218
iteration 5447 : model1 loss : 0.168320 model2 loss : 0.136188
iteration 5448 : model1 loss : 0.164198 model2 loss : 0.145243
iteration 5449 : model1 loss : 0.172749 model2 loss : 0.121814
iteration 5450 : model1 loss : 0.137494 model2 loss : 0.122253
iteration 5451 : model1 loss : 0.141274 model2 loss : 0.138144
iteration 5452 : model1 loss : 0.120987 model2 loss : 0.116809
iteration 5453 : model1 loss : 0.121402 model2 loss : 0.099006
iteration 5454 : model1 loss : 0.139990 model2 loss : 0.128370
iteration 5455 : model1 loss : 0.133549 model2 loss : 0.115235
iteration 5456 : model1 loss : 0.137416 model2 loss : 0.136597
iteration 5457 : model1 loss : 0.158066 model2 loss : 0.138611
 54%|██████████████▋            | 321/589 [2:10:51<1:51:04, 24.87s/it]iteration 5458 : model1 loss : 0.158544 model2 loss : 0.179444
iteration 5459 : model1 loss : 0.126368 model2 loss : 0.101464
iteration 5460 : model1 loss : 0.165052 model2 loss : 0.161323
iteration 5461 : model1 loss : 0.153545 model2 loss : 0.117688
iteration 5462 : model1 loss : 0.128252 model2 loss : 0.102782
iteration 5463 : model1 loss : 0.162642 model2 loss : 0.131023
iteration 5464 : model1 loss : 0.139879 model2 loss : 0.124929
iteration 5465 : model1 loss : 0.167534 model2 loss : 0.170763
iteration 5466 : model1 loss : 0.138160 model2 loss : 0.158169
iteration 5467 : model1 loss : 0.154619 model2 loss : 0.132014
iteration 5468 : model1 loss : 0.186515 model2 loss : 0.143922
iteration 5469 : model1 loss : 0.128152 model2 loss : 0.133297
iteration 5470 : model1 loss : 0.121546 model2 loss : 0.116668
iteration 5471 : model1 loss : 0.158400 model2 loss : 0.129960
iteration 5472 : model1 loss : 0.163847 model2 loss : 0.168272
iteration 5473 : model1 loss : 0.154164 model2 loss : 0.151047
iteration 5474 : model1 loss : 0.169871 model2 loss : 0.095804
 55%|██████████████▊            | 322/589 [2:11:14<1:47:48, 24.23s/it]iteration 5475 : model1 loss : 0.144166 model2 loss : 0.130608
iteration 5476 : model1 loss : 0.191117 model2 loss : 0.127927
iteration 5477 : model1 loss : 0.092607 model2 loss : 0.076696
iteration 5478 : model1 loss : 0.218789 model2 loss : 0.138789
iteration 5479 : model1 loss : 0.142978 model2 loss : 0.107233
iteration 5480 : model1 loss : 0.175785 model2 loss : 0.159584
iteration 5481 : model1 loss : 0.165871 model2 loss : 0.139097
iteration 5482 : model1 loss : 0.152577 model2 loss : 0.113439
iteration 5483 : model1 loss : 0.154276 model2 loss : 0.134488
iteration 5484 : model1 loss : 0.150722 model2 loss : 0.114785
iteration 5485 : model1 loss : 0.128158 model2 loss : 0.106237
iteration 5486 : model1 loss : 0.152218 model2 loss : 0.099754
iteration 5487 : model1 loss : 0.158676 model2 loss : 0.119018
iteration 5488 : model1 loss : 0.187504 model2 loss : 0.112977
iteration 5489 : model1 loss : 0.187688 model2 loss : 0.130727
iteration 5490 : model1 loss : 0.094561 model2 loss : 0.061356
iteration 5491 : model1 loss : 0.164244 model2 loss : 0.127493
 55%|██████████████▊            | 323/589 [2:11:37<1:45:25, 23.78s/it]iteration 5492 : model1 loss : 0.154939 model2 loss : 0.139475
iteration 5493 : model1 loss : 0.140101 model2 loss : 0.091559
iteration 5494 : model1 loss : 0.147182 model2 loss : 0.101149
iteration 5495 : model1 loss : 0.147822 model2 loss : 0.135275
iteration 5496 : model1 loss : 0.105744 model2 loss : 0.102562
iteration 5497 : model1 loss : 0.147841 model2 loss : 0.137561
iteration 5498 : model1 loss : 0.159100 model2 loss : 0.166713
iteration 5499 : model1 loss : 0.146504 model2 loss : 0.129285
iteration 5500 : model1 loss : 0.115514 model2 loss : 0.092709
iteration 5501 : model1 loss : 0.164797 model2 loss : 0.150727
iteration 5502 : model1 loss : 0.158741 model2 loss : 0.129011
iteration 5503 : model1 loss : 0.152329 model2 loss : 0.124907
iteration 5504 : model1 loss : 0.173486 model2 loss : 0.147244
iteration 5505 : model1 loss : 0.155570 model2 loss : 0.123852
iteration 5506 : model1 loss : 0.145622 model2 loss : 0.096920
iteration 5507 : model1 loss : 0.152307 model2 loss : 0.113170
iteration 5508 : model1 loss : 0.170755 model2 loss : 0.119493
 55%|██████████████▊            | 324/589 [2:11:59<1:43:45, 23.49s/it]iteration 5509 : model1 loss : 0.126693 model2 loss : 0.116574
iteration 5510 : model1 loss : 0.129281 model2 loss : 0.125568
iteration 5511 : model1 loss : 0.162554 model2 loss : 0.130557
iteration 5512 : model1 loss : 0.130530 model2 loss : 0.099917
iteration 5513 : model1 loss : 0.150746 model2 loss : 0.137169
iteration 5514 : model1 loss : 0.134259 model2 loss : 0.091619
iteration 5515 : model1 loss : 0.140461 model2 loss : 0.094128
iteration 5516 : model1 loss : 0.125532 model2 loss : 0.122364
iteration 5517 : model1 loss : 0.146072 model2 loss : 0.147088
iteration 5518 : model1 loss : 0.142367 model2 loss : 0.118420
iteration 5519 : model1 loss : 0.160656 model2 loss : 0.134084
iteration 5520 : model1 loss : 0.140724 model2 loss : 0.121787
iteration 5521 : model1 loss : 0.169859 model2 loss : 0.125410
iteration 5522 : model1 loss : 0.145281 model2 loss : 0.122513
iteration 5523 : model1 loss : 0.166729 model2 loss : 0.128094
iteration 5524 : model1 loss : 0.174386 model2 loss : 0.164489
iteration 5525 : model1 loss : 0.124574 model2 loss : 0.104120
 55%|██████████████▉            | 325/589 [2:12:22<1:42:21, 23.27s/it]iteration 5526 : model1 loss : 0.136152 model2 loss : 0.134711
iteration 5527 : model1 loss : 0.128973 model2 loss : 0.105366
iteration 5528 : model1 loss : 0.199719 model2 loss : 0.163259
iteration 5529 : model1 loss : 0.125468 model2 loss : 0.112539
iteration 5530 : model1 loss : 0.118469 model2 loss : 0.104783
iteration 5531 : model1 loss : 0.147659 model2 loss : 0.145718
iteration 5532 : model1 loss : 0.135188 model2 loss : 0.132007
iteration 5533 : model1 loss : 0.181817 model2 loss : 0.176454
iteration 5534 : model1 loss : 0.132101 model2 loss : 0.106375
iteration 5535 : model1 loss : 0.169273 model2 loss : 0.156776
iteration 5536 : model1 loss : 0.190645 model2 loss : 0.141578
iteration 5537 : model1 loss : 0.147223 model2 loss : 0.105932
iteration 5538 : model1 loss : 0.162928 model2 loss : 0.108683
iteration 5539 : model1 loss : 0.157643 model2 loss : 0.127751
iteration 5540 : model1 loss : 0.149241 model2 loss : 0.127365
iteration 5541 : model1 loss : 0.164127 model2 loss : 0.141364
iteration 5542 : model1 loss : 0.169926 model2 loss : 0.137513
 55%|██████████████▉            | 326/589 [2:12:45<1:41:20, 23.12s/it]iteration 5543 : model1 loss : 0.157979 model2 loss : 0.153319
iteration 5544 : model1 loss : 0.140909 model2 loss : 0.111016
iteration 5545 : model1 loss : 0.131230 model2 loss : 0.123235
iteration 5546 : model1 loss : 0.136774 model2 loss : 0.119434
iteration 5547 : model1 loss : 0.167725 model2 loss : 0.135883
iteration 5548 : model1 loss : 0.151852 model2 loss : 0.137506
iteration 5549 : model1 loss : 0.126293 model2 loss : 0.095786
iteration 5550 : model1 loss : 0.174848 model2 loss : 0.134720
iteration 5551 : model1 loss : 0.149195 model2 loss : 0.103958
iteration 5552 : model1 loss : 0.152668 model2 loss : 0.127387
iteration 5553 : model1 loss : 0.141554 model2 loss : 0.123394
iteration 5554 : model1 loss : 0.152478 model2 loss : 0.164809
iteration 5555 : model1 loss : 0.169314 model2 loss : 0.108388
iteration 5556 : model1 loss : 0.154169 model2 loss : 0.140096
iteration 5557 : model1 loss : 0.126776 model2 loss : 0.097386
iteration 5558 : model1 loss : 0.118577 model2 loss : 0.113396
iteration 5559 : model1 loss : 0.168512 model2 loss : 0.124177
 56%|██████████████▉            | 327/589 [2:13:08<1:40:38, 23.05s/it]iteration 5560 : model1 loss : 0.129757 model2 loss : 0.080916
iteration 5561 : model1 loss : 0.109180 model2 loss : 0.134856
iteration 5562 : model1 loss : 0.150535 model2 loss : 0.125487
iteration 5563 : model1 loss : 0.123028 model2 loss : 0.089048
iteration 5564 : model1 loss : 0.116828 model2 loss : 0.121855
iteration 5565 : model1 loss : 0.143062 model2 loss : 0.113087
iteration 5566 : model1 loss : 0.134562 model2 loss : 0.107155
iteration 5567 : model1 loss : 0.125346 model2 loss : 0.071469
iteration 5568 : model1 loss : 0.178862 model2 loss : 0.120700
iteration 5569 : model1 loss : 0.141315 model2 loss : 0.120752
iteration 5570 : model1 loss : 0.154729 model2 loss : 0.110699
iteration 5571 : model1 loss : 0.193041 model2 loss : 0.132537
iteration 5572 : model1 loss : 0.158860 model2 loss : 0.130520
iteration 5573 : model1 loss : 0.178559 model2 loss : 0.111524
iteration 5574 : model1 loss : 0.167963 model2 loss : 0.134565
iteration 5575 : model1 loss : 0.112322 model2 loss : 0.116656
iteration 5576 : model1 loss : 0.149220 model2 loss : 0.125960
 56%|███████████████            | 328/589 [2:13:31<1:39:53, 22.96s/it]iteration 5577 : model1 loss : 0.156870 model2 loss : 0.099811
iteration 5578 : model1 loss : 0.115091 model2 loss : 0.105898
iteration 5579 : model1 loss : 0.139993 model2 loss : 0.111102
iteration 5580 : model1 loss : 0.143868 model2 loss : 0.099512
iteration 5581 : model1 loss : 0.150056 model2 loss : 0.114037
iteration 5582 : model1 loss : 0.136735 model2 loss : 0.087929
iteration 5583 : model1 loss : 0.176813 model2 loss : 0.191720
iteration 5584 : model1 loss : 0.157575 model2 loss : 0.144127
iteration 5585 : model1 loss : 0.124544 model2 loss : 0.089173
iteration 5586 : model1 loss : 0.144919 model2 loss : 0.122435
iteration 5587 : model1 loss : 0.098817 model2 loss : 0.093488
iteration 5588 : model1 loss : 0.138593 model2 loss : 0.127595
iteration 5589 : model1 loss : 0.147352 model2 loss : 0.118701
iteration 5590 : model1 loss : 0.178277 model2 loss : 0.175763
iteration 5591 : model1 loss : 0.139106 model2 loss : 0.118054
iteration 5592 : model1 loss : 0.138448 model2 loss : 0.097991
iteration 5593 : model1 loss : 0.188862 model2 loss : 0.144363
 56%|███████████████            | 329/589 [2:13:53<1:39:15, 22.91s/it]iteration 5594 : model1 loss : 0.183826 model2 loss : 0.147779
iteration 5595 : model1 loss : 0.150430 model2 loss : 0.137179
iteration 5596 : model1 loss : 0.133379 model2 loss : 0.113359
iteration 5597 : model1 loss : 0.171810 model2 loss : 0.127126
iteration 5598 : model1 loss : 0.159002 model2 loss : 0.141239
iteration 5599 : model1 loss : 0.161836 model2 loss : 0.133034
iteration 5600 : model1 loss : 0.133626 model2 loss : 0.103706
iteration 5600 : model1_mean_dice : 0.695765 model1_mean_hd95 : 87.489146 model1_mean_iou : 0.569901
iteration 5600 : model2_mean_dice : 0.761470 model2_mean_hd95 : 67.493587 model2_mean_iou : 0.647278
iteration 5601 : model1 loss : 0.162280 model2 loss : 0.133741
iteration 5602 : model1 loss : 0.158771 model2 loss : 0.118738
iteration 5603 : model1 loss : 0.116712 model2 loss : 0.126765
iteration 5604 : model1 loss : 0.140924 model2 loss : 0.114374
iteration 5605 : model1 loss : 0.105502 model2 loss : 0.093783
iteration 5606 : model1 loss : 0.123821 model2 loss : 0.137874
iteration 5607 : model1 loss : 0.153836 model2 loss : 0.119586
iteration 5608 : model1 loss : 0.132959 model2 loss : 0.127727
iteration 5609 : model1 loss : 0.177473 model2 loss : 0.103509
iteration 5610 : model1 loss : 0.212984 model2 loss : 0.137004
 56%|███████████████▏           | 330/589 [2:14:36<2:04:34, 28.86s/it]iteration 5611 : model1 loss : 0.156112 model2 loss : 0.151125
iteration 5612 : model1 loss : 0.124807 model2 loss : 0.103637
iteration 5613 : model1 loss : 0.172235 model2 loss : 0.116352
iteration 5614 : model1 loss : 0.147862 model2 loss : 0.111496
iteration 5615 : model1 loss : 0.132676 model2 loss : 0.092586
iteration 5616 : model1 loss : 0.148209 model2 loss : 0.151609
iteration 5617 : model1 loss : 0.158585 model2 loss : 0.116993
iteration 5618 : model1 loss : 0.129357 model2 loss : 0.114557
iteration 5619 : model1 loss : 0.147179 model2 loss : 0.103403
iteration 5620 : model1 loss : 0.150534 model2 loss : 0.136069
iteration 5621 : model1 loss : 0.163845 model2 loss : 0.106456
iteration 5622 : model1 loss : 0.141823 model2 loss : 0.105642
iteration 5623 : model1 loss : 0.162134 model2 loss : 0.126827
iteration 5624 : model1 loss : 0.179390 model2 loss : 0.103938
iteration 5625 : model1 loss : 0.133518 model2 loss : 0.092524
iteration 5626 : model1 loss : 0.142393 model2 loss : 0.118898
iteration 5627 : model1 loss : 0.193632 model2 loss : 0.155814
 56%|███████████████▏           | 331/589 [2:14:59<1:56:11, 27.02s/it]iteration 5628 : model1 loss : 0.159873 model2 loss : 0.115597
iteration 5629 : model1 loss : 0.101008 model2 loss : 0.096720
iteration 5630 : model1 loss : 0.216845 model2 loss : 0.151527
iteration 5631 : model1 loss : 0.111198 model2 loss : 0.097731
iteration 5632 : model1 loss : 0.142860 model2 loss : 0.139675
iteration 5633 : model1 loss : 0.120385 model2 loss : 0.097911
iteration 5634 : model1 loss : 0.192057 model2 loss : 0.139881
iteration 5635 : model1 loss : 0.143266 model2 loss : 0.129827
iteration 5636 : model1 loss : 0.151898 model2 loss : 0.139993
iteration 5637 : model1 loss : 0.159943 model2 loss : 0.150828
iteration 5638 : model1 loss : 0.144759 model2 loss : 0.105072
iteration 5639 : model1 loss : 0.117093 model2 loss : 0.101585
iteration 5640 : model1 loss : 0.162107 model2 loss : 0.107378
iteration 5641 : model1 loss : 0.123804 model2 loss : 0.105853
iteration 5642 : model1 loss : 0.125740 model2 loss : 0.099977
iteration 5643 : model1 loss : 0.132202 model2 loss : 0.100140
iteration 5644 : model1 loss : 0.140942 model2 loss : 0.133952
 56%|███████████████▏           | 332/589 [2:15:22<1:50:14, 25.74s/it]iteration 5645 : model1 loss : 0.109393 model2 loss : 0.105189
iteration 5646 : model1 loss : 0.158704 model2 loss : 0.106815
iteration 5647 : model1 loss : 0.155141 model2 loss : 0.120612
iteration 5648 : model1 loss : 0.163175 model2 loss : 0.150553
iteration 5649 : model1 loss : 0.124608 model2 loss : 0.082385
iteration 5650 : model1 loss : 0.160767 model2 loss : 0.101822
iteration 5651 : model1 loss : 0.178186 model2 loss : 0.118038
iteration 5652 : model1 loss : 0.135673 model2 loss : 0.116237
iteration 5653 : model1 loss : 0.139240 model2 loss : 0.136892
iteration 5654 : model1 loss : 0.131440 model2 loss : 0.124310
iteration 5655 : model1 loss : 0.158884 model2 loss : 0.135630
iteration 5656 : model1 loss : 0.156858 model2 loss : 0.156759
iteration 5657 : model1 loss : 0.213996 model2 loss : 0.150015
iteration 5658 : model1 loss : 0.112597 model2 loss : 0.110982
iteration 5659 : model1 loss : 0.130171 model2 loss : 0.091978
iteration 5660 : model1 loss : 0.104143 model2 loss : 0.099222
iteration 5661 : model1 loss : 0.179738 model2 loss : 0.110664
 57%|███████████████▎           | 333/589 [2:15:44<1:46:06, 24.87s/it]iteration 5662 : model1 loss : 0.144411 model2 loss : 0.118753
iteration 5663 : model1 loss : 0.145513 model2 loss : 0.098601
iteration 5664 : model1 loss : 0.169661 model2 loss : 0.177095
iteration 5665 : model1 loss : 0.156312 model2 loss : 0.109542
iteration 5666 : model1 loss : 0.126304 model2 loss : 0.129658
iteration 5667 : model1 loss : 0.135716 model2 loss : 0.107852
iteration 5668 : model1 loss : 0.136685 model2 loss : 0.100049
iteration 5669 : model1 loss : 0.141853 model2 loss : 0.109008
iteration 5670 : model1 loss : 0.143288 model2 loss : 0.112786
iteration 5671 : model1 loss : 0.155834 model2 loss : 0.147614
iteration 5672 : model1 loss : 0.162587 model2 loss : 0.175448
iteration 5673 : model1 loss : 0.205173 model2 loss : 0.113045
iteration 5674 : model1 loss : 0.142525 model2 loss : 0.119272
iteration 5675 : model1 loss : 0.134445 model2 loss : 0.121389
iteration 5676 : model1 loss : 0.130692 model2 loss : 0.098897
iteration 5677 : model1 loss : 0.125731 model2 loss : 0.113227
iteration 5678 : model1 loss : 0.136470 model2 loss : 0.118305
 57%|███████████████▎           | 334/589 [2:16:07<1:42:59, 24.23s/it]iteration 5679 : model1 loss : 0.138574 model2 loss : 0.138079
iteration 5680 : model1 loss : 0.155115 model2 loss : 0.126764
iteration 5681 : model1 loss : 0.138756 model2 loss : 0.109987
iteration 5682 : model1 loss : 0.131148 model2 loss : 0.116121
iteration 5683 : model1 loss : 0.102101 model2 loss : 0.128371
iteration 5684 : model1 loss : 0.156871 model2 loss : 0.127856
iteration 5685 : model1 loss : 0.178862 model2 loss : 0.178006
iteration 5686 : model1 loss : 0.140912 model2 loss : 0.223860
iteration 5687 : model1 loss : 0.137939 model2 loss : 0.107237
iteration 5688 : model1 loss : 0.132751 model2 loss : 0.113245
iteration 5689 : model1 loss : 0.214624 model2 loss : 0.160401
iteration 5690 : model1 loss : 0.174385 model2 loss : 0.138264
iteration 5691 : model1 loss : 0.133943 model2 loss : 0.119180
iteration 5692 : model1 loss : 0.187159 model2 loss : 0.148587
iteration 5693 : model1 loss : 0.161287 model2 loss : 0.122575
iteration 5694 : model1 loss : 0.142594 model2 loss : 0.164262
iteration 5695 : model1 loss : 0.145737 model2 loss : 0.092453
 57%|███████████████▎           | 335/589 [2:16:30<1:40:39, 23.78s/it]iteration 5696 : model1 loss : 0.155871 model2 loss : 0.174865
iteration 5697 : model1 loss : 0.144394 model2 loss : 0.112602
iteration 5698 : model1 loss : 0.165174 model2 loss : 0.137694
iteration 5699 : model1 loss : 0.160977 model2 loss : 0.133861
iteration 5700 : model1 loss : 0.111299 model2 loss : 0.124672
iteration 5701 : model1 loss : 0.145849 model2 loss : 0.123033
iteration 5702 : model1 loss : 0.131423 model2 loss : 0.153389
iteration 5703 : model1 loss : 0.170969 model2 loss : 0.109232
iteration 5704 : model1 loss : 0.138893 model2 loss : 0.135243
iteration 5705 : model1 loss : 0.119776 model2 loss : 0.160285
iteration 5706 : model1 loss : 0.127358 model2 loss : 0.114183
iteration 5707 : model1 loss : 0.185687 model2 loss : 0.149411
iteration 5708 : model1 loss : 0.167481 model2 loss : 0.106695
iteration 5709 : model1 loss : 0.158814 model2 loss : 0.137858
iteration 5710 : model1 loss : 0.143666 model2 loss : 0.125163
iteration 5711 : model1 loss : 0.143627 model2 loss : 0.168659
iteration 5712 : model1 loss : 0.172467 model2 loss : 0.107877
 57%|███████████████▍           | 336/589 [2:16:53<1:39:01, 23.48s/it]iteration 5713 : model1 loss : 0.140965 model2 loss : 0.107779
iteration 5714 : model1 loss : 0.143842 model2 loss : 0.108691
iteration 5715 : model1 loss : 0.169031 model2 loss : 0.119123
iteration 5716 : model1 loss : 0.133756 model2 loss : 0.130063
iteration 5717 : model1 loss : 0.141393 model2 loss : 0.121515
iteration 5718 : model1 loss : 0.156204 model2 loss : 0.149535
iteration 5719 : model1 loss : 0.198557 model2 loss : 0.153313
iteration 5720 : model1 loss : 0.122246 model2 loss : 0.129361
iteration 5721 : model1 loss : 0.162685 model2 loss : 0.144905
iteration 5722 : model1 loss : 0.165226 model2 loss : 0.153840
iteration 5723 : model1 loss : 0.163632 model2 loss : 0.122141
iteration 5724 : model1 loss : 0.145903 model2 loss : 0.099856
iteration 5725 : model1 loss : 0.125669 model2 loss : 0.108898
iteration 5726 : model1 loss : 0.139816 model2 loss : 0.133137
iteration 5727 : model1 loss : 0.126917 model2 loss : 0.130238
iteration 5728 : model1 loss : 0.137499 model2 loss : 0.148182
iteration 5729 : model1 loss : 0.100643 model2 loss : 0.087986
 57%|███████████████▍           | 337/589 [2:17:15<1:37:42, 23.27s/it]iteration 5730 : model1 loss : 0.107166 model2 loss : 0.096437
iteration 5731 : model1 loss : 0.160629 model2 loss : 0.109074
iteration 5732 : model1 loss : 0.155933 model2 loss : 0.133486
iteration 5733 : model1 loss : 0.149983 model2 loss : 0.115271
iteration 5734 : model1 loss : 0.120262 model2 loss : 0.135803
iteration 5735 : model1 loss : 0.173725 model2 loss : 0.106730
iteration 5736 : model1 loss : 0.172671 model2 loss : 0.104483
iteration 5737 : model1 loss : 0.129799 model2 loss : 0.100545
iteration 5738 : model1 loss : 0.163470 model2 loss : 0.129219
iteration 5739 : model1 loss : 0.130843 model2 loss : 0.104921
iteration 5740 : model1 loss : 0.147746 model2 loss : 0.108632
iteration 5741 : model1 loss : 0.183890 model2 loss : 0.120222
iteration 5742 : model1 loss : 0.193471 model2 loss : 0.142566
iteration 5743 : model1 loss : 0.134809 model2 loss : 0.155835
iteration 5744 : model1 loss : 0.129306 model2 loss : 0.135303
iteration 5745 : model1 loss : 0.172268 model2 loss : 0.104433
iteration 5746 : model1 loss : 0.144494 model2 loss : 0.094111
 57%|███████████████▍           | 338/589 [2:17:38<1:36:43, 23.12s/it]iteration 5747 : model1 loss : 0.179153 model2 loss : 0.129345
iteration 5748 : model1 loss : 0.129733 model2 loss : 0.111708
iteration 5749 : model1 loss : 0.169439 model2 loss : 0.126145
iteration 5750 : model1 loss : 0.153694 model2 loss : 0.121824
iteration 5751 : model1 loss : 0.158984 model2 loss : 0.112456
iteration 5752 : model1 loss : 0.141785 model2 loss : 0.086108
iteration 5753 : model1 loss : 0.113035 model2 loss : 0.113195
iteration 5754 : model1 loss : 0.144089 model2 loss : 0.103167
iteration 5755 : model1 loss : 0.128355 model2 loss : 0.099759
iteration 5756 : model1 loss : 0.151566 model2 loss : 0.143549
iteration 5757 : model1 loss : 0.186880 model2 loss : 0.130404
iteration 5758 : model1 loss : 0.182900 model2 loss : 0.144268
iteration 5759 : model1 loss : 0.167045 model2 loss : 0.111374
iteration 5760 : model1 loss : 0.187120 model2 loss : 0.104957
iteration 5761 : model1 loss : 0.120511 model2 loss : 0.073425
iteration 5762 : model1 loss : 0.144808 model2 loss : 0.153199
iteration 5763 : model1 loss : 0.188475 model2 loss : 0.094188
 58%|███████████████▌           | 339/589 [2:18:01<1:36:02, 23.05s/it]iteration 5764 : model1 loss : 0.147856 model2 loss : 0.130466
iteration 5765 : model1 loss : 0.117178 model2 loss : 0.097713
iteration 5766 : model1 loss : 0.148276 model2 loss : 0.101149
iteration 5767 : model1 loss : 0.128376 model2 loss : 0.112293
iteration 5768 : model1 loss : 0.101842 model2 loss : 0.091777
iteration 5769 : model1 loss : 0.156439 model2 loss : 0.107030
iteration 5770 : model1 loss : 0.116752 model2 loss : 0.085308
iteration 5771 : model1 loss : 0.168989 model2 loss : 0.110925
iteration 5772 : model1 loss : 0.243570 model2 loss : 0.144771
iteration 5773 : model1 loss : 0.175742 model2 loss : 0.150318
iteration 5774 : model1 loss : 0.135309 model2 loss : 0.129673
iteration 5775 : model1 loss : 0.164706 model2 loss : 0.186294
iteration 5776 : model1 loss : 0.151512 model2 loss : 0.134076
iteration 5777 : model1 loss : 0.115119 model2 loss : 0.107506
iteration 5778 : model1 loss : 0.131998 model2 loss : 0.120815
iteration 5779 : model1 loss : 0.130447 model2 loss : 0.141621
iteration 5780 : model1 loss : 0.189274 model2 loss : 0.182088
 58%|███████████████▌           | 340/589 [2:18:24<1:35:18, 22.97s/it]iteration 5781 : model1 loss : 0.136516 model2 loss : 0.119716
iteration 5782 : model1 loss : 0.150842 model2 loss : 0.122733
iteration 5783 : model1 loss : 0.129263 model2 loss : 0.107086
iteration 5784 : model1 loss : 0.179030 model2 loss : 0.134098
iteration 5785 : model1 loss : 0.161214 model2 loss : 0.145707
iteration 5786 : model1 loss : 0.158624 model2 loss : 0.127683
iteration 5787 : model1 loss : 0.143659 model2 loss : 0.124530
iteration 5788 : model1 loss : 0.204433 model2 loss : 0.126764
iteration 5789 : model1 loss : 0.116430 model2 loss : 0.101607
iteration 5790 : model1 loss : 0.183512 model2 loss : 0.124635
iteration 5791 : model1 loss : 0.135743 model2 loss : 0.099415
iteration 5792 : model1 loss : 0.138117 model2 loss : 0.125937
iteration 5793 : model1 loss : 0.160833 model2 loss : 0.146006
iteration 5794 : model1 loss : 0.178588 model2 loss : 0.120673
iteration 5795 : model1 loss : 0.190054 model2 loss : 0.147077
iteration 5796 : model1 loss : 0.143875 model2 loss : 0.118716
iteration 5797 : model1 loss : 0.118123 model2 loss : 0.096613
 58%|███████████████▋           | 341/589 [2:18:47<1:34:42, 22.91s/it]iteration 5798 : model1 loss : 0.126622 model2 loss : 0.096081
iteration 5799 : model1 loss : 0.091150 model2 loss : 0.090035
iteration 5800 : model1 loss : 0.206246 model2 loss : 0.149223
iteration 5800 : model1_mean_dice : 0.715272 model1_mean_hd95 : 87.587217 model1_mean_iou : 0.588971
iteration 5800 : model2_mean_dice : 0.746144 model2_mean_hd95 : 76.300949 model2_mean_iou : 0.633541
iteration 5801 : model1 loss : 0.144808 model2 loss : 0.116404
iteration 5802 : model1 loss : 0.166110 model2 loss : 0.125861
iteration 5803 : model1 loss : 0.146439 model2 loss : 0.110173
iteration 5804 : model1 loss : 0.174544 model2 loss : 0.108486
iteration 5805 : model1 loss : 0.133737 model2 loss : 0.127682
iteration 5806 : model1 loss : 0.121667 model2 loss : 0.119023
iteration 5807 : model1 loss : 0.134685 model2 loss : 0.135974
iteration 5808 : model1 loss : 0.177166 model2 loss : 0.107089
iteration 5809 : model1 loss : 0.144370 model2 loss : 0.105894
iteration 5810 : model1 loss : 0.201199 model2 loss : 0.147136
iteration 5811 : model1 loss : 0.194727 model2 loss : 0.128367
iteration 5812 : model1 loss : 0.121623 model2 loss : 0.123600
iteration 5813 : model1 loss : 0.134995 model2 loss : 0.091381
iteration 5814 : model1 loss : 0.155953 model2 loss : 0.107169
Traceback (most recent call last):
  File "/ext3/miniconda3/envs/ssl/lib/python3.9/multiprocessing/queues.py", line 251, in _feed
    send_bytes(obj)
  File "/ext3/miniconda3/envs/ssl/lib/python3.9/multiprocessing/connection.py", line 205, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/ext3/miniconda3/envs/ssl/lib/python3.9/multiprocessing/connection.py", line 416, in _send_bytes
    self._send(header + buf)
  File "/ext3/miniconda3/envs/ssl/lib/python3.9/multiprocessing/connection.py", line 373, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
 58%|███████████████▋           | 342/589 [2:19:29<1:58:52, 28.88s/it]iteration 5815 : model1 loss : 0.192126 model2 loss : 0.173696
iteration 5816 : model1 loss : 0.145305 model2 loss : 0.166353
iteration 5817 : model1 loss : 0.150318 model2 loss : 0.108851
iteration 5818 : model1 loss : 0.113691 model2 loss : 0.116256
iteration 5819 : model1 loss : 0.127040 model2 loss : 0.104725
iteration 5820 : model1 loss : 0.163322 model2 loss : 0.120692
iteration 5821 : model1 loss : 0.170862 model2 loss : 0.113784
iteration 5822 : model1 loss : 0.116873 model2 loss : 0.099565
iteration 5823 : model1 loss : 0.123755 model2 loss : 0.109927
iteration 5824 : model1 loss : 0.186644 model2 loss : 0.179717
iteration 5825 : model1 loss : 0.119272 model2 loss : 0.100346
iteration 5826 : model1 loss : 0.149376 model2 loss : 0.122890
iteration 5827 : model1 loss : 0.154845 model2 loss : 0.137557
iteration 5828 : model1 loss : 0.164911 model2 loss : 0.131551
iteration 5829 : model1 loss : 0.148807 model2 loss : 0.092913
iteration 5830 : model1 loss : 0.159699 model2 loss : 0.120126
iteration 5831 : model1 loss : 0.107892 model2 loss : 0.101866
 58%|███████████████▋           | 343/589 [2:19:52<1:50:51, 27.04s/it]iteration 5832 : model1 loss : 0.115681 model2 loss : 0.109378
iteration 5833 : model1 loss : 0.197393 model2 loss : 0.147830
iteration 5834 : model1 loss : 0.146441 model2 loss : 0.143978
iteration 5835 : model1 loss : 0.145266 model2 loss : 0.106221
iteration 5836 : model1 loss : 0.138962 model2 loss : 0.148970
iteration 5837 : model1 loss : 0.146713 model2 loss : 0.123041
iteration 5838 : model1 loss : 0.153922 model2 loss : 0.110982
iteration 5839 : model1 loss : 0.145828 model2 loss : 0.107008
iteration 5840 : model1 loss : 0.170692 model2 loss : 0.117148
iteration 5841 : model1 loss : 0.127611 model2 loss : 0.119613
iteration 5842 : model1 loss : 0.147984 model2 loss : 0.102941
iteration 5843 : model1 loss : 0.139502 model2 loss : 0.123403
iteration 5844 : model1 loss : 0.102518 model2 loss : 0.094712
iteration 5845 : model1 loss : 0.173851 model2 loss : 0.123472
iteration 5846 : model1 loss : 0.130676 model2 loss : 0.129939
iteration 5847 : model1 loss : 0.174272 model2 loss : 0.105653
iteration 5848 : model1 loss : 0.135791 model2 loss : 0.119307
 58%|███████████████▊           | 344/589 [2:20:15<1:45:10, 25.76s/it]iteration 5849 : model1 loss : 0.122616 model2 loss : 0.112714
iteration 5850 : model1 loss : 0.172135 model2 loss : 0.172632
iteration 5851 : model1 loss : 0.137805 model2 loss : 0.120779
iteration 5852 : model1 loss : 0.150585 model2 loss : 0.091611
iteration 5853 : model1 loss : 0.127272 model2 loss : 0.105211
iteration 5854 : model1 loss : 0.171045 model2 loss : 0.127595
iteration 5855 : model1 loss : 0.160844 model2 loss : 0.148897
iteration 5856 : model1 loss : 0.160700 model2 loss : 0.118543
iteration 5857 : model1 loss : 0.176915 model2 loss : 0.180254
iteration 5858 : model1 loss : 0.092779 model2 loss : 0.079896
iteration 5859 : model1 loss : 0.145404 model2 loss : 0.120854
iteration 5860 : model1 loss : 0.130885 model2 loss : 0.098807
iteration 5861 : model1 loss : 0.174462 model2 loss : 0.129897
iteration 5862 : model1 loss : 0.149089 model2 loss : 0.125892
iteration 5863 : model1 loss : 0.105108 model2 loss : 0.150911
iteration 5864 : model1 loss : 0.159737 model2 loss : 0.120924
iteration 5865 : model1 loss : 0.123905 model2 loss : 0.105476
 59%|███████████████▊           | 345/589 [2:20:38<1:41:14, 24.90s/it]iteration 5866 : model1 loss : 0.136524 model2 loss : 0.116899
iteration 5867 : model1 loss : 0.172814 model2 loss : 0.120486
iteration 5868 : model1 loss : 0.136725 model2 loss : 0.099694
iteration 5869 : model1 loss : 0.127048 model2 loss : 0.099291
iteration 5870 : model1 loss : 0.101876 model2 loss : 0.078124
iteration 5871 : model1 loss : 0.110773 model2 loss : 0.084898
iteration 5872 : model1 loss : 0.157415 model2 loss : 0.124096
iteration 5873 : model1 loss : 0.149039 model2 loss : 0.105521
iteration 5874 : model1 loss : 0.154952 model2 loss : 0.110396
iteration 5875 : model1 loss : 0.151783 model2 loss : 0.221523
iteration 5876 : model1 loss : 0.123315 model2 loss : 0.125922
iteration 5877 : model1 loss : 0.165430 model2 loss : 0.142284
iteration 5878 : model1 loss : 0.158872 model2 loss : 0.109473
iteration 5879 : model1 loss : 0.135332 model2 loss : 0.098156
iteration 5880 : model1 loss : 0.149095 model2 loss : 0.117215
iteration 5881 : model1 loss : 0.129815 model2 loss : 0.105163
iteration 5882 : model1 loss : 0.166270 model2 loss : 0.141023
 59%|███████████████▊           | 346/589 [2:21:01<1:38:16, 24.27s/it]iteration 5883 : model1 loss : 0.108531 model2 loss : 0.098097
iteration 5884 : model1 loss : 0.160602 model2 loss : 0.109768
iteration 5885 : model1 loss : 0.158355 model2 loss : 0.089389
iteration 5886 : model1 loss : 0.144817 model2 loss : 0.109759
iteration 5887 : model1 loss : 0.198065 model2 loss : 0.172929
iteration 5888 : model1 loss : 0.124565 model2 loss : 0.108516
iteration 5889 : model1 loss : 0.142961 model2 loss : 0.132112
iteration 5890 : model1 loss : 0.097811 model2 loss : 0.092584
iteration 5891 : model1 loss : 0.127151 model2 loss : 0.120796
iteration 5892 : model1 loss : 0.182571 model2 loss : 0.152676
iteration 5893 : model1 loss : 0.165043 model2 loss : 0.132242
iteration 5894 : model1 loss : 0.124770 model2 loss : 0.118532
iteration 5895 : model1 loss : 0.177747 model2 loss : 0.143942
iteration 5896 : model1 loss : 0.151199 model2 loss : 0.090672
iteration 5897 : model1 loss : 0.118235 model2 loss : 0.080782
iteration 5898 : model1 loss : 0.156675 model2 loss : 0.119905
iteration 5899 : model1 loss : 0.151448 model2 loss : 0.137453
 59%|███████████████▉           | 347/589 [2:21:23<1:36:02, 23.81s/it]iteration 5900 : model1 loss : 0.130685 model2 loss : 0.082250
iteration 5901 : model1 loss : 0.137254 model2 loss : 0.135147
iteration 5902 : model1 loss : 0.170238 model2 loss : 0.111187
iteration 5903 : model1 loss : 0.171238 model2 loss : 0.125745
iteration 5904 : model1 loss : 0.181338 model2 loss : 0.110337
iteration 5905 : model1 loss : 0.123439 model2 loss : 0.104913
iteration 5906 : model1 loss : 0.132172 model2 loss : 0.100864
iteration 5907 : model1 loss : 0.139814 model2 loss : 0.106839
iteration 5908 : model1 loss : 0.135408 model2 loss : 0.114378
iteration 5909 : model1 loss : 0.101140 model2 loss : 0.111826
iteration 5910 : model1 loss : 0.146249 model2 loss : 0.120682
iteration 5911 : model1 loss : 0.133579 model2 loss : 0.123675
iteration 5912 : model1 loss : 0.149135 model2 loss : 0.098190
iteration 5913 : model1 loss : 0.214246 model2 loss : 0.130824
iteration 5914 : model1 loss : 0.161944 model2 loss : 0.132482
iteration 5915 : model1 loss : 0.118976 model2 loss : 0.098145
iteration 5916 : model1 loss : 0.179546 model2 loss : 0.118318
 59%|███████████████▉           | 348/589 [2:21:46<1:34:40, 23.57s/it]iteration 5917 : model1 loss : 0.171710 model2 loss : 0.140190
iteration 5918 : model1 loss : 0.153907 model2 loss : 0.125533
iteration 5919 : model1 loss : 0.163064 model2 loss : 0.139887
iteration 5920 : model1 loss : 0.132128 model2 loss : 0.088999
iteration 5921 : model1 loss : 0.126176 model2 loss : 0.112481
iteration 5922 : model1 loss : 0.114751 model2 loss : 0.088778
iteration 5923 : model1 loss : 0.179787 model2 loss : 0.149903
iteration 5924 : model1 loss : 0.159073 model2 loss : 0.129523
iteration 5925 : model1 loss : 0.187227 model2 loss : 0.135852
iteration 5926 : model1 loss : 0.153303 model2 loss : 0.111889
iteration 5927 : model1 loss : 0.157367 model2 loss : 0.139365
iteration 5928 : model1 loss : 0.135172 model2 loss : 0.120268
iteration 5929 : model1 loss : 0.142620 model2 loss : 0.124626
iteration 5930 : model1 loss : 0.132492 model2 loss : 0.120704
iteration 5931 : model1 loss : 0.168169 model2 loss : 0.123764
iteration 5932 : model1 loss : 0.136097 model2 loss : 0.098269
iteration 5933 : model1 loss : 0.137962 model2 loss : 0.126873
 59%|███████████████▉           | 349/589 [2:22:09<1:33:17, 23.32s/it]iteration 5934 : model1 loss : 0.144931 model2 loss : 0.155355
iteration 5935 : model1 loss : 0.145511 model2 loss : 0.115199
iteration 5936 : model1 loss : 0.116982 model2 loss : 0.118905
iteration 5937 : model1 loss : 0.115867 model2 loss : 0.087114
iteration 5938 : model1 loss : 0.102513 model2 loss : 0.096426
iteration 5939 : model1 loss : 0.149359 model2 loss : 0.123170
iteration 5940 : model1 loss : 0.135020 model2 loss : 0.123381
iteration 5941 : model1 loss : 0.122443 model2 loss : 0.121438
iteration 5942 : model1 loss : 0.154330 model2 loss : 0.096893
iteration 5943 : model1 loss : 0.148672 model2 loss : 0.137881
iteration 5944 : model1 loss : 0.159209 model2 loss : 0.118220
iteration 5945 : model1 loss : 0.157212 model2 loss : 0.153572
iteration 5946 : model1 loss : 0.157309 model2 loss : 0.092408
iteration 5947 : model1 loss : 0.177375 model2 loss : 0.126561
iteration 5948 : model1 loss : 0.171357 model2 loss : 0.120161
iteration 5949 : model1 loss : 0.160952 model2 loss : 0.123479
iteration 5950 : model1 loss : 0.109998 model2 loss : 0.118271
 59%|████████████████           | 350/589 [2:22:32<1:32:24, 23.20s/it]iteration 5951 : model1 loss : 0.132350 model2 loss : 0.122246
iteration 5952 : model1 loss : 0.149135 model2 loss : 0.133752
iteration 5953 : model1 loss : 0.166197 model2 loss : 0.116408
iteration 5954 : model1 loss : 0.111525 model2 loss : 0.090682
iteration 5955 : model1 loss : 0.159650 model2 loss : 0.130423
iteration 5956 : model1 loss : 0.142067 model2 loss : 0.157973
iteration 5957 : model1 loss : 0.125707 model2 loss : 0.105581
iteration 5958 : model1 loss : 0.191566 model2 loss : 0.144290
iteration 5959 : model1 loss : 0.155473 model2 loss : 0.104552
iteration 5960 : model1 loss : 0.116651 model2 loss : 0.098632
iteration 5961 : model1 loss : 0.113712 model2 loss : 0.090739
iteration 5962 : model1 loss : 0.137772 model2 loss : 0.122263
iteration 5963 : model1 loss : 0.138865 model2 loss : 0.110788
iteration 5964 : model1 loss : 0.170906 model2 loss : 0.131653
iteration 5965 : model1 loss : 0.116589 model2 loss : 0.078914
iteration 5966 : model1 loss : 0.132506 model2 loss : 0.105438
iteration 5967 : model1 loss : 0.164937 model2 loss : 0.103306
 60%|████████████████           | 351/589 [2:22:55<1:31:32, 23.08s/it]iteration 5968 : model1 loss : 0.142736 model2 loss : 0.114749
iteration 5969 : model1 loss : 0.133138 model2 loss : 0.095188
iteration 5970 : model1 loss : 0.160672 model2 loss : 0.121952
iteration 5971 : model1 loss : 0.190876 model2 loss : 0.103953
iteration 5972 : model1 loss : 0.133515 model2 loss : 0.124856
iteration 5973 : model1 loss : 0.143191 model2 loss : 0.116565
iteration 5974 : model1 loss : 0.149571 model2 loss : 0.100869
iteration 5975 : model1 loss : 0.153396 model2 loss : 0.147821
iteration 5976 : model1 loss : 0.126415 model2 loss : 0.100428
iteration 5977 : model1 loss : 0.147521 model2 loss : 0.122173
iteration 5978 : model1 loss : 0.164733 model2 loss : 0.143032
iteration 5979 : model1 loss : 0.119097 model2 loss : 0.118649
iteration 5980 : model1 loss : 0.110980 model2 loss : 0.116229
iteration 5981 : model1 loss : 0.143249 model2 loss : 0.155269
iteration 5982 : model1 loss : 0.166034 model2 loss : 0.118529
iteration 5983 : model1 loss : 0.159835 model2 loss : 0.161716
iteration 5984 : model1 loss : 0.103993 model2 loss : 0.085407
 60%|████████████████▏          | 352/589 [2:23:18<1:30:44, 22.97s/it]iteration 5985 : model1 loss : 0.124199 model2 loss : 0.086493
iteration 5986 : model1 loss : 0.138428 model2 loss : 0.099302
iteration 5987 : model1 loss : 0.137487 model2 loss : 0.143780
iteration 5988 : model1 loss : 0.136581 model2 loss : 0.104513
iteration 5989 : model1 loss : 0.144117 model2 loss : 0.119336
iteration 5990 : model1 loss : 0.114502 model2 loss : 0.102522
iteration 5991 : model1 loss : 0.143561 model2 loss : 0.131051
iteration 5992 : model1 loss : 0.134989 model2 loss : 0.105663
iteration 5993 : model1 loss : 0.196417 model2 loss : 0.146703
iteration 5994 : model1 loss : 0.128220 model2 loss : 0.127520
iteration 5995 : model1 loss : 0.166753 model2 loss : 0.148442
iteration 5996 : model1 loss : 0.117308 model2 loss : 0.086981
iteration 5997 : model1 loss : 0.159400 model2 loss : 0.133709
iteration 5998 : model1 loss : 0.168696 model2 loss : 0.129984
iteration 5999 : model1 loss : 0.136506 model2 loss : 0.121524
iteration 6000 : model1 loss : 0.108331 model2 loss : 0.116426
iteration 6000 : model1_mean_dice : 0.706942 model1_mean_hd95 : 88.840574 model1_mean_iou : 0.578227
iteration 6000 : model2_mean_dice : 0.767367 model2_mean_hd95 : 67.515273 model2_mean_iou : 0.656655
save model1 to ../model/Dermofit/Cross_Teaching_Between_CNN_Transformer_7/unet/model1_iter_6000.pth
save model2 to ../model/Dermofit/Cross_Teaching_Between_CNN_Transformer_7/unet/model2_iter_6000.pth
iteration 6001 : model1 loss : 0.136380 model2 loss : 0.119071
 60%|████████████████▏          | 353/589 [2:24:01<1:54:08, 29.02s/it]iteration 6002 : model1 loss : 0.129597 model2 loss : 0.111959
iteration 6003 : model1 loss : 0.142021 model2 loss : 0.109644
iteration 6004 : model1 loss : 0.155582 model2 loss : 0.120961
iteration 6005 : model1 loss : 0.149211 model2 loss : 0.111025
iteration 6006 : model1 loss : 0.114952 model2 loss : 0.134524
iteration 6007 : model1 loss : 0.181656 model2 loss : 0.088613
iteration 6008 : model1 loss : 0.181134 model2 loss : 0.091779
iteration 6009 : model1 loss : 0.184097 model2 loss : 0.142264
iteration 6010 : model1 loss : 0.126462 model2 loss : 0.099483
iteration 6011 : model1 loss : 0.135632 model2 loss : 0.106226
iteration 6012 : model1 loss : 0.167119 model2 loss : 0.123024
iteration 6013 : model1 loss : 0.126148 model2 loss : 0.103786
iteration 6014 : model1 loss : 0.150462 model2 loss : 0.117397
iteration 6015 : model1 loss : 0.175615 model2 loss : 0.108773
iteration 6016 : model1 loss : 0.148500 model2 loss : 0.105320
iteration 6017 : model1 loss : 0.095162 model2 loss : 0.083739
iteration 6018 : model1 loss : 0.140661 model2 loss : 0.095811
 60%|████████████████▏          | 354/589 [2:24:23<1:46:13, 27.12s/it]iteration 6019 : model1 loss : 0.163213 model2 loss : 0.111284
iteration 6020 : model1 loss : 0.164912 model2 loss : 0.095836
iteration 6021 : model1 loss : 0.155877 model2 loss : 0.148238
iteration 6022 : model1 loss : 0.180230 model2 loss : 0.119573
iteration 6023 : model1 loss : 0.135027 model2 loss : 0.164743
iteration 6024 : model1 loss : 0.133221 model2 loss : 0.115637
iteration 6025 : model1 loss : 0.151106 model2 loss : 0.077690
iteration 6026 : model1 loss : 0.184821 model2 loss : 0.148438
iteration 6027 : model1 loss : 0.151844 model2 loss : 0.126615
iteration 6028 : model1 loss : 0.151136 model2 loss : 0.116953
iteration 6029 : model1 loss : 0.151085 model2 loss : 0.131104
iteration 6030 : model1 loss : 0.107153 model2 loss : 0.109187
iteration 6031 : model1 loss : 0.139158 model2 loss : 0.160014
iteration 6032 : model1 loss : 0.148654 model2 loss : 0.142927
iteration 6033 : model1 loss : 0.158582 model2 loss : 0.115529
iteration 6034 : model1 loss : 0.160450 model2 loss : 0.133761
iteration 6035 : model1 loss : 0.127511 model2 loss : 0.107608
 60%|████████████████▎          | 355/589 [2:24:46<1:40:38, 25.81s/it]iteration 6036 : model1 loss : 0.154891 model2 loss : 0.142642
iteration 6037 : model1 loss : 0.153746 model2 loss : 0.137739
iteration 6038 : model1 loss : 0.125235 model2 loss : 0.098685
iteration 6039 : model1 loss : 0.122059 model2 loss : 0.101731
iteration 6040 : model1 loss : 0.158957 model2 loss : 0.121540
iteration 6041 : model1 loss : 0.158333 model2 loss : 0.160944
iteration 6042 : model1 loss : 0.171833 model2 loss : 0.142699
iteration 6043 : model1 loss : 0.167376 model2 loss : 0.132034
iteration 6044 : model1 loss : 0.126298 model2 loss : 0.090716
iteration 6045 : model1 loss : 0.117090 model2 loss : 0.125686
iteration 6046 : model1 loss : 0.125466 model2 loss : 0.128241
iteration 6047 : model1 loss : 0.145733 model2 loss : 0.149211
iteration 6048 : model1 loss : 0.147985 model2 loss : 0.121225
iteration 6049 : model1 loss : 0.125738 model2 loss : 0.111788
iteration 6050 : model1 loss : 0.233335 model2 loss : 0.115366
iteration 6051 : model1 loss : 0.127930 model2 loss : 0.099483
iteration 6052 : model1 loss : 0.178184 model2 loss : 0.135559
 60%|████████████████▎          | 356/589 [2:25:09<1:36:44, 24.91s/it]iteration 6053 : model1 loss : 0.106683 model2 loss : 0.106461
iteration 6054 : model1 loss : 0.124665 model2 loss : 0.115515
iteration 6055 : model1 loss : 0.140219 model2 loss : 0.114127
iteration 6056 : model1 loss : 0.131859 model2 loss : 0.092456
iteration 6057 : model1 loss : 0.142784 model2 loss : 0.098919
iteration 6058 : model1 loss : 0.170634 model2 loss : 0.148203
iteration 6059 : model1 loss : 0.138249 model2 loss : 0.103221
iteration 6060 : model1 loss : 0.160201 model2 loss : 0.110527
iteration 6061 : model1 loss : 0.140057 model2 loss : 0.105258
iteration 6062 : model1 loss : 0.168856 model2 loss : 0.164840
iteration 6063 : model1 loss : 0.155555 model2 loss : 0.143708
iteration 6064 : model1 loss : 0.119908 model2 loss : 0.093092
iteration 6065 : model1 loss : 0.139709 model2 loss : 0.112758
iteration 6066 : model1 loss : 0.137479 model2 loss : 0.110806
iteration 6067 : model1 loss : 0.115180 model2 loss : 0.131209
iteration 6068 : model1 loss : 0.162116 model2 loss : 0.149995
iteration 6069 : model1 loss : 0.152226 model2 loss : 0.101358
 61%|████████████████▎          | 357/589 [2:25:32<1:33:51, 24.27s/it]iteration 6070 : model1 loss : 0.117583 model2 loss : 0.097491
iteration 6071 : model1 loss : 0.120551 model2 loss : 0.088456
iteration 6072 : model1 loss : 0.170187 model2 loss : 0.112820
iteration 6073 : model1 loss : 0.138528 model2 loss : 0.069964
iteration 6074 : model1 loss : 0.134641 model2 loss : 0.120160
iteration 6075 : model1 loss : 0.111769 model2 loss : 0.119839
iteration 6076 : model1 loss : 0.162022 model2 loss : 0.138314
iteration 6077 : model1 loss : 0.129996 model2 loss : 0.138936
iteration 6078 : model1 loss : 0.176546 model2 loss : 0.122547
iteration 6079 : model1 loss : 0.129193 model2 loss : 0.100697
iteration 6080 : model1 loss : 0.141834 model2 loss : 0.116090
iteration 6081 : model1 loss : 0.141458 model2 loss : 0.179219
iteration 6082 : model1 loss : 0.187169 model2 loss : 0.160587
iteration 6083 : model1 loss : 0.157990 model2 loss : 0.113351
iteration 6084 : model1 loss : 0.114689 model2 loss : 0.098122
iteration 6085 : model1 loss : 0.102409 model2 loss : 0.072653
iteration 6086 : model1 loss : 0.193038 model2 loss : 0.127945
 61%|████████████████▍          | 358/589 [2:25:55<1:31:40, 23.81s/it]iteration 6087 : model1 loss : 0.169509 model2 loss : 0.120724
iteration 6088 : model1 loss : 0.110733 model2 loss : 0.096313
iteration 6089 : model1 loss : 0.152096 model2 loss : 0.117278
iteration 6090 : model1 loss : 0.171138 model2 loss : 0.159623
iteration 6091 : model1 loss : 0.141879 model2 loss : 0.120335
iteration 6092 : model1 loss : 0.140689 model2 loss : 0.088498
iteration 6093 : model1 loss : 0.127309 model2 loss : 0.110442
iteration 6094 : model1 loss : 0.182751 model2 loss : 0.146115
iteration 6095 : model1 loss : 0.118058 model2 loss : 0.142008
iteration 6096 : model1 loss : 0.179038 model2 loss : 0.107167
iteration 6097 : model1 loss : 0.143390 model2 loss : 0.122261
iteration 6098 : model1 loss : 0.133278 model2 loss : 0.081392
iteration 6099 : model1 loss : 0.177891 model2 loss : 0.100000
iteration 6100 : model1 loss : 0.120048 model2 loss : 0.095299
iteration 6101 : model1 loss : 0.118172 model2 loss : 0.102298
iteration 6102 : model1 loss : 0.139016 model2 loss : 0.118757
iteration 6103 : model1 loss : 0.152545 model2 loss : 0.116593
 61%|████████████████▍          | 359/589 [2:26:17<1:30:16, 23.55s/it]iteration 6104 : model1 loss : 0.142849 model2 loss : 0.122623
iteration 6105 : model1 loss : 0.159153 model2 loss : 0.114051
iteration 6106 : model1 loss : 0.110216 model2 loss : 0.097678
iteration 6107 : model1 loss : 0.104694 model2 loss : 0.111796
iteration 6108 : model1 loss : 0.169806 model2 loss : 0.095467
iteration 6109 : model1 loss : 0.150798 model2 loss : 0.125901
iteration 6110 : model1 loss : 0.155096 model2 loss : 0.123971
iteration 6111 : model1 loss : 0.144464 model2 loss : 0.124318
iteration 6112 : model1 loss : 0.162293 model2 loss : 0.117930
iteration 6113 : model1 loss : 0.105702 model2 loss : 0.092757
iteration 6114 : model1 loss : 0.151667 model2 loss : 0.090270
iteration 6115 : model1 loss : 0.162708 model2 loss : 0.114929
iteration 6116 : model1 loss : 0.141924 model2 loss : 0.098323
iteration 6117 : model1 loss : 0.135870 model2 loss : 0.118027
iteration 6118 : model1 loss : 0.143850 model2 loss : 0.125278
iteration 6119 : model1 loss : 0.133624 model2 loss : 0.115259
iteration 6120 : model1 loss : 0.138768 model2 loss : 0.120691
 61%|████████████████▌          | 360/589 [2:26:40<1:28:58, 23.31s/it]iteration 6121 : model1 loss : 0.153569 model2 loss : 0.137332
iteration 6122 : model1 loss : 0.143054 model2 loss : 0.108046
iteration 6123 : model1 loss : 0.153597 model2 loss : 0.094598
iteration 6124 : model1 loss : 0.115084 model2 loss : 0.100096
iteration 6125 : model1 loss : 0.149425 model2 loss : 0.089748
iteration 6126 : model1 loss : 0.114686 model2 loss : 0.102572
iteration 6127 : model1 loss : 0.189729 model2 loss : 0.176506
iteration 6128 : model1 loss : 0.126149 model2 loss : 0.091114
iteration 6129 : model1 loss : 0.177967 model2 loss : 0.099001
iteration 6130 : model1 loss : 0.123241 model2 loss : 0.111200
iteration 6131 : model1 loss : 0.165843 model2 loss : 0.114434
iteration 6132 : model1 loss : 0.143021 model2 loss : 0.099131
iteration 6133 : model1 loss : 0.136465 model2 loss : 0.106976
iteration 6134 : model1 loss : 0.192378 model2 loss : 0.148356
iteration 6135 : model1 loss : 0.118395 model2 loss : 0.100094
iteration 6136 : model1 loss : 0.093237 model2 loss : 0.094151
iteration 6137 : model1 loss : 0.146212 model2 loss : 0.119115
 61%|████████████████▌          | 361/589 [2:27:03<1:28:02, 23.17s/it]iteration 6138 : model1 loss : 0.150977 model2 loss : 0.119007
iteration 6139 : model1 loss : 0.120518 model2 loss : 0.108429
iteration 6140 : model1 loss : 0.143597 model2 loss : 0.107228
iteration 6141 : model1 loss : 0.164556 model2 loss : 0.135044
iteration 6142 : model1 loss : 0.128323 model2 loss : 0.114743
iteration 6143 : model1 loss : 0.120823 model2 loss : 0.130218
iteration 6144 : model1 loss : 0.134084 model2 loss : 0.110846
iteration 6145 : model1 loss : 0.132582 model2 loss : 0.138844
iteration 6146 : model1 loss : 0.183674 model2 loss : 0.127385
iteration 6147 : model1 loss : 0.143011 model2 loss : 0.106782
iteration 6148 : model1 loss : 0.177118 model2 loss : 0.109322
iteration 6149 : model1 loss : 0.141641 model2 loss : 0.105778
iteration 6150 : model1 loss : 0.126033 model2 loss : 0.109440
iteration 6151 : model1 loss : 0.178481 model2 loss : 0.125327
iteration 6152 : model1 loss : 0.123373 model2 loss : 0.110721
iteration 6153 : model1 loss : 0.130631 model2 loss : 0.099390
iteration 6154 : model1 loss : 0.128952 model2 loss : 0.095690
 61%|████████████████▌          | 362/589 [2:27:26<1:27:21, 23.09s/it]iteration 6155 : model1 loss : 0.150346 model2 loss : 0.107663
iteration 6156 : model1 loss : 0.119921 model2 loss : 0.103036
iteration 6157 : model1 loss : 0.152060 model2 loss : 0.122395
iteration 6158 : model1 loss : 0.126543 model2 loss : 0.094361
iteration 6159 : model1 loss : 0.120566 model2 loss : 0.108404
iteration 6160 : model1 loss : 0.123900 model2 loss : 0.127262
iteration 6161 : model1 loss : 0.184700 model2 loss : 0.141124
iteration 6162 : model1 loss : 0.117482 model2 loss : 0.092762
iteration 6163 : model1 loss : 0.142660 model2 loss : 0.121491
iteration 6164 : model1 loss : 0.135123 model2 loss : 0.103924
iteration 6165 : model1 loss : 0.129049 model2 loss : 0.102017
iteration 6166 : model1 loss : 0.161113 model2 loss : 0.110741
iteration 6167 : model1 loss : 0.131285 model2 loss : 0.095166
iteration 6168 : model1 loss : 0.130160 model2 loss : 0.097523
iteration 6169 : model1 loss : 0.144463 model2 loss : 0.083343
iteration 6170 : model1 loss : 0.136930 model2 loss : 0.116123
iteration 6171 : model1 loss : 0.133932 model2 loss : 0.099210
 62%|████████████████▋          | 363/589 [2:27:49<1:26:39, 23.01s/it]iteration 6172 : model1 loss : 0.150627 model2 loss : 0.122647
iteration 6173 : model1 loss : 0.121836 model2 loss : 0.098217
iteration 6174 : model1 loss : 0.152773 model2 loss : 0.089974
iteration 6175 : model1 loss : 0.174043 model2 loss : 0.124417
iteration 6176 : model1 loss : 0.167297 model2 loss : 0.136121
iteration 6177 : model1 loss : 0.197699 model2 loss : 0.117809
iteration 6178 : model1 loss : 0.150957 model2 loss : 0.100203
iteration 6179 : model1 loss : 0.116812 model2 loss : 0.099480
iteration 6180 : model1 loss : 0.094690 model2 loss : 0.080053
iteration 6181 : model1 loss : 0.105633 model2 loss : 0.130619
iteration 6182 : model1 loss : 0.212972 model2 loss : 0.140028
iteration 6183 : model1 loss : 0.122576 model2 loss : 0.122229
iteration 6184 : model1 loss : 0.114259 model2 loss : 0.096153
iteration 6185 : model1 loss : 0.125706 model2 loss : 0.126378
iteration 6186 : model1 loss : 0.119600 model2 loss : 0.104676
iteration 6187 : model1 loss : 0.179404 model2 loss : 0.099416
iteration 6188 : model1 loss : 0.181699 model2 loss : 0.132930
 62%|████████████████▋          | 364/589 [2:28:12<1:26:01, 22.94s/it]iteration 6189 : model1 loss : 0.097915 model2 loss : 0.081880
iteration 6190 : model1 loss : 0.129749 model2 loss : 0.094774
iteration 6191 : model1 loss : 0.215890 model2 loss : 0.128536
iteration 6192 : model1 loss : 0.154458 model2 loss : 0.110027
iteration 6193 : model1 loss : 0.101364 model2 loss : 0.105565
iteration 6194 : model1 loss : 0.180494 model2 loss : 0.141918
iteration 6195 : model1 loss : 0.125274 model2 loss : 0.088722
iteration 6196 : model1 loss : 0.148714 model2 loss : 0.133952
iteration 6197 : model1 loss : 0.127194 model2 loss : 0.102373
iteration 6198 : model1 loss : 0.154773 model2 loss : 0.107052
iteration 6199 : model1 loss : 0.131844 model2 loss : 0.109289
iteration 6200 : model1 loss : 0.136892 model2 loss : 0.137362
iteration 6200 : model1_mean_dice : 0.702460 model1_mean_hd95 : 88.781539 model1_mean_iou : 0.574189
iteration 6200 : model2_mean_dice : 0.772035 model2_mean_hd95 : 63.006247 model2_mean_iou : 0.664068
iteration 6201 : model1 loss : 0.167195 model2 loss : 0.098239
iteration 6202 : model1 loss : 0.148213 model2 loss : 0.143127
iteration 6203 : model1 loss : 0.139745 model2 loss : 0.115713
iteration 6204 : model1 loss : 0.123591 model2 loss : 0.077806
iteration 6205 : model1 loss : 0.115939 model2 loss : 0.109714
 62%|████████████████▋          | 365/589 [2:28:55<1:48:04, 28.95s/it]iteration 6206 : model1 loss : 0.117399 model2 loss : 0.072517
iteration 6207 : model1 loss : 0.133414 model2 loss : 0.092175
iteration 6208 : model1 loss : 0.175051 model2 loss : 0.143871
iteration 6209 : model1 loss : 0.140661 model2 loss : 0.130558
iteration 6210 : model1 loss : 0.128197 model2 loss : 0.118024
iteration 6211 : model1 loss : 0.153107 model2 loss : 0.130669
iteration 6212 : model1 loss : 0.125733 model2 loss : 0.131593
iteration 6213 : model1 loss : 0.146310 model2 loss : 0.114950
iteration 6214 : model1 loss : 0.143055 model2 loss : 0.094109
iteration 6215 : model1 loss : 0.144610 model2 loss : 0.107834
iteration 6216 : model1 loss : 0.115895 model2 loss : 0.110272
iteration 6217 : model1 loss : 0.183050 model2 loss : 0.135399
iteration 6218 : model1 loss : 0.119975 model2 loss : 0.117530
iteration 6219 : model1 loss : 0.134398 model2 loss : 0.106363
iteration 6220 : model1 loss : 0.126077 model2 loss : 0.106598
iteration 6221 : model1 loss : 0.160304 model2 loss : 0.120160
iteration 6222 : model1 loss : 0.111489 model2 loss : 0.090523
 62%|████████████████▊          | 366/589 [2:29:17<1:40:37, 27.08s/it]iteration 6223 : model1 loss : 0.154867 model2 loss : 0.110620
iteration 6224 : model1 loss : 0.144148 model2 loss : 0.100990
iteration 6225 : model1 loss : 0.156279 model2 loss : 0.131436
iteration 6226 : model1 loss : 0.096406 model2 loss : 0.075813
iteration 6227 : model1 loss : 0.134268 model2 loss : 0.106287
iteration 6228 : model1 loss : 0.121761 model2 loss : 0.113656
iteration 6229 : model1 loss : 0.155623 model2 loss : 0.111984
iteration 6230 : model1 loss : 0.169848 model2 loss : 0.141853
iteration 6231 : model1 loss : 0.134876 model2 loss : 0.130409
iteration 6232 : model1 loss : 0.146521 model2 loss : 0.148893
iteration 6233 : model1 loss : 0.153846 model2 loss : 0.112397
iteration 6234 : model1 loss : 0.106405 model2 loss : 0.118409
iteration 6235 : model1 loss : 0.100300 model2 loss : 0.091315
iteration 6236 : model1 loss : 0.220565 model2 loss : 0.189871
iteration 6237 : model1 loss : 0.152956 model2 loss : 0.120299
iteration 6238 : model1 loss : 0.110199 model2 loss : 0.096078
iteration 6239 : model1 loss : 0.147118 model2 loss : 0.101812
 62%|████████████████▊          | 367/589 [2:29:40<1:35:20, 25.77s/it]iteration 6240 : model1 loss : 0.125043 model2 loss : 0.102460
iteration 6241 : model1 loss : 0.201711 model2 loss : 0.129450
iteration 6242 : model1 loss : 0.163398 model2 loss : 0.106723
iteration 6243 : model1 loss : 0.145256 model2 loss : 0.117046
iteration 6244 : model1 loss : 0.108307 model2 loss : 0.105798
iteration 6245 : model1 loss : 0.113921 model2 loss : 0.095754
iteration 6246 : model1 loss : 0.146091 model2 loss : 0.128533
iteration 6247 : model1 loss : 0.103704 model2 loss : 0.093868
iteration 6248 : model1 loss : 0.172736 model2 loss : 0.147590
iteration 6249 : model1 loss : 0.147393 model2 loss : 0.123905
iteration 6250 : model1 loss : 0.186522 model2 loss : 0.149361
iteration 6251 : model1 loss : 0.145497 model2 loss : 0.112820
iteration 6252 : model1 loss : 0.138806 model2 loss : 0.140928
iteration 6253 : model1 loss : 0.114948 model2 loss : 0.088571
iteration 6254 : model1 loss : 0.148155 model2 loss : 0.107127
iteration 6255 : model1 loss : 0.114420 model2 loss : 0.159124
iteration 6256 : model1 loss : 0.169124 model2 loss : 0.081788
 62%|████████████████▊          | 368/589 [2:30:03<1:31:42, 24.90s/it]iteration 6257 : model1 loss : 0.128497 model2 loss : 0.106553
iteration 6258 : model1 loss : 0.137114 model2 loss : 0.117778
iteration 6259 : model1 loss : 0.150342 model2 loss : 0.136338
iteration 6260 : model1 loss : 0.146427 model2 loss : 0.096201
iteration 6261 : model1 loss : 0.116797 model2 loss : 0.090153
iteration 6262 : model1 loss : 0.130088 model2 loss : 0.094877
iteration 6263 : model1 loss : 0.164898 model2 loss : 0.105331
iteration 6264 : model1 loss : 0.174775 model2 loss : 0.137109
iteration 6265 : model1 loss : 0.175666 model2 loss : 0.109647
iteration 6266 : model1 loss : 0.121925 model2 loss : 0.085730
iteration 6267 : model1 loss : 0.163022 model2 loss : 0.148890
iteration 6268 : model1 loss : 0.125178 model2 loss : 0.089105
iteration 6269 : model1 loss : 0.123569 model2 loss : 0.089159
iteration 6270 : model1 loss : 0.160710 model2 loss : 0.095303
iteration 6271 : model1 loss : 0.122575 model2 loss : 0.095695
iteration 6272 : model1 loss : 0.138504 model2 loss : 0.117710
iteration 6273 : model1 loss : 0.171353 model2 loss : 0.117794
 63%|████████████████▉          | 369/589 [2:30:26<1:28:52, 24.24s/it]iteration 6274 : model1 loss : 0.185287 model2 loss : 0.186604
iteration 6275 : model1 loss : 0.132598 model2 loss : 0.096972
iteration 6276 : model1 loss : 0.141533 model2 loss : 0.095114
iteration 6277 : model1 loss : 0.120619 model2 loss : 0.127642
iteration 6278 : model1 loss : 0.114444 model2 loss : 0.102766
iteration 6279 : model1 loss : 0.114003 model2 loss : 0.095530
iteration 6280 : model1 loss : 0.157991 model2 loss : 0.136270
iteration 6281 : model1 loss : 0.156127 model2 loss : 0.089573
iteration 6282 : model1 loss : 0.178611 model2 loss : 0.102374
iteration 6283 : model1 loss : 0.126419 model2 loss : 0.111314
iteration 6284 : model1 loss : 0.161445 model2 loss : 0.137924
iteration 6285 : model1 loss : 0.124764 model2 loss : 0.103514
iteration 6286 : model1 loss : 0.119691 model2 loss : 0.088301
iteration 6287 : model1 loss : 0.142404 model2 loss : 0.111339
iteration 6288 : model1 loss : 0.156188 model2 loss : 0.157687
iteration 6289 : model1 loss : 0.127365 model2 loss : 0.094901
iteration 6290 : model1 loss : 0.143070 model2 loss : 0.105114
 63%|████████████████▉          | 370/589 [2:30:48<1:26:52, 23.80s/it]iteration 6291 : model1 loss : 0.147727 model2 loss : 0.112870
iteration 6292 : model1 loss : 0.133910 model2 loss : 0.110980
iteration 6293 : model1 loss : 0.137657 model2 loss : 0.107153
iteration 6294 : model1 loss : 0.137171 model2 loss : 0.084700
iteration 6295 : model1 loss : 0.121485 model2 loss : 0.099075
iteration 6296 : model1 loss : 0.152129 model2 loss : 0.141511
iteration 6297 : model1 loss : 0.165808 model2 loss : 0.141518
iteration 6298 : model1 loss : 0.151689 model2 loss : 0.108186
iteration 6299 : model1 loss : 0.139709 model2 loss : 0.121287
iteration 6300 : model1 loss : 0.189193 model2 loss : 0.103868
iteration 6301 : model1 loss : 0.123362 model2 loss : 0.101396
iteration 6302 : model1 loss : 0.175938 model2 loss : 0.108234
iteration 6303 : model1 loss : 0.105509 model2 loss : 0.093324
iteration 6304 : model1 loss : 0.145153 model2 loss : 0.153578
iteration 6305 : model1 loss : 0.176708 model2 loss : 0.123910
iteration 6306 : model1 loss : 0.108660 model2 loss : 0.096587
iteration 6307 : model1 loss : 0.105356 model2 loss : 0.097405
 63%|█████████████████          | 371/589 [2:31:11<1:25:26, 23.52s/it]iteration 6308 : model1 loss : 0.134655 model2 loss : 0.144687
iteration 6309 : model1 loss : 0.152205 model2 loss : 0.124078
iteration 6310 : model1 loss : 0.106541 model2 loss : 0.101348
iteration 6311 : model1 loss : 0.162042 model2 loss : 0.102015
iteration 6312 : model1 loss : 0.098241 model2 loss : 0.087064
iteration 6313 : model1 loss : 0.153015 model2 loss : 0.145142
iteration 6314 : model1 loss : 0.194791 model2 loss : 0.115409
iteration 6315 : model1 loss : 0.183189 model2 loss : 0.130778
iteration 6316 : model1 loss : 0.142217 model2 loss : 0.120283
iteration 6317 : model1 loss : 0.161013 model2 loss : 0.139047
iteration 6318 : model1 loss : 0.158347 model2 loss : 0.113870
iteration 6319 : model1 loss : 0.134950 model2 loss : 0.102795
iteration 6320 : model1 loss : 0.116071 model2 loss : 0.117200
iteration 6321 : model1 loss : 0.107509 model2 loss : 0.084044
iteration 6322 : model1 loss : 0.158435 model2 loss : 0.093913
iteration 6323 : model1 loss : 0.118537 model2 loss : 0.094060
iteration 6324 : model1 loss : 0.119076 model2 loss : 0.102340
 63%|█████████████████          | 372/589 [2:31:34<1:24:14, 23.29s/it]iteration 6325 : model1 loss : 0.114778 model2 loss : 0.101556
iteration 6326 : model1 loss : 0.114735 model2 loss : 0.097090
iteration 6327 : model1 loss : 0.171332 model2 loss : 0.096045
iteration 6328 : model1 loss : 0.127620 model2 loss : 0.107689
iteration 6329 : model1 loss : 0.135097 model2 loss : 0.108339
iteration 6330 : model1 loss : 0.119805 model2 loss : 0.089086
iteration 6331 : model1 loss : 0.162171 model2 loss : 0.140929
iteration 6332 : model1 loss : 0.172060 model2 loss : 0.101674
iteration 6333 : model1 loss : 0.130902 model2 loss : 0.107125
iteration 6334 : model1 loss : 0.136462 model2 loss : 0.120276
iteration 6335 : model1 loss : 0.164460 model2 loss : 0.102713
iteration 6336 : model1 loss : 0.140994 model2 loss : 0.098299
iteration 6337 : model1 loss : 0.152466 model2 loss : 0.118186
iteration 6338 : model1 loss : 0.140063 model2 loss : 0.110324
iteration 6339 : model1 loss : 0.178240 model2 loss : 0.149924
iteration 6340 : model1 loss : 0.119761 model2 loss : 0.093764
iteration 6341 : model1 loss : 0.115463 model2 loss : 0.162066
 63%|█████████████████          | 373/589 [2:31:57<1:23:15, 23.13s/it]iteration 6342 : model1 loss : 0.135748 model2 loss : 0.111393
iteration 6343 : model1 loss : 0.157317 model2 loss : 0.142337
iteration 6344 : model1 loss : 0.159735 model2 loss : 0.098817
iteration 6345 : model1 loss : 0.167640 model2 loss : 0.101745
iteration 6346 : model1 loss : 0.093199 model2 loss : 0.084265
iteration 6347 : model1 loss : 0.136565 model2 loss : 0.111208
iteration 6348 : model1 loss : 0.203370 model2 loss : 0.164682
iteration 6349 : model1 loss : 0.126093 model2 loss : 0.105061
iteration 6350 : model1 loss : 0.129307 model2 loss : 0.128795
iteration 6351 : model1 loss : 0.137319 model2 loss : 0.104310
iteration 6352 : model1 loss : 0.125290 model2 loss : 0.115228
iteration 6353 : model1 loss : 0.130756 model2 loss : 0.127867
iteration 6354 : model1 loss : 0.122836 model2 loss : 0.093904
iteration 6355 : model1 loss : 0.142483 model2 loss : 0.087835
iteration 6356 : model1 loss : 0.119620 model2 loss : 0.116241
iteration 6357 : model1 loss : 0.167625 model2 loss : 0.142869
iteration 6358 : model1 loss : 0.108595 model2 loss : 0.117084
 63%|█████████████████▏         | 374/589 [2:32:20<1:22:40, 23.07s/it]iteration 6359 : model1 loss : 0.097265 model2 loss : 0.096098
iteration 6360 : model1 loss : 0.126617 model2 loss : 0.133898
iteration 6361 : model1 loss : 0.152377 model2 loss : 0.143408
iteration 6362 : model1 loss : 0.135263 model2 loss : 0.138617
iteration 6363 : model1 loss : 0.164773 model2 loss : 0.132438
iteration 6364 : model1 loss : 0.129359 model2 loss : 0.082275
iteration 6365 : model1 loss : 0.171389 model2 loss : 0.102241
iteration 6366 : model1 loss : 0.148101 model2 loss : 0.127738
iteration 6367 : model1 loss : 0.112995 model2 loss : 0.099152
iteration 6368 : model1 loss : 0.131976 model2 loss : 0.107886
iteration 6369 : model1 loss : 0.162547 model2 loss : 0.107311
iteration 6370 : model1 loss : 0.172962 model2 loss : 0.123061
iteration 6371 : model1 loss : 0.198746 model2 loss : 0.178447
iteration 6372 : model1 loss : 0.129611 model2 loss : 0.106783
iteration 6373 : model1 loss : 0.139727 model2 loss : 0.118044
iteration 6374 : model1 loss : 0.103906 model2 loss : 0.089206
iteration 6375 : model1 loss : 0.128784 model2 loss : 0.086440
 64%|█████████████████▏         | 375/589 [2:32:42<1:22:00, 22.99s/it]iteration 6376 : model1 loss : 0.160016 model2 loss : 0.085372
iteration 6377 : model1 loss : 0.144979 model2 loss : 0.151482
iteration 6378 : model1 loss : 0.143990 model2 loss : 0.201260
iteration 6379 : model1 loss : 0.156028 model2 loss : 0.124783
iteration 6380 : model1 loss : 0.126579 model2 loss : 0.110931
iteration 6381 : model1 loss : 0.163366 model2 loss : 0.123241
iteration 6382 : model1 loss : 0.122874 model2 loss : 0.099708
iteration 6383 : model1 loss : 0.195144 model2 loss : 0.159243
iteration 6384 : model1 loss : 0.142106 model2 loss : 0.122319
iteration 6385 : model1 loss : 0.131330 model2 loss : 0.091898
iteration 6386 : model1 loss : 0.157124 model2 loss : 0.132521
iteration 6387 : model1 loss : 0.142969 model2 loss : 0.119031
iteration 6388 : model1 loss : 0.151111 model2 loss : 0.129149
iteration 6389 : model1 loss : 0.110600 model2 loss : 0.097098
iteration 6390 : model1 loss : 0.138934 model2 loss : 0.101484
iteration 6391 : model1 loss : 0.108109 model2 loss : 0.074023
iteration 6392 : model1 loss : 0.170371 model2 loss : 0.120133
 64%|█████████████████▏         | 376/589 [2:33:05<1:21:26, 22.94s/it]iteration 6393 : model1 loss : 0.117123 model2 loss : 0.081315
iteration 6394 : model1 loss : 0.130497 model2 loss : 0.104169
iteration 6395 : model1 loss : 0.112949 model2 loss : 0.071378
iteration 6396 : model1 loss : 0.167321 model2 loss : 0.130774
iteration 6397 : model1 loss : 0.097927 model2 loss : 0.073985
iteration 6398 : model1 loss : 0.170229 model2 loss : 0.138328
iteration 6399 : model1 loss : 0.177885 model2 loss : 0.160841
iteration 6400 : model1 loss : 0.116217 model2 loss : 0.091996
iteration 6400 : model1_mean_dice : 0.715049 model1_mean_hd95 : 85.121494 model1_mean_iou : 0.587877
iteration 6400 : model2_mean_dice : 0.742839 model2_mean_hd95 : 68.317858 model2_mean_iou : 0.633977
iteration 6401 : model1 loss : 0.121800 model2 loss : 0.093471
iteration 6402 : model1 loss : 0.122736 model2 loss : 0.105664
iteration 6403 : model1 loss : 0.122920 model2 loss : 0.095161
iteration 6404 : model1 loss : 0.165805 model2 loss : 0.115967
iteration 6405 : model1 loss : 0.175592 model2 loss : 0.135600
iteration 6406 : model1 loss : 0.128451 model2 loss : 0.106329
iteration 6407 : model1 loss : 0.159149 model2 loss : 0.141730
iteration 6408 : model1 loss : 0.120798 model2 loss : 0.101740
iteration 6409 : model1 loss : 0.173344 model2 loss : 0.105623
 64%|█████████████████▎         | 377/589 [2:33:48<1:42:25, 28.99s/it]iteration 6410 : model1 loss : 0.124590 model2 loss : 0.095557
iteration 6411 : model1 loss : 0.168162 model2 loss : 0.123388
iteration 6412 : model1 loss : 0.137419 model2 loss : 0.107305
iteration 6413 : model1 loss : 0.153029 model2 loss : 0.097584
iteration 6414 : model1 loss : 0.110237 model2 loss : 0.109906
iteration 6415 : model1 loss : 0.148702 model2 loss : 0.125645
iteration 6416 : model1 loss : 0.151451 model2 loss : 0.106173
iteration 6417 : model1 loss : 0.133891 model2 loss : 0.104155
iteration 6418 : model1 loss : 0.132379 model2 loss : 0.083763
iteration 6419 : model1 loss : 0.119466 model2 loss : 0.106478
iteration 6420 : model1 loss : 0.159031 model2 loss : 0.118036
iteration 6421 : model1 loss : 0.170003 model2 loss : 0.151418
iteration 6422 : model1 loss : 0.129491 model2 loss : 0.114885
iteration 6423 : model1 loss : 0.158226 model2 loss : 0.102595
iteration 6424 : model1 loss : 0.129506 model2 loss : 0.108397
iteration 6425 : model1 loss : 0.145783 model2 loss : 0.112959
iteration 6426 : model1 loss : 0.110018 model2 loss : 0.084369
 64%|█████████████████▎         | 378/589 [2:34:11<1:35:46, 27.24s/it]iteration 6427 : model1 loss : 0.142760 model2 loss : 0.127907
iteration 6428 : model1 loss : 0.188195 model2 loss : 0.129377
iteration 6429 : model1 loss : 0.170138 model2 loss : 0.111308
iteration 6430 : model1 loss : 0.115325 model2 loss : 0.106591
iteration 6431 : model1 loss : 0.143096 model2 loss : 0.118627
iteration 6432 : model1 loss : 0.168583 model2 loss : 0.113897
iteration 6433 : model1 loss : 0.175666 model2 loss : 0.122582
iteration 6434 : model1 loss : 0.117615 model2 loss : 0.129590
iteration 6435 : model1 loss : 0.103575 model2 loss : 0.075016
iteration 6436 : model1 loss : 0.138479 model2 loss : 0.086267
iteration 6437 : model1 loss : 0.166948 model2 loss : 0.189933
iteration 6438 : model1 loss : 0.105666 model2 loss : 0.090725
iteration 6439 : model1 loss : 0.122487 model2 loss : 0.088737
iteration 6440 : model1 loss : 0.147477 model2 loss : 0.092274
iteration 6441 : model1 loss : 0.132156 model2 loss : 0.116425
iteration 6442 : model1 loss : 0.137258 model2 loss : 0.128938
iteration 6443 : model1 loss : 0.134000 model2 loss : 0.082782
 64%|█████████████████▎         | 379/589 [2:34:35<1:31:11, 26.06s/it]iteration 6444 : model1 loss : 0.177488 model2 loss : 0.146831
iteration 6445 : model1 loss : 0.129940 model2 loss : 0.104545
iteration 6446 : model1 loss : 0.162362 model2 loss : 0.104563
iteration 6447 : model1 loss : 0.120773 model2 loss : 0.094345
iteration 6448 : model1 loss : 0.110237 model2 loss : 0.079501
iteration 6449 : model1 loss : 0.173912 model2 loss : 0.093436
iteration 6450 : model1 loss : 0.152190 model2 loss : 0.100891
iteration 6451 : model1 loss : 0.146909 model2 loss : 0.081400
iteration 6452 : model1 loss : 0.133650 model2 loss : 0.100825
iteration 6453 : model1 loss : 0.123443 model2 loss : 0.089187
iteration 6454 : model1 loss : 0.130028 model2 loss : 0.107291
iteration 6455 : model1 loss : 0.140897 model2 loss : 0.113966
iteration 6456 : model1 loss : 0.143525 model2 loss : 0.092627
iteration 6457 : model1 loss : 0.166646 model2 loss : 0.101351
iteration 6458 : model1 loss : 0.125231 model2 loss : 0.101168
iteration 6459 : model1 loss : 0.123733 model2 loss : 0.113002
iteration 6460 : model1 loss : 0.147459 model2 loss : 0.112115
 65%|█████████████████▍         | 380/589 [2:34:58<1:27:24, 25.09s/it]iteration 6461 : model1 loss : 0.104942 model2 loss : 0.100497
iteration 6462 : model1 loss : 0.157727 model2 loss : 0.101385
iteration 6463 : model1 loss : 0.141051 model2 loss : 0.117565
iteration 6464 : model1 loss : 0.168658 model2 loss : 0.109947
iteration 6465 : model1 loss : 0.114539 model2 loss : 0.084126
iteration 6466 : model1 loss : 0.126876 model2 loss : 0.098358
iteration 6467 : model1 loss : 0.184350 model2 loss : 0.112715
iteration 6468 : model1 loss : 0.152438 model2 loss : 0.087035
iteration 6469 : model1 loss : 0.166829 model2 loss : 0.087758
iteration 6470 : model1 loss : 0.108725 model2 loss : 0.094919
iteration 6471 : model1 loss : 0.157062 model2 loss : 0.132556
iteration 6472 : model1 loss : 0.133287 model2 loss : 0.098583
iteration 6473 : model1 loss : 0.170256 model2 loss : 0.145758
iteration 6474 : model1 loss : 0.187584 model2 loss : 0.139492
iteration 6475 : model1 loss : 0.120354 model2 loss : 0.092267
iteration 6476 : model1 loss : 0.116398 model2 loss : 0.086748
iteration 6477 : model1 loss : 0.121422 model2 loss : 0.092030
 65%|█████████████████▍         | 381/589 [2:35:20<1:24:34, 24.40s/it]iteration 6478 : model1 loss : 0.167788 model2 loss : 0.125456
iteration 6479 : model1 loss : 0.102593 model2 loss : 0.088260
iteration 6480 : model1 loss : 0.161517 model2 loss : 0.098524
iteration 6481 : model1 loss : 0.178103 model2 loss : 0.150244
iteration 6482 : model1 loss : 0.121171 model2 loss : 0.098499
iteration 6483 : model1 loss : 0.166835 model2 loss : 0.131847
iteration 6484 : model1 loss : 0.208339 model2 loss : 0.127535
iteration 6485 : model1 loss : 0.153955 model2 loss : 0.115469
iteration 6486 : model1 loss : 0.126023 model2 loss : 0.102348
iteration 6487 : model1 loss : 0.119407 model2 loss : 0.083364
iteration 6488 : model1 loss : 0.181828 model2 loss : 0.121903
iteration 6489 : model1 loss : 0.126091 model2 loss : 0.086116
iteration 6490 : model1 loss : 0.124492 model2 loss : 0.100673
iteration 6491 : model1 loss : 0.139857 model2 loss : 0.106361
iteration 6492 : model1 loss : 0.138432 model2 loss : 0.114860
iteration 6493 : model1 loss : 0.107838 model2 loss : 0.132436
iteration 6494 : model1 loss : 0.165817 model2 loss : 0.131028
 65%|█████████████████▌         | 382/589 [2:35:43<1:22:26, 23.90s/it]iteration 6495 : model1 loss : 0.153339 model2 loss : 0.095598
iteration 6496 : model1 loss : 0.136087 model2 loss : 0.102442
iteration 6497 : model1 loss : 0.137987 model2 loss : 0.109146
iteration 6498 : model1 loss : 0.129824 model2 loss : 0.100305
iteration 6499 : model1 loss : 0.125303 model2 loss : 0.136826
iteration 6500 : model1 loss : 0.130538 model2 loss : 0.088265
iteration 6501 : model1 loss : 0.211477 model2 loss : 0.148836
iteration 6502 : model1 loss : 0.157434 model2 loss : 0.140737
iteration 6503 : model1 loss : 0.167672 model2 loss : 0.120954
iteration 6504 : model1 loss : 0.118210 model2 loss : 0.070959
iteration 6505 : model1 loss : 0.119956 model2 loss : 0.104326
iteration 6506 : model1 loss : 0.172199 model2 loss : 0.103922
iteration 6507 : model1 loss : 0.127062 model2 loss : 0.131719
iteration 6508 : model1 loss : 0.179982 model2 loss : 0.096017
iteration 6509 : model1 loss : 0.166088 model2 loss : 0.127062
iteration 6510 : model1 loss : 0.104538 model2 loss : 0.099683
iteration 6511 : model1 loss : 0.167767 model2 loss : 0.117588
 65%|█████████████████▌         | 383/589 [2:36:06<1:21:00, 23.60s/it]iteration 6512 : model1 loss : 0.113161 model2 loss : 0.094488
iteration 6513 : model1 loss : 0.185207 model2 loss : 0.146225
iteration 6514 : model1 loss : 0.123324 model2 loss : 0.106661
iteration 6515 : model1 loss : 0.122373 model2 loss : 0.105248
iteration 6516 : model1 loss : 0.149182 model2 loss : 0.141384
iteration 6517 : model1 loss : 0.157656 model2 loss : 0.101779
iteration 6518 : model1 loss : 0.117371 model2 loss : 0.089533
iteration 6519 : model1 loss : 0.145235 model2 loss : 0.101820
iteration 6520 : model1 loss : 0.129616 model2 loss : 0.100236
iteration 6521 : model1 loss : 0.142521 model2 loss : 0.100104
iteration 6522 : model1 loss : 0.145717 model2 loss : 0.134777
iteration 6523 : model1 loss : 0.167762 model2 loss : 0.118198
iteration 6524 : model1 loss : 0.115039 model2 loss : 0.117180
iteration 6525 : model1 loss : 0.142012 model2 loss : 0.117302
iteration 6526 : model1 loss : 0.119142 model2 loss : 0.098626
iteration 6527 : model1 loss : 0.149432 model2 loss : 0.108151
iteration 6528 : model1 loss : 0.182597 model2 loss : 0.117640
 65%|█████████████████▌         | 384/589 [2:36:29<1:19:49, 23.36s/it]iteration 6529 : model1 loss : 0.154065 model2 loss : 0.126339
iteration 6530 : model1 loss : 0.139379 model2 loss : 0.107943
iteration 6531 : model1 loss : 0.139896 model2 loss : 0.117992
iteration 6532 : model1 loss : 0.174062 model2 loss : 0.089825
iteration 6533 : model1 loss : 0.119111 model2 loss : 0.104813
iteration 6534 : model1 loss : 0.152512 model2 loss : 0.155880
iteration 6535 : model1 loss : 0.114695 model2 loss : 0.103817
iteration 6536 : model1 loss : 0.162113 model2 loss : 0.124169
iteration 6537 : model1 loss : 0.118737 model2 loss : 0.161871
iteration 6538 : model1 loss : 0.114452 model2 loss : 0.092072
iteration 6539 : model1 loss : 0.142133 model2 loss : 0.111459
iteration 6540 : model1 loss : 0.094393 model2 loss : 0.077414
iteration 6541 : model1 loss : 0.134815 model2 loss : 0.126583
iteration 6542 : model1 loss : 0.144432 model2 loss : 0.101739
iteration 6543 : model1 loss : 0.160210 model2 loss : 0.108322
iteration 6544 : model1 loss : 0.138476 model2 loss : 0.114572
iteration 6545 : model1 loss : 0.221298 model2 loss : 0.125320
 65%|█████████████████▋         | 385/589 [2:36:52<1:18:48, 23.18s/it]iteration 6546 : model1 loss : 0.132654 model2 loss : 0.135924
iteration 6547 : model1 loss : 0.163017 model2 loss : 0.126845
iteration 6548 : model1 loss : 0.118100 model2 loss : 0.101866
iteration 6549 : model1 loss : 0.137140 model2 loss : 0.114871
iteration 6550 : model1 loss : 0.076025 model2 loss : 0.071746
iteration 6551 : model1 loss : 0.142059 model2 loss : 0.121564
iteration 6552 : model1 loss : 0.142349 model2 loss : 0.082570
iteration 6553 : model1 loss : 0.149799 model2 loss : 0.099903
iteration 6554 : model1 loss : 0.117410 model2 loss : 0.090588
iteration 6555 : model1 loss : 0.153701 model2 loss : 0.088387
iteration 6556 : model1 loss : 0.121737 model2 loss : 0.105219
iteration 6557 : model1 loss : 0.148439 model2 loss : 0.124630
iteration 6558 : model1 loss : 0.211843 model2 loss : 0.182266
iteration 6559 : model1 loss : 0.149961 model2 loss : 0.098082
iteration 6560 : model1 loss : 0.150532 model2 loss : 0.099993
iteration 6561 : model1 loss : 0.166945 model2 loss : 0.148088
iteration 6562 : model1 loss : 0.156755 model2 loss : 0.114812
 66%|█████████████████▋         | 386/589 [2:37:14<1:18:07, 23.09s/it]iteration 6563 : model1 loss : 0.160864 model2 loss : 0.100059
iteration 6564 : model1 loss : 0.116175 model2 loss : 0.088461
iteration 6565 : model1 loss : 0.146175 model2 loss : 0.104504
iteration 6566 : model1 loss : 0.133623 model2 loss : 0.111341
iteration 6567 : model1 loss : 0.109881 model2 loss : 0.086205
iteration 6568 : model1 loss : 0.157610 model2 loss : 0.107253
iteration 6569 : model1 loss : 0.153371 model2 loss : 0.132237
iteration 6570 : model1 loss : 0.138619 model2 loss : 0.179255
iteration 6571 : model1 loss : 0.116586 model2 loss : 0.142542
iteration 6572 : model1 loss : 0.136136 model2 loss : 0.112952
iteration 6573 : model1 loss : 0.172426 model2 loss : 0.122704
iteration 6574 : model1 loss : 0.115258 model2 loss : 0.088870
iteration 6575 : model1 loss : 0.185013 model2 loss : 0.149396
iteration 6576 : model1 loss : 0.129384 model2 loss : 0.084246
iteration 6577 : model1 loss : 0.116211 model2 loss : 0.101774
iteration 6578 : model1 loss : 0.108187 model2 loss : 0.136890
iteration 6579 : model1 loss : 0.166519 model2 loss : 0.130253
 66%|█████████████████▋         | 387/589 [2:37:37<1:17:24, 22.99s/it]iteration 6580 : model1 loss : 0.150977 model2 loss : 0.125233
iteration 6581 : model1 loss : 0.116019 model2 loss : 0.077309
iteration 6582 : model1 loss : 0.141583 model2 loss : 0.142537
iteration 6583 : model1 loss : 0.128443 model2 loss : 0.089854
iteration 6584 : model1 loss : 0.160641 model2 loss : 0.145189
iteration 6585 : model1 loss : 0.143562 model2 loss : 0.119612
iteration 6586 : model1 loss : 0.168842 model2 loss : 0.094112
iteration 6587 : model1 loss : 0.151327 model2 loss : 0.109325
iteration 6588 : model1 loss : 0.149432 model2 loss : 0.109319
iteration 6589 : model1 loss : 0.130861 model2 loss : 0.103143
iteration 6590 : model1 loss : 0.134073 model2 loss : 0.125564
iteration 6591 : model1 loss : 0.126921 model2 loss : 0.130900
iteration 6592 : model1 loss : 0.113626 model2 loss : 0.140332
iteration 6593 : model1 loss : 0.120468 model2 loss : 0.094405
iteration 6594 : model1 loss : 0.118595 model2 loss : 0.089149
iteration 6595 : model1 loss : 0.152809 model2 loss : 0.123900
iteration 6596 : model1 loss : 0.156826 model2 loss : 0.105031
 66%|█████████████████▊         | 388/589 [2:38:00<1:16:48, 22.93s/it]iteration 6597 : model1 loss : 0.129208 model2 loss : 0.119414
iteration 6598 : model1 loss : 0.138193 model2 loss : 0.111100
iteration 6599 : model1 loss : 0.095835 model2 loss : 0.081199
iteration 6600 : model1 loss : 0.153183 model2 loss : 0.145719
iteration 6600 : model1_mean_dice : 0.704248 model1_mean_hd95 : 85.957626 model1_mean_iou : 0.577272
iteration 6600 : model2_mean_dice : 0.779724 model2_mean_hd95 : 65.968234 model2_mean_iou : 0.667790
iteration 6601 : model1 loss : 0.134468 model2 loss : 0.147166
iteration 6602 : model1 loss : 0.131366 model2 loss : 0.133174
iteration 6603 : model1 loss : 0.118312 model2 loss : 0.107035
iteration 6604 : model1 loss : 0.151209 model2 loss : 0.096747
iteration 6605 : model1 loss : 0.115367 model2 loss : 0.111253
iteration 6606 : model1 loss : 0.126829 model2 loss : 0.100295
iteration 6607 : model1 loss : 0.150717 model2 loss : 0.123871
iteration 6608 : model1 loss : 0.104451 model2 loss : 0.117852
iteration 6609 : model1 loss : 0.159541 model2 loss : 0.111922
iteration 6610 : model1 loss : 0.155457 model2 loss : 0.139649
iteration 6611 : model1 loss : 0.132245 model2 loss : 0.118701
iteration 6612 : model1 loss : 0.111967 model2 loss : 0.105219
iteration 6613 : model1 loss : 0.171108 model2 loss : 0.105379
 66%|█████████████████▊         | 389/589 [2:38:43<1:36:17, 28.89s/it]iteration 6614 : model1 loss : 0.109175 model2 loss : 0.093357
iteration 6615 : model1 loss : 0.129628 model2 loss : 0.112953
iteration 6616 : model1 loss : 0.159237 model2 loss : 0.103057
iteration 6617 : model1 loss : 0.102298 model2 loss : 0.095636
iteration 6618 : model1 loss : 0.124361 model2 loss : 0.137652
iteration 6619 : model1 loss : 0.157663 model2 loss : 0.102650
iteration 6620 : model1 loss : 0.144822 model2 loss : 0.100346
iteration 6621 : model1 loss : 0.121169 model2 loss : 0.090256
iteration 6622 : model1 loss : 0.154677 model2 loss : 0.120970
iteration 6623 : model1 loss : 0.140661 model2 loss : 0.107625
iteration 6624 : model1 loss : 0.147172 model2 loss : 0.105170
iteration 6625 : model1 loss : 0.129961 model2 loss : 0.101152
iteration 6626 : model1 loss : 0.171605 model2 loss : 0.141355
iteration 6627 : model1 loss : 0.154653 model2 loss : 0.110883
iteration 6628 : model1 loss : 0.104723 model2 loss : 0.074111
iteration 6629 : model1 loss : 0.110553 model2 loss : 0.128734
iteration 6630 : model1 loss : 0.172168 model2 loss : 0.161566
 66%|█████████████████▉         | 390/589 [2:39:06<1:29:40, 27.04s/it]iteration 6631 : model1 loss : 0.172643 model2 loss : 0.101694
iteration 6632 : model1 loss : 0.127269 model2 loss : 0.094469
iteration 6633 : model1 loss : 0.167018 model2 loss : 0.114902
iteration 6634 : model1 loss : 0.156133 model2 loss : 0.129858
iteration 6635 : model1 loss : 0.117175 model2 loss : 0.102081
iteration 6636 : model1 loss : 0.144811 model2 loss : 0.102925
iteration 6637 : model1 loss : 0.129789 model2 loss : 0.095879
iteration 6638 : model1 loss : 0.153502 model2 loss : 0.109131
iteration 6639 : model1 loss : 0.136832 model2 loss : 0.112270
iteration 6640 : model1 loss : 0.130415 model2 loss : 0.088452
iteration 6641 : model1 loss : 0.109323 model2 loss : 0.092158
iteration 6642 : model1 loss : 0.120770 model2 loss : 0.092345
iteration 6643 : model1 loss : 0.164018 model2 loss : 0.096883
iteration 6644 : model1 loss : 0.148014 model2 loss : 0.114346
iteration 6645 : model1 loss : 0.145654 model2 loss : 0.110913
iteration 6646 : model1 loss : 0.111736 model2 loss : 0.083567
iteration 6647 : model1 loss : 0.125818 model2 loss : 0.083125
 66%|█████████████████▉         | 391/589 [2:39:28<1:24:59, 25.75s/it]iteration 6648 : model1 loss : 0.136216 model2 loss : 0.081315
iteration 6649 : model1 loss : 0.144585 model2 loss : 0.093844
iteration 6650 : model1 loss : 0.149691 model2 loss : 0.167468
iteration 6651 : model1 loss : 0.174090 model2 loss : 0.103778
iteration 6652 : model1 loss : 0.149822 model2 loss : 0.129799
iteration 6653 : model1 loss : 0.131480 model2 loss : 0.116546
iteration 6654 : model1 loss : 0.147211 model2 loss : 0.128369
iteration 6655 : model1 loss : 0.105994 model2 loss : 0.092949
iteration 6656 : model1 loss : 0.216012 model2 loss : 0.114336
iteration 6657 : model1 loss : 0.107992 model2 loss : 0.088023
iteration 6658 : model1 loss : 0.132780 model2 loss : 0.122227
iteration 6659 : model1 loss : 0.148626 model2 loss : 0.117881
iteration 6660 : model1 loss : 0.143079 model2 loss : 0.091476
iteration 6661 : model1 loss : 0.104431 model2 loss : 0.087879
iteration 6662 : model1 loss : 0.141163 model2 loss : 0.112790
iteration 6663 : model1 loss : 0.121636 model2 loss : 0.113465
iteration 6664 : model1 loss : 0.173548 model2 loss : 0.100070
 67%|█████████████████▉         | 392/589 [2:39:51<1:21:43, 24.89s/it]iteration 6665 : model1 loss : 0.150053 model2 loss : 0.143804
iteration 6666 : model1 loss : 0.123254 model2 loss : 0.108131
iteration 6667 : model1 loss : 0.154537 model2 loss : 0.102540
iteration 6668 : model1 loss : 0.126494 model2 loss : 0.130527
iteration 6669 : model1 loss : 0.154462 model2 loss : 0.115630
iteration 6670 : model1 loss : 0.132221 model2 loss : 0.087878
iteration 6671 : model1 loss : 0.153735 model2 loss : 0.103363
iteration 6672 : model1 loss : 0.125823 model2 loss : 0.082385
iteration 6673 : model1 loss : 0.163563 model2 loss : 0.101750
iteration 6674 : model1 loss : 0.129037 model2 loss : 0.139734
iteration 6675 : model1 loss : 0.140409 model2 loss : 0.106326
iteration 6676 : model1 loss : 0.135002 model2 loss : 0.111727
iteration 6677 : model1 loss : 0.116600 model2 loss : 0.103991
iteration 6678 : model1 loss : 0.127754 model2 loss : 0.096662
iteration 6679 : model1 loss : 0.159574 model2 loss : 0.088976
iteration 6680 : model1 loss : 0.118989 model2 loss : 0.095957
iteration 6681 : model1 loss : 0.134851 model2 loss : 0.084541
 67%|██████████████████         | 393/589 [2:40:14<1:19:16, 24.27s/it]iteration 6682 : model1 loss : 0.168297 model2 loss : 0.107549
iteration 6683 : model1 loss : 0.112831 model2 loss : 0.109732
iteration 6684 : model1 loss : 0.148585 model2 loss : 0.092238
iteration 6685 : model1 loss : 0.119306 model2 loss : 0.080014
iteration 6686 : model1 loss : 0.118367 model2 loss : 0.120336
iteration 6687 : model1 loss : 0.144006 model2 loss : 0.102110
iteration 6688 : model1 loss : 0.167172 model2 loss : 0.136474
iteration 6689 : model1 loss : 0.140672 model2 loss : 0.099833
iteration 6690 : model1 loss : 0.135977 model2 loss : 0.103126
iteration 6691 : model1 loss : 0.103611 model2 loss : 0.068325
iteration 6692 : model1 loss : 0.132705 model2 loss : 0.096279
iteration 6693 : model1 loss : 0.118126 model2 loss : 0.084822
iteration 6694 : model1 loss : 0.112796 model2 loss : 0.098726
iteration 6695 : model1 loss : 0.159000 model2 loss : 0.136348
iteration 6696 : model1 loss : 0.181087 model2 loss : 0.145266
iteration 6697 : model1 loss : 0.115517 model2 loss : 0.138119
iteration 6698 : model1 loss : 0.193308 model2 loss : 0.132488
 67%|██████████████████         | 394/589 [2:40:37<1:17:25, 23.82s/it]iteration 6699 : model1 loss : 0.127807 model2 loss : 0.123899
iteration 6700 : model1 loss : 0.144405 model2 loss : 0.099611
iteration 6701 : model1 loss : 0.134955 model2 loss : 0.114907
iteration 6702 : model1 loss : 0.134906 model2 loss : 0.092178
iteration 6703 : model1 loss : 0.128029 model2 loss : 0.124805
iteration 6704 : model1 loss : 0.114472 model2 loss : 0.123462
iteration 6705 : model1 loss : 0.174757 model2 loss : 0.141776
iteration 6706 : model1 loss : 0.124813 model2 loss : 0.091248
iteration 6707 : model1 loss : 0.152676 model2 loss : 0.079743
iteration 6708 : model1 loss : 0.156213 model2 loss : 0.099214
iteration 6709 : model1 loss : 0.167994 model2 loss : 0.135389
iteration 6710 : model1 loss : 0.162072 model2 loss : 0.126099
iteration 6711 : model1 loss : 0.116440 model2 loss : 0.095485
iteration 6712 : model1 loss : 0.150533 model2 loss : 0.119625
iteration 6713 : model1 loss : 0.105444 model2 loss : 0.086598
iteration 6714 : model1 loss : 0.126257 model2 loss : 0.114857
iteration 6715 : model1 loss : 0.157908 model2 loss : 0.092131
 67%|██████████████████         | 395/589 [2:41:00<1:16:08, 23.55s/it]iteration 6716 : model1 loss : 0.155547 model2 loss : 0.113366
iteration 6717 : model1 loss : 0.155385 model2 loss : 0.115189
iteration 6718 : model1 loss : 0.132728 model2 loss : 0.087197
iteration 6719 : model1 loss : 0.137434 model2 loss : 0.125614
iteration 6720 : model1 loss : 0.129387 model2 loss : 0.126302
iteration 6721 : model1 loss : 0.126664 model2 loss : 0.076460
iteration 6722 : model1 loss : 0.132798 model2 loss : 0.092233
iteration 6723 : model1 loss : 0.133285 model2 loss : 0.085443
iteration 6724 : model1 loss : 0.143530 model2 loss : 0.112214
iteration 6725 : model1 loss : 0.108266 model2 loss : 0.090222
iteration 6726 : model1 loss : 0.153950 model2 loss : 0.091450
iteration 6727 : model1 loss : 0.133322 model2 loss : 0.083002
iteration 6728 : model1 loss : 0.136363 model2 loss : 0.118271
iteration 6729 : model1 loss : 0.132780 model2 loss : 0.088563
iteration 6730 : model1 loss : 0.108688 model2 loss : 0.084474
iteration 6731 : model1 loss : 0.131835 model2 loss : 0.102952
iteration 6732 : model1 loss : 0.150829 model2 loss : 0.105230
 67%|██████████████████▏        | 396/589 [2:41:22<1:14:58, 23.31s/it]iteration 6733 : model1 loss : 0.120009 model2 loss : 0.094568
iteration 6734 : model1 loss : 0.125062 model2 loss : 0.105159
iteration 6735 : model1 loss : 0.149923 model2 loss : 0.112909
iteration 6736 : model1 loss : 0.194339 model2 loss : 0.109638
iteration 6737 : model1 loss : 0.170774 model2 loss : 0.134418
iteration 6738 : model1 loss : 0.155832 model2 loss : 0.120281
iteration 6739 : model1 loss : 0.142688 model2 loss : 0.116531
iteration 6740 : model1 loss : 0.151818 model2 loss : 0.094286
iteration 6741 : model1 loss : 0.111947 model2 loss : 0.081479
iteration 6742 : model1 loss : 0.138832 model2 loss : 0.089539
iteration 6743 : model1 loss : 0.159635 model2 loss : 0.094233
iteration 6744 : model1 loss : 0.131581 model2 loss : 0.086955
iteration 6745 : model1 loss : 0.136229 model2 loss : 0.087867
iteration 6746 : model1 loss : 0.122695 model2 loss : 0.099394
iteration 6747 : model1 loss : 0.112740 model2 loss : 0.111405
iteration 6748 : model1 loss : 0.130251 model2 loss : 0.109289
iteration 6749 : model1 loss : 0.143327 model2 loss : 0.106380
 67%|██████████████████▏        | 397/589 [2:41:45<1:14:02, 23.14s/it]iteration 6750 : model1 loss : 0.143132 model2 loss : 0.135848
iteration 6751 : model1 loss : 0.119294 model2 loss : 0.091185
iteration 6752 : model1 loss : 0.160025 model2 loss : 0.106801
iteration 6753 : model1 loss : 0.151181 model2 loss : 0.080841
iteration 6754 : model1 loss : 0.237181 model2 loss : 0.111347
iteration 6755 : model1 loss : 0.154254 model2 loss : 0.123395
iteration 6756 : model1 loss : 0.151090 model2 loss : 0.096462
iteration 6757 : model1 loss : 0.117361 model2 loss : 0.091975
iteration 6758 : model1 loss : 0.172834 model2 loss : 0.095790
iteration 6759 : model1 loss : 0.138374 model2 loss : 0.106347
iteration 6760 : model1 loss : 0.115652 model2 loss : 0.096765
iteration 6761 : model1 loss : 0.124949 model2 loss : 0.092890
iteration 6762 : model1 loss : 0.131886 model2 loss : 0.092018
iteration 6763 : model1 loss : 0.145424 model2 loss : 0.098312
iteration 6764 : model1 loss : 0.124186 model2 loss : 0.099422
iteration 6765 : model1 loss : 0.129558 model2 loss : 0.108864
iteration 6766 : model1 loss : 0.120732 model2 loss : 0.081795
 68%|██████████████████▏        | 398/589 [2:42:08<1:13:27, 23.08s/it]iteration 6767 : model1 loss : 0.169082 model2 loss : 0.124061
iteration 6768 : model1 loss : 0.168936 model2 loss : 0.100010
iteration 6769 : model1 loss : 0.110773 model2 loss : 0.094858
iteration 6770 : model1 loss : 0.151950 model2 loss : 0.094687
iteration 6771 : model1 loss : 0.113721 model2 loss : 0.096173
iteration 6772 : model1 loss : 0.121207 model2 loss : 0.101862
iteration 6773 : model1 loss : 0.123323 model2 loss : 0.079624
iteration 6774 : model1 loss : 0.191182 model2 loss : 0.108688
iteration 6775 : model1 loss : 0.104208 model2 loss : 0.089587
iteration 6776 : model1 loss : 0.141910 model2 loss : 0.130241
iteration 6777 : model1 loss : 0.143780 model2 loss : 0.084963
iteration 6778 : model1 loss : 0.172657 model2 loss : 0.125592
iteration 6779 : model1 loss : 0.140942 model2 loss : 0.126920
iteration 6780 : model1 loss : 0.103163 model2 loss : 0.105229
iteration 6781 : model1 loss : 0.154786 model2 loss : 0.077880
iteration 6782 : model1 loss : 0.146217 model2 loss : 0.106792
iteration 6783 : model1 loss : 0.142565 model2 loss : 0.109929
 68%|██████████████████▎        | 399/589 [2:42:31<1:12:46, 22.98s/it]iteration 6784 : model1 loss : 0.094890 model2 loss : 0.082545
iteration 6785 : model1 loss : 0.157223 model2 loss : 0.097700
iteration 6786 : model1 loss : 0.187673 model2 loss : 0.086587
iteration 6787 : model1 loss : 0.148903 model2 loss : 0.099235
iteration 6788 : model1 loss : 0.132280 model2 loss : 0.089888
iteration 6789 : model1 loss : 0.154750 model2 loss : 0.129729
iteration 6790 : model1 loss : 0.139259 model2 loss : 0.111314
iteration 6791 : model1 loss : 0.156321 model2 loss : 0.110952
iteration 6792 : model1 loss : 0.140012 model2 loss : 0.097140
iteration 6793 : model1 loss : 0.125890 model2 loss : 0.157439
iteration 6794 : model1 loss : 0.146829 model2 loss : 0.119087
iteration 6795 : model1 loss : 0.128342 model2 loss : 0.108524
iteration 6796 : model1 loss : 0.159317 model2 loss : 0.162577
iteration 6797 : model1 loss : 0.135826 model2 loss : 0.105581
iteration 6798 : model1 loss : 0.118694 model2 loss : 0.122133
iteration 6799 : model1 loss : 0.141799 model2 loss : 0.124502
iteration 6800 : model1 loss : 0.122164 model2 loss : 0.114661
iteration 6800 : model1_mean_dice : 0.686290 model1_mean_hd95 : 93.624696 model1_mean_iou : 0.561009
iteration 6800 : model2_mean_dice : 0.753356 model2_mean_hd95 : 68.084010 model2_mean_iou : 0.644931
 68%|██████████████████▎        | 400/589 [2:43:14<1:31:03, 28.91s/it]iteration 6801 : model1 loss : 0.101290 model2 loss : 0.097320
iteration 6802 : model1 loss : 0.130526 model2 loss : 0.130023
iteration 6803 : model1 loss : 0.197735 model2 loss : 0.146521
iteration 6804 : model1 loss : 0.143978 model2 loss : 0.095408
iteration 6805 : model1 loss : 0.171328 model2 loss : 0.132343
iteration 6806 : model1 loss : 0.124422 model2 loss : 0.107665
iteration 6807 : model1 loss : 0.166613 model2 loss : 0.158223
iteration 6808 : model1 loss : 0.128279 model2 loss : 0.103330
iteration 6809 : model1 loss : 0.116923 model2 loss : 0.076207
iteration 6810 : model1 loss : 0.128912 model2 loss : 0.120014
iteration 6811 : model1 loss : 0.126284 model2 loss : 0.085726
iteration 6812 : model1 loss : 0.144792 model2 loss : 0.109093
iteration 6813 : model1 loss : 0.138652 model2 loss : 0.072260
iteration 6814 : model1 loss : 0.197334 model2 loss : 0.106076
iteration 6815 : model1 loss : 0.159525 model2 loss : 0.133405
iteration 6816 : model1 loss : 0.116679 model2 loss : 0.075778
iteration 6817 : model1 loss : 0.111723 model2 loss : 0.099014
 68%|██████████████████▍        | 401/589 [2:43:36<1:24:46, 27.06s/it]iteration 6818 : model1 loss : 0.145090 model2 loss : 0.108200
iteration 6819 : model1 loss : 0.160381 model2 loss : 0.099467
iteration 6820 : model1 loss : 0.143586 model2 loss : 0.077712
iteration 6821 : model1 loss : 0.131563 model2 loss : 0.099238
iteration 6822 : model1 loss : 0.134909 model2 loss : 0.121550
iteration 6823 : model1 loss : 0.132446 model2 loss : 0.093258
iteration 6824 : model1 loss : 0.151946 model2 loss : 0.120461
iteration 6825 : model1 loss : 0.116808 model2 loss : 0.101577
iteration 6826 : model1 loss : 0.131481 model2 loss : 0.078873
iteration 6827 : model1 loss : 0.122371 model2 loss : 0.116159
iteration 6828 : model1 loss : 0.131383 model2 loss : 0.101342
iteration 6829 : model1 loss : 0.113211 model2 loss : 0.107336
iteration 6830 : model1 loss : 0.097630 model2 loss : 0.078884
iteration 6831 : model1 loss : 0.150169 model2 loss : 0.111856
iteration 6832 : model1 loss : 0.137974 model2 loss : 0.090788
iteration 6833 : model1 loss : 0.125867 model2 loss : 0.117225
iteration 6834 : model1 loss : 0.147746 model2 loss : 0.101291
 68%|██████████████████▍        | 402/589 [2:43:59<1:20:16, 25.76s/it]iteration 6835 : model1 loss : 0.158423 model2 loss : 0.104297
iteration 6836 : model1 loss : 0.129914 model2 loss : 0.114802
iteration 6837 : model1 loss : 0.133598 model2 loss : 0.100354
iteration 6838 : model1 loss : 0.142331 model2 loss : 0.101759
iteration 6839 : model1 loss : 0.151179 model2 loss : 0.109711
iteration 6840 : model1 loss : 0.094795 model2 loss : 0.092743
iteration 6841 : model1 loss : 0.118812 model2 loss : 0.102366
iteration 6842 : model1 loss : 0.163552 model2 loss : 0.116824
iteration 6843 : model1 loss : 0.112943 model2 loss : 0.076790
iteration 6844 : model1 loss : 0.147739 model2 loss : 0.106824
iteration 6845 : model1 loss : 0.131438 model2 loss : 0.083460
iteration 6846 : model1 loss : 0.129337 model2 loss : 0.075674
iteration 6847 : model1 loss : 0.133631 model2 loss : 0.106079
iteration 6848 : model1 loss : 0.121691 model2 loss : 0.099903
iteration 6849 : model1 loss : 0.177795 model2 loss : 0.124243
iteration 6850 : model1 loss : 0.146102 model2 loss : 0.103675
iteration 6851 : model1 loss : 0.138071 model2 loss : 0.113179
 68%|██████████████████▍        | 403/589 [2:44:22<1:17:11, 24.90s/it]iteration 6852 : model1 loss : 0.113458 model2 loss : 0.091424
iteration 6853 : model1 loss : 0.164913 model2 loss : 0.111309
iteration 6854 : model1 loss : 0.159632 model2 loss : 0.129673
iteration 6855 : model1 loss : 0.114773 model2 loss : 0.087473
iteration 6856 : model1 loss : 0.127567 model2 loss : 0.105131
iteration 6857 : model1 loss : 0.181722 model2 loss : 0.100793
iteration 6858 : model1 loss : 0.132613 model2 loss : 0.109990
iteration 6859 : model1 loss : 0.175035 model2 loss : 0.104030
iteration 6860 : model1 loss : 0.139612 model2 loss : 0.136753
iteration 6861 : model1 loss : 0.130757 model2 loss : 0.083874
iteration 6862 : model1 loss : 0.107044 model2 loss : 0.061132
iteration 6863 : model1 loss : 0.154429 model2 loss : 0.144593
iteration 6864 : model1 loss : 0.128590 model2 loss : 0.143830
iteration 6865 : model1 loss : 0.158733 model2 loss : 0.092225
iteration 6866 : model1 loss : 0.130125 model2 loss : 0.094984
iteration 6867 : model1 loss : 0.117255 model2 loss : 0.100859
iteration 6868 : model1 loss : 0.113327 model2 loss : 0.102530
 69%|██████████████████▌        | 404/589 [2:44:45<1:14:46, 24.25s/it]iteration 6869 : model1 loss : 0.131743 model2 loss : 0.106088
iteration 6870 : model1 loss : 0.128298 model2 loss : 0.093097
iteration 6871 : model1 loss : 0.143235 model2 loss : 0.090458
iteration 6872 : model1 loss : 0.162595 model2 loss : 0.089204
iteration 6873 : model1 loss : 0.111316 model2 loss : 0.099076
iteration 6874 : model1 loss : 0.149507 model2 loss : 0.093288
iteration 6875 : model1 loss : 0.169395 model2 loss : 0.151940
iteration 6876 : model1 loss : 0.132829 model2 loss : 0.099455
iteration 6877 : model1 loss : 0.121744 model2 loss : 0.097421
iteration 6878 : model1 loss : 0.142447 model2 loss : 0.095985
iteration 6879 : model1 loss : 0.134221 model2 loss : 0.108602
iteration 6880 : model1 loss : 0.148079 model2 loss : 0.112729
iteration 6881 : model1 loss : 0.167917 model2 loss : 0.100251
iteration 6882 : model1 loss : 0.115918 model2 loss : 0.094897
iteration 6883 : model1 loss : 0.129370 model2 loss : 0.103755
iteration 6884 : model1 loss : 0.148854 model2 loss : 0.065966
iteration 6885 : model1 loss : 0.114920 model2 loss : 0.069862
 69%|██████████████████▌        | 405/589 [2:45:07<1:13:01, 23.81s/it]iteration 6886 : model1 loss : 0.104734 model2 loss : 0.095895
iteration 6887 : model1 loss : 0.157882 model2 loss : 0.111589
iteration 6888 : model1 loss : 0.165158 model2 loss : 0.110847
iteration 6889 : model1 loss : 0.095754 model2 loss : 0.065676
iteration 6890 : model1 loss : 0.138857 model2 loss : 0.122909
iteration 6891 : model1 loss : 0.207074 model2 loss : 0.127029
iteration 6892 : model1 loss : 0.131650 model2 loss : 0.084099
iteration 6893 : model1 loss : 0.130779 model2 loss : 0.092136
iteration 6894 : model1 loss : 0.101227 model2 loss : 0.083340
iteration 6895 : model1 loss : 0.132380 model2 loss : 0.079932
iteration 6896 : model1 loss : 0.114712 model2 loss : 0.087304
iteration 6897 : model1 loss : 0.148935 model2 loss : 0.108442
iteration 6898 : model1 loss : 0.168283 model2 loss : 0.097339
iteration 6899 : model1 loss : 0.160066 model2 loss : 0.118634
iteration 6900 : model1 loss : 0.130593 model2 loss : 0.139847
iteration 6901 : model1 loss : 0.139447 model2 loss : 0.112822
iteration 6902 : model1 loss : 0.170162 model2 loss : 0.114177
 69%|██████████████████▌        | 406/589 [2:45:30<1:11:45, 23.53s/it]iteration 6903 : model1 loss : 0.110014 model2 loss : 0.088226
iteration 6904 : model1 loss : 0.148240 model2 loss : 0.098602
iteration 6905 : model1 loss : 0.143043 model2 loss : 0.096634
iteration 6906 : model1 loss : 0.145675 model2 loss : 0.081158
iteration 6907 : model1 loss : 0.075016 model2 loss : 0.083052
iteration 6908 : model1 loss : 0.134427 model2 loss : 0.115293
iteration 6909 : model1 loss : 0.146420 model2 loss : 0.089972
iteration 6910 : model1 loss : 0.115025 model2 loss : 0.071073
iteration 6911 : model1 loss : 0.125601 model2 loss : 0.100595
iteration 6912 : model1 loss : 0.101261 model2 loss : 0.086476
iteration 6913 : model1 loss : 0.177923 model2 loss : 0.118058
iteration 6914 : model1 loss : 0.177598 model2 loss : 0.152951
iteration 6915 : model1 loss : 0.162414 model2 loss : 0.146656
iteration 6916 : model1 loss : 0.173122 model2 loss : 0.102163
iteration 6917 : model1 loss : 0.137705 model2 loss : 0.093386
iteration 6918 : model1 loss : 0.145930 model2 loss : 0.105307
iteration 6919 : model1 loss : 0.128847 model2 loss : 0.091331
 69%|██████████████████▋        | 407/589 [2:45:53<1:10:38, 23.29s/it]iteration 6920 : model1 loss : 0.199869 model2 loss : 0.113850
iteration 6921 : model1 loss : 0.128771 model2 loss : 0.092627
iteration 6922 : model1 loss : 0.150751 model2 loss : 0.114672
iteration 6923 : model1 loss : 0.133262 model2 loss : 0.102128
iteration 6924 : model1 loss : 0.120458 model2 loss : 0.077078
iteration 6925 : model1 loss : 0.113904 model2 loss : 0.117386
iteration 6926 : model1 loss : 0.131093 model2 loss : 0.100890
iteration 6927 : model1 loss : 0.110034 model2 loss : 0.085250
iteration 6928 : model1 loss : 0.133998 model2 loss : 0.091104
iteration 6929 : model1 loss : 0.162540 model2 loss : 0.095563
iteration 6930 : model1 loss : 0.137479 model2 loss : 0.122181
iteration 6931 : model1 loss : 0.135803 model2 loss : 0.083540
iteration 6932 : model1 loss : 0.104811 model2 loss : 0.100242
iteration 6933 : model1 loss : 0.132409 model2 loss : 0.096442
iteration 6934 : model1 loss : 0.125053 model2 loss : 0.083028
iteration 6935 : model1 loss : 0.146303 model2 loss : 0.115776
iteration 6936 : model1 loss : 0.151295 model2 loss : 0.128686
 69%|██████████████████▋        | 408/589 [2:46:16<1:09:49, 23.15s/it]iteration 6937 : model1 loss : 0.162163 model2 loss : 0.114790
iteration 6938 : model1 loss : 0.137700 model2 loss : 0.129612
iteration 6939 : model1 loss : 0.154773 model2 loss : 0.114542
iteration 6940 : model1 loss : 0.119468 model2 loss : 0.076920
iteration 6941 : model1 loss : 0.125463 model2 loss : 0.100698
iteration 6942 : model1 loss : 0.149645 model2 loss : 0.123376
iteration 6943 : model1 loss : 0.170179 model2 loss : 0.132710
iteration 6944 : model1 loss : 0.148340 model2 loss : 0.128616
iteration 6945 : model1 loss : 0.141671 model2 loss : 0.126618
iteration 6946 : model1 loss : 0.164508 model2 loss : 0.116696
iteration 6947 : model1 loss : 0.140649 model2 loss : 0.141149
iteration 6948 : model1 loss : 0.120750 model2 loss : 0.102354
iteration 6949 : model1 loss : 0.159130 model2 loss : 0.131627
iteration 6950 : model1 loss : 0.114917 model2 loss : 0.107608
iteration 6951 : model1 loss : 0.123323 model2 loss : 0.074216
iteration 6952 : model1 loss : 0.092868 model2 loss : 0.082442
iteration 6953 : model1 loss : 0.124459 model2 loss : 0.088400
 69%|██████████████████▋        | 409/589 [2:46:39<1:09:13, 23.07s/it]iteration 6954 : model1 loss : 0.138243 model2 loss : 0.111898
iteration 6955 : model1 loss : 0.121261 model2 loss : 0.081073
iteration 6956 : model1 loss : 0.143749 model2 loss : 0.098693
iteration 6957 : model1 loss : 0.123471 model2 loss : 0.098310
iteration 6958 : model1 loss : 0.117384 model2 loss : 0.088210
iteration 6959 : model1 loss : 0.152236 model2 loss : 0.111580
iteration 6960 : model1 loss : 0.154533 model2 loss : 0.127633
iteration 6961 : model1 loss : 0.120307 model2 loss : 0.097995
iteration 6962 : model1 loss : 0.134293 model2 loss : 0.111169
iteration 6963 : model1 loss : 0.164961 model2 loss : 0.103651
iteration 6964 : model1 loss : 0.144928 model2 loss : 0.115885
iteration 6965 : model1 loss : 0.104265 model2 loss : 0.077096
iteration 6966 : model1 loss : 0.077344 model2 loss : 0.064464
iteration 6967 : model1 loss : 0.149032 model2 loss : 0.102944
iteration 6968 : model1 loss : 0.139350 model2 loss : 0.101917
iteration 6969 : model1 loss : 0.124988 model2 loss : 0.107049
iteration 6970 : model1 loss : 0.160940 model2 loss : 0.112905
 70%|██████████████████▊        | 410/589 [2:47:02<1:08:35, 22.99s/it]iteration 6971 : model1 loss : 0.130640 model2 loss : 0.075607
iteration 6972 : model1 loss : 0.186968 model2 loss : 0.137271
iteration 6973 : model1 loss : 0.151463 model2 loss : 0.134010
iteration 6974 : model1 loss : 0.158042 model2 loss : 0.115135
iteration 6975 : model1 loss : 0.145437 model2 loss : 0.105310
iteration 6976 : model1 loss : 0.115791 model2 loss : 0.096604
iteration 6977 : model1 loss : 0.177210 model2 loss : 0.144070
iteration 6978 : model1 loss : 0.125878 model2 loss : 0.108822
iteration 6979 : model1 loss : 0.165050 model2 loss : 0.106552
iteration 6980 : model1 loss : 0.093559 model2 loss : 0.102051
iteration 6981 : model1 loss : 0.123094 model2 loss : 0.099498
iteration 6982 : model1 loss : 0.119355 model2 loss : 0.083682
iteration 6983 : model1 loss : 0.155053 model2 loss : 0.108447
iteration 6984 : model1 loss : 0.106489 model2 loss : 0.083281
iteration 6985 : model1 loss : 0.136359 model2 loss : 0.106893
iteration 6986 : model1 loss : 0.146437 model2 loss : 0.113688
iteration 6987 : model1 loss : 0.099719 model2 loss : 0.082877
 70%|██████████████████▊        | 411/589 [2:47:24<1:08:01, 22.93s/it]iteration 6988 : model1 loss : 0.158448 model2 loss : 0.125039
iteration 6989 : model1 loss : 0.148930 model2 loss : 0.170721
iteration 6990 : model1 loss : 0.137115 model2 loss : 0.124938
iteration 6991 : model1 loss : 0.123292 model2 loss : 0.127399
iteration 6992 : model1 loss : 0.137865 model2 loss : 0.104416
iteration 6993 : model1 loss : 0.132915 model2 loss : 0.103576
iteration 6994 : model1 loss : 0.145646 model2 loss : 0.114941
iteration 6995 : model1 loss : 0.156994 model2 loss : 0.115625
iteration 6996 : model1 loss : 0.105090 model2 loss : 0.083597
iteration 6997 : model1 loss : 0.114028 model2 loss : 0.111177
iteration 6998 : model1 loss : 0.155886 model2 loss : 0.089099
iteration 6999 : model1 loss : 0.090788 model2 loss : 0.090086
iteration 7000 : model1 loss : 0.151497 model2 loss : 0.107732
iteration 7000 : model1_mean_dice : 0.664005 model1_mean_hd95 : 88.468707 model1_mean_iou : 0.531753
iteration 7000 : model2_mean_dice : 0.759873 model2_mean_hd95 : 67.947725 model2_mean_iou : 0.648827
iteration 7001 : model1 loss : 0.147510 model2 loss : 0.116468
iteration 7002 : model1 loss : 0.122773 model2 loss : 0.091408
iteration 7003 : model1 loss : 0.106739 model2 loss : 0.080607
iteration 7004 : model1 loss : 0.117753 model2 loss : 0.080719
 70%|██████████████████▉        | 412/589 [2:48:07<1:25:07, 28.86s/it]iteration 7005 : model1 loss : 0.151537 model2 loss : 0.121110
iteration 7006 : model1 loss : 0.125983 model2 loss : 0.078858
iteration 7007 : model1 loss : 0.165714 model2 loss : 0.104173
iteration 7008 : model1 loss : 0.157284 model2 loss : 0.116878
iteration 7009 : model1 loss : 0.123480 model2 loss : 0.109182
iteration 7010 : model1 loss : 0.139937 model2 loss : 0.092636
iteration 7011 : model1 loss : 0.145700 model2 loss : 0.089852
iteration 7012 : model1 loss : 0.174619 model2 loss : 0.098780
iteration 7013 : model1 loss : 0.104999 model2 loss : 0.084177
iteration 7014 : model1 loss : 0.109384 model2 loss : 0.114355
iteration 7015 : model1 loss : 0.128138 model2 loss : 0.108074
iteration 7016 : model1 loss : 0.132828 model2 loss : 0.096972
iteration 7017 : model1 loss : 0.147854 model2 loss : 0.141186
iteration 7018 : model1 loss : 0.151766 model2 loss : 0.073492
iteration 7019 : model1 loss : 0.134618 model2 loss : 0.106661
iteration 7020 : model1 loss : 0.116916 model2 loss : 0.090265
iteration 7021 : model1 loss : 0.162015 model2 loss : 0.117511
 70%|██████████████████▉        | 413/589 [2:48:30<1:19:12, 27.00s/it]iteration 7022 : model1 loss : 0.096979 model2 loss : 0.098163
iteration 7023 : model1 loss : 0.143963 model2 loss : 0.105433
iteration 7024 : model1 loss : 0.224540 model2 loss : 0.158669
iteration 7025 : model1 loss : 0.122203 model2 loss : 0.116615
iteration 7026 : model1 loss : 0.112766 model2 loss : 0.094262
iteration 7027 : model1 loss : 0.185247 model2 loss : 0.106801
iteration 7028 : model1 loss : 0.114761 model2 loss : 0.096586
iteration 7029 : model1 loss : 0.121113 model2 loss : 0.119818
iteration 7030 : model1 loss : 0.103395 model2 loss : 0.078229
iteration 7031 : model1 loss : 0.129257 model2 loss : 0.083119
iteration 7032 : model1 loss : 0.107236 model2 loss : 0.077156
iteration 7033 : model1 loss : 0.138839 model2 loss : 0.107280
iteration 7034 : model1 loss : 0.128940 model2 loss : 0.116225
iteration 7035 : model1 loss : 0.189795 model2 loss : 0.127884
iteration 7036 : model1 loss : 0.113341 model2 loss : 0.085070
iteration 7037 : model1 loss : 0.173259 model2 loss : 0.097354
iteration 7038 : model1 loss : 0.142650 model2 loss : 0.103635
 70%|██████████████████▉        | 414/589 [2:48:52<1:14:58, 25.71s/it]iteration 7039 : model1 loss : 0.147727 model2 loss : 0.096867
iteration 7040 : model1 loss : 0.129167 model2 loss : 0.121565
iteration 7041 : model1 loss : 0.139951 model2 loss : 0.089217
iteration 7042 : model1 loss : 0.112336 model2 loss : 0.077434
iteration 7043 : model1 loss : 0.148594 model2 loss : 0.114077
iteration 7044 : model1 loss : 0.139568 model2 loss : 0.102002
iteration 7045 : model1 loss : 0.122231 model2 loss : 0.103505
iteration 7046 : model1 loss : 0.131949 model2 loss : 0.089671
iteration 7047 : model1 loss : 0.126727 model2 loss : 0.158900
iteration 7048 : model1 loss : 0.140447 model2 loss : 0.084439
iteration 7049 : model1 loss : 0.143554 model2 loss : 0.138512
iteration 7050 : model1 loss : 0.130370 model2 loss : 0.091032
iteration 7051 : model1 loss : 0.120272 model2 loss : 0.080558
iteration 7052 : model1 loss : 0.124763 model2 loss : 0.107571
iteration 7053 : model1 loss : 0.155062 model2 loss : 0.125813
iteration 7054 : model1 loss : 0.146383 model2 loss : 0.107123
iteration 7055 : model1 loss : 0.103605 model2 loss : 0.093610
 70%|███████████████████        | 415/589 [2:49:15<1:12:05, 24.86s/it]iteration 7056 : model1 loss : 0.141414 model2 loss : 0.111105
iteration 7057 : model1 loss : 0.147891 model2 loss : 0.107048
iteration 7058 : model1 loss : 0.110462 model2 loss : 0.111806
iteration 7059 : model1 loss : 0.159550 model2 loss : 0.104229
iteration 7060 : model1 loss : 0.151741 model2 loss : 0.113420
iteration 7061 : model1 loss : 0.162152 model2 loss : 0.093250
iteration 7062 : model1 loss : 0.136639 model2 loss : 0.094686
iteration 7063 : model1 loss : 0.112565 model2 loss : 0.088448
iteration 7064 : model1 loss : 0.136835 model2 loss : 0.118446
iteration 7065 : model1 loss : 0.192670 model2 loss : 0.118137
iteration 7066 : model1 loss : 0.123048 model2 loss : 0.107190
iteration 7067 : model1 loss : 0.110169 model2 loss : 0.103971
iteration 7068 : model1 loss : 0.163293 model2 loss : 0.104908
iteration 7069 : model1 loss : 0.124897 model2 loss : 0.101758
iteration 7070 : model1 loss : 0.119282 model2 loss : 0.090266
iteration 7071 : model1 loss : 0.096014 model2 loss : 0.094964
iteration 7072 : model1 loss : 0.104344 model2 loss : 0.062780
 71%|███████████████████        | 416/589 [2:49:38<1:09:48, 24.21s/it]iteration 7073 : model1 loss : 0.103067 model2 loss : 0.081456
iteration 7074 : model1 loss : 0.136732 model2 loss : 0.095311
iteration 7075 : model1 loss : 0.136048 model2 loss : 0.109574
iteration 7076 : model1 loss : 0.168184 model2 loss : 0.121897
iteration 7077 : model1 loss : 0.120053 model2 loss : 0.124591
iteration 7078 : model1 loss : 0.099196 model2 loss : 0.091626
iteration 7079 : model1 loss : 0.224769 model2 loss : 0.138950
iteration 7080 : model1 loss : 0.111071 model2 loss : 0.082969
iteration 7081 : model1 loss : 0.153354 model2 loss : 0.118671
iteration 7082 : model1 loss : 0.141728 model2 loss : 0.088751
iteration 7083 : model1 loss : 0.144162 model2 loss : 0.117706
iteration 7084 : model1 loss : 0.122390 model2 loss : 0.124722
iteration 7085 : model1 loss : 0.154374 model2 loss : 0.116492
iteration 7086 : model1 loss : 0.145906 model2 loss : 0.114388
iteration 7087 : model1 loss : 0.127604 model2 loss : 0.123404
iteration 7088 : model1 loss : 0.126751 model2 loss : 0.081152
iteration 7089 : model1 loss : 0.136117 model2 loss : 0.110815
 71%|███████████████████        | 417/589 [2:50:01<1:08:07, 23.77s/it]iteration 7090 : model1 loss : 0.147792 model2 loss : 0.092394
iteration 7091 : model1 loss : 0.089415 model2 loss : 0.094353
iteration 7092 : model1 loss : 0.137097 model2 loss : 0.119426
iteration 7093 : model1 loss : 0.101540 model2 loss : 0.082411
iteration 7094 : model1 loss : 0.140829 model2 loss : 0.091094
iteration 7095 : model1 loss : 0.148222 model2 loss : 0.143676
iteration 7096 : model1 loss : 0.108034 model2 loss : 0.105192
iteration 7097 : model1 loss : 0.122425 model2 loss : 0.126376
iteration 7098 : model1 loss : 0.109764 model2 loss : 0.081261
iteration 7099 : model1 loss : 0.157755 model2 loss : 0.112426
iteration 7100 : model1 loss : 0.138037 model2 loss : 0.120506
iteration 7101 : model1 loss : 0.151441 model2 loss : 0.130309
iteration 7102 : model1 loss : 0.164641 model2 loss : 0.113896
iteration 7103 : model1 loss : 0.157606 model2 loss : 0.112721
iteration 7104 : model1 loss : 0.122110 model2 loss : 0.112629
iteration 7105 : model1 loss : 0.101101 model2 loss : 0.083449
iteration 7106 : model1 loss : 0.123379 model2 loss : 0.090245
 71%|███████████████████▏       | 418/589 [2:50:24<1:06:56, 23.49s/it]iteration 7107 : model1 loss : 0.214327 model2 loss : 0.168034
iteration 7108 : model1 loss : 0.146614 model2 loss : 0.100060
iteration 7109 : model1 loss : 0.157527 model2 loss : 0.103200
iteration 7110 : model1 loss : 0.099829 model2 loss : 0.078439
iteration 7111 : model1 loss : 0.104399 model2 loss : 0.092275
iteration 7112 : model1 loss : 0.159334 model2 loss : 0.099279
iteration 7113 : model1 loss : 0.172322 model2 loss : 0.105117
iteration 7114 : model1 loss : 0.117252 model2 loss : 0.102575
iteration 7115 : model1 loss : 0.117184 model2 loss : 0.110282
iteration 7116 : model1 loss : 0.124051 model2 loss : 0.088088
iteration 7117 : model1 loss : 0.147383 model2 loss : 0.113593
iteration 7118 : model1 loss : 0.102389 model2 loss : 0.119786
iteration 7119 : model1 loss : 0.120801 model2 loss : 0.099436
iteration 7120 : model1 loss : 0.179408 model2 loss : 0.081504
iteration 7121 : model1 loss : 0.108851 model2 loss : 0.086297
iteration 7122 : model1 loss : 0.126567 model2 loss : 0.104441
iteration 7123 : model1 loss : 0.137991 model2 loss : 0.088795
 71%|███████████████████▏       | 419/589 [2:50:46<1:05:54, 23.26s/it]iteration 7124 : model1 loss : 0.157447 model2 loss : 0.102997
iteration 7125 : model1 loss : 0.099788 model2 loss : 0.085501
iteration 7126 : model1 loss : 0.153536 model2 loss : 0.111479
iteration 7127 : model1 loss : 0.123179 model2 loss : 0.086875
iteration 7128 : model1 loss : 0.156573 model2 loss : 0.094048
iteration 7129 : model1 loss : 0.169243 model2 loss : 0.111994
iteration 7130 : model1 loss : 0.125270 model2 loss : 0.111877
iteration 7131 : model1 loss : 0.139516 model2 loss : 0.075807
iteration 7132 : model1 loss : 0.095253 model2 loss : 0.102454
iteration 7133 : model1 loss : 0.131109 model2 loss : 0.110405
iteration 7134 : model1 loss : 0.145565 model2 loss : 0.112118
iteration 7135 : model1 loss : 0.165223 model2 loss : 0.091536
iteration 7136 : model1 loss : 0.129356 model2 loss : 0.114588
iteration 7137 : model1 loss : 0.120431 model2 loss : 0.098615
iteration 7138 : model1 loss : 0.113571 model2 loss : 0.109033
iteration 7139 : model1 loss : 0.117758 model2 loss : 0.122494
iteration 7140 : model1 loss : 0.151165 model2 loss : 0.094226
 71%|███████████████████▎       | 420/589 [2:51:09<1:05:09, 23.13s/it]iteration 7141 : model1 loss : 0.139230 model2 loss : 0.092490
iteration 7142 : model1 loss : 0.140084 model2 loss : 0.064834
iteration 7143 : model1 loss : 0.111712 model2 loss : 0.104284
iteration 7144 : model1 loss : 0.127858 model2 loss : 0.099463
iteration 7145 : model1 loss : 0.140242 model2 loss : 0.113426
iteration 7146 : model1 loss : 0.136579 model2 loss : 0.091269
iteration 7147 : model1 loss : 0.142496 model2 loss : 0.121927
iteration 7148 : model1 loss : 0.126197 model2 loss : 0.101166
iteration 7149 : model1 loss : 0.129121 model2 loss : 0.089772
iteration 7150 : model1 loss : 0.164526 model2 loss : 0.161734
iteration 7151 : model1 loss : 0.186129 model2 loss : 0.151790
iteration 7152 : model1 loss : 0.104430 model2 loss : 0.093794
iteration 7153 : model1 loss : 0.130699 model2 loss : 0.115658
iteration 7154 : model1 loss : 0.160344 model2 loss : 0.130814
iteration 7155 : model1 loss : 0.145185 model2 loss : 0.103424
iteration 7156 : model1 loss : 0.119056 model2 loss : 0.110863
iteration 7157 : model1 loss : 0.136677 model2 loss : 0.097550
 71%|███████████████████▎       | 421/589 [2:51:32<1:04:38, 23.09s/it]iteration 7158 : model1 loss : 0.126650 model2 loss : 0.112271
iteration 7159 : model1 loss : 0.138266 model2 loss : 0.094821
iteration 7160 : model1 loss : 0.148487 model2 loss : 0.122850
iteration 7161 : model1 loss : 0.148174 model2 loss : 0.122523
iteration 7162 : model1 loss : 0.146111 model2 loss : 0.075217
iteration 7163 : model1 loss : 0.119631 model2 loss : 0.094136
iteration 7164 : model1 loss : 0.130851 model2 loss : 0.095425
iteration 7165 : model1 loss : 0.135059 model2 loss : 0.110164
iteration 7166 : model1 loss : 0.096370 model2 loss : 0.063131
iteration 7167 : model1 loss : 0.142716 model2 loss : 0.121567
iteration 7168 : model1 loss : 0.117828 model2 loss : 0.093667
iteration 7169 : model1 loss : 0.115447 model2 loss : 0.105552
iteration 7170 : model1 loss : 0.156479 model2 loss : 0.109361
iteration 7171 : model1 loss : 0.100178 model2 loss : 0.097639
iteration 7172 : model1 loss : 0.130768 model2 loss : 0.086461
iteration 7173 : model1 loss : 0.187760 model2 loss : 0.105040
iteration 7174 : model1 loss : 0.162898 model2 loss : 0.093458
 72%|███████████████████▎       | 422/589 [2:51:55<1:04:02, 23.01s/it]iteration 7175 : model1 loss : 0.150306 model2 loss : 0.109049
iteration 7176 : model1 loss : 0.125355 model2 loss : 0.091435
iteration 7177 : model1 loss : 0.146334 model2 loss : 0.137181
iteration 7178 : model1 loss : 0.140158 model2 loss : 0.085518
iteration 7179 : model1 loss : 0.149605 model2 loss : 0.105167
iteration 7180 : model1 loss : 0.138208 model2 loss : 0.134230
iteration 7181 : model1 loss : 0.168249 model2 loss : 0.208144
iteration 7182 : model1 loss : 0.118001 model2 loss : 0.081456
iteration 7183 : model1 loss : 0.161070 model2 loss : 0.092493
iteration 7184 : model1 loss : 0.095062 model2 loss : 0.069929
iteration 7185 : model1 loss : 0.111038 model2 loss : 0.084579
iteration 7186 : model1 loss : 0.132901 model2 loss : 0.127571
iteration 7187 : model1 loss : 0.206527 model2 loss : 0.102131
iteration 7188 : model1 loss : 0.161343 model2 loss : 0.092071
iteration 7189 : model1 loss : 0.116950 model2 loss : 0.084901
iteration 7190 : model1 loss : 0.145460 model2 loss : 0.106193
iteration 7191 : model1 loss : 0.138591 model2 loss : 0.083403
 72%|███████████████████▍       | 423/589 [2:52:18<1:03:26, 22.93s/it]iteration 7192 : model1 loss : 0.163321 model2 loss : 0.110231
iteration 7193 : model1 loss : 0.169638 model2 loss : 0.134209
iteration 7194 : model1 loss : 0.146767 model2 loss : 0.104082
iteration 7195 : model1 loss : 0.141826 model2 loss : 0.102911
iteration 7196 : model1 loss : 0.110726 model2 loss : 0.087984
iteration 7197 : model1 loss : 0.111643 model2 loss : 0.088816
iteration 7198 : model1 loss : 0.131054 model2 loss : 0.121519
iteration 7199 : model1 loss : 0.148014 model2 loss : 0.115326
iteration 7200 : model1 loss : 0.135407 model2 loss : 0.115371
iteration 7200 : model1_mean_dice : 0.690124 model1_mean_hd95 : 89.542725 model1_mean_iou : 0.559968
iteration 7200 : model2_mean_dice : 0.767440 model2_mean_hd95 : 66.091570 model2_mean_iou : 0.655231
iteration 7201 : model1 loss : 0.138840 model2 loss : 0.096210
iteration 7202 : model1 loss : 0.157123 model2 loss : 0.120148
iteration 7203 : model1 loss : 0.126078 model2 loss : 0.099301
iteration 7204 : model1 loss : 0.112199 model2 loss : 0.100749
iteration 7205 : model1 loss : 0.138785 model2 loss : 0.097874
iteration 7206 : model1 loss : 0.135288 model2 loss : 0.076184
iteration 7207 : model1 loss : 0.100081 model2 loss : 0.103086
iteration 7208 : model1 loss : 0.144505 model2 loss : 0.124725
 72%|███████████████████▍       | 424/589 [2:53:01<1:19:30, 28.91s/it]iteration 7209 : model1 loss : 0.178399 model2 loss : 0.094489
iteration 7210 : model1 loss : 0.139664 model2 loss : 0.105780
iteration 7211 : model1 loss : 0.150525 model2 loss : 0.123235
iteration 7212 : model1 loss : 0.110898 model2 loss : 0.081473
iteration 7213 : model1 loss : 0.130329 model2 loss : 0.113163
iteration 7214 : model1 loss : 0.125168 model2 loss : 0.105171
iteration 7215 : model1 loss : 0.106835 model2 loss : 0.080738
iteration 7216 : model1 loss : 0.103158 model2 loss : 0.085741
iteration 7217 : model1 loss : 0.137533 model2 loss : 0.113607
iteration 7218 : model1 loss : 0.097509 model2 loss : 0.085222
iteration 7219 : model1 loss : 0.106329 model2 loss : 0.072614
iteration 7220 : model1 loss : 0.124075 model2 loss : 0.096483
iteration 7221 : model1 loss : 0.144309 model2 loss : 0.103693
iteration 7222 : model1 loss : 0.148455 model2 loss : 0.112867
iteration 7223 : model1 loss : 0.142990 model2 loss : 0.105705
iteration 7224 : model1 loss : 0.124902 model2 loss : 0.124024
iteration 7225 : model1 loss : 0.147347 model2 loss : 0.098387
 72%|███████████████████▍       | 425/589 [2:53:23<1:13:55, 27.05s/it]iteration 7226 : model1 loss : 0.123793 model2 loss : 0.103448
iteration 7227 : model1 loss : 0.172959 model2 loss : 0.102950
iteration 7228 : model1 loss : 0.128453 model2 loss : 0.092431
iteration 7229 : model1 loss : 0.125379 model2 loss : 0.117912
iteration 7230 : model1 loss : 0.154502 model2 loss : 0.108621
iteration 7231 : model1 loss : 0.118502 model2 loss : 0.105330
iteration 7232 : model1 loss : 0.095650 model2 loss : 0.078676
iteration 7233 : model1 loss : 0.136636 model2 loss : 0.094907
iteration 7234 : model1 loss : 0.111026 model2 loss : 0.099413
iteration 7235 : model1 loss : 0.179018 model2 loss : 0.123403
iteration 7236 : model1 loss : 0.133444 model2 loss : 0.101077
iteration 7237 : model1 loss : 0.119629 model2 loss : 0.111871
iteration 7238 : model1 loss : 0.171032 model2 loss : 0.100539
iteration 7239 : model1 loss : 0.135847 model2 loss : 0.081286
iteration 7240 : model1 loss : 0.108670 model2 loss : 0.070153
iteration 7241 : model1 loss : 0.088871 model2 loss : 0.111592
iteration 7242 : model1 loss : 0.123695 model2 loss : 0.098771
 72%|███████████████████▌       | 426/589 [2:53:46<1:09:57, 25.75s/it]iteration 7243 : model1 loss : 0.148926 model2 loss : 0.100817
iteration 7244 : model1 loss : 0.145351 model2 loss : 0.139738
iteration 7245 : model1 loss : 0.109285 model2 loss : 0.078361
iteration 7246 : model1 loss : 0.127469 model2 loss : 0.101735
iteration 7247 : model1 loss : 0.096063 model2 loss : 0.080640
iteration 7248 : model1 loss : 0.180523 model2 loss : 0.085963
iteration 7249 : model1 loss : 0.156426 model2 loss : 0.098393
iteration 7250 : model1 loss : 0.120065 model2 loss : 0.084069
iteration 7251 : model1 loss : 0.183401 model2 loss : 0.126969
iteration 7252 : model1 loss : 0.146717 model2 loss : 0.154198
iteration 7253 : model1 loss : 0.113327 model2 loss : 0.109370
iteration 7254 : model1 loss : 0.151318 model2 loss : 0.113293
iteration 7255 : model1 loss : 0.147124 model2 loss : 0.069742
iteration 7256 : model1 loss : 0.136354 model2 loss : 0.090607
iteration 7257 : model1 loss : 0.140696 model2 loss : 0.142798
iteration 7258 : model1 loss : 0.108964 model2 loss : 0.092869
iteration 7259 : model1 loss : 0.168125 model2 loss : 0.114093
 72%|███████████████████▌       | 427/589 [2:54:09<1:07:10, 24.88s/it]iteration 7260 : model1 loss : 0.149300 model2 loss : 0.130651
iteration 7261 : model1 loss : 0.099557 model2 loss : 0.095039
iteration 7262 : model1 loss : 0.176476 model2 loss : 0.085764
iteration 7263 : model1 loss : 0.131791 model2 loss : 0.107978
iteration 7264 : model1 loss : 0.118327 model2 loss : 0.168747
iteration 7265 : model1 loss : 0.173373 model2 loss : 0.130261
iteration 7266 : model1 loss : 0.150613 model2 loss : 0.114372
iteration 7267 : model1 loss : 0.154315 model2 loss : 0.121464
iteration 7268 : model1 loss : 0.115142 model2 loss : 0.075998
iteration 7269 : model1 loss : 0.158201 model2 loss : 0.120747
iteration 7270 : model1 loss : 0.137879 model2 loss : 0.111050
iteration 7271 : model1 loss : 0.166172 model2 loss : 0.141777
iteration 7272 : model1 loss : 0.104390 model2 loss : 0.075753
iteration 7273 : model1 loss : 0.105076 model2 loss : 0.093842
iteration 7274 : model1 loss : 0.135838 model2 loss : 0.096119
iteration 7275 : model1 loss : 0.124738 model2 loss : 0.098478
iteration 7276 : model1 loss : 0.137717 model2 loss : 0.092866
 73%|███████████████████▌       | 428/589 [2:54:32<1:05:02, 24.24s/it]iteration 7277 : model1 loss : 0.143656 model2 loss : 0.099825
iteration 7278 : model1 loss : 0.132409 model2 loss : 0.097625
iteration 7279 : model1 loss : 0.125619 model2 loss : 0.091445
iteration 7280 : model1 loss : 0.127285 model2 loss : 0.081749
iteration 7281 : model1 loss : 0.135127 model2 loss : 0.085794
iteration 7282 : model1 loss : 0.114249 model2 loss : 0.096854
iteration 7283 : model1 loss : 0.127365 model2 loss : 0.088148
iteration 7284 : model1 loss : 0.166952 model2 loss : 0.076691
iteration 7285 : model1 loss : 0.125171 model2 loss : 0.117365
iteration 7286 : model1 loss : 0.146968 model2 loss : 0.102565
iteration 7287 : model1 loss : 0.172137 model2 loss : 0.126874
iteration 7288 : model1 loss : 0.115252 model2 loss : 0.088107
iteration 7289 : model1 loss : 0.133243 model2 loss : 0.115212
iteration 7290 : model1 loss : 0.205026 model2 loss : 0.113327
iteration 7291 : model1 loss : 0.115387 model2 loss : 0.109296
iteration 7292 : model1 loss : 0.126133 model2 loss : 0.105598
iteration 7293 : model1 loss : 0.114068 model2 loss : 0.110539
 73%|███████████████████▋       | 429/589 [2:54:54<1:03:26, 23.79s/it]iteration 7294 : model1 loss : 0.134413 model2 loss : 0.089669
iteration 7295 : model1 loss : 0.117040 model2 loss : 0.084611
iteration 7296 : model1 loss : 0.107683 model2 loss : 0.086368
iteration 7297 : model1 loss : 0.155288 model2 loss : 0.097725
iteration 7298 : model1 loss : 0.125209 model2 loss : 0.117550
iteration 7299 : model1 loss : 0.172306 model2 loss : 0.108031
iteration 7300 : model1 loss : 0.122319 model2 loss : 0.103303
iteration 7301 : model1 loss : 0.176609 model2 loss : 0.155532
iteration 7302 : model1 loss : 0.179142 model2 loss : 0.104732
iteration 7303 : model1 loss : 0.121084 model2 loss : 0.089890
iteration 7304 : model1 loss : 0.125334 model2 loss : 0.171479
iteration 7305 : model1 loss : 0.094345 model2 loss : 0.095817
iteration 7306 : model1 loss : 0.143697 model2 loss : 0.114348
iteration 7307 : model1 loss : 0.152822 model2 loss : 0.105768
iteration 7308 : model1 loss : 0.138328 model2 loss : 0.083909
iteration 7309 : model1 loss : 0.163054 model2 loss : 0.104459
iteration 7310 : model1 loss : 0.151061 model2 loss : 0.101606
 73%|███████████████████▋       | 430/589 [2:55:17<1:02:17, 23.51s/it]iteration 7311 : model1 loss : 0.111131 model2 loss : 0.103669
iteration 7312 : model1 loss : 0.124571 model2 loss : 0.075552
iteration 7313 : model1 loss : 0.128354 model2 loss : 0.114006
iteration 7314 : model1 loss : 0.186695 model2 loss : 0.171792
iteration 7315 : model1 loss : 0.134519 model2 loss : 0.109347
iteration 7316 : model1 loss : 0.102039 model2 loss : 0.103336
iteration 7317 : model1 loss : 0.118804 model2 loss : 0.072923
iteration 7318 : model1 loss : 0.124233 model2 loss : 0.096399
iteration 7319 : model1 loss : 0.116219 model2 loss : 0.118128
iteration 7320 : model1 loss : 0.129927 model2 loss : 0.093385
iteration 7321 : model1 loss : 0.148007 model2 loss : 0.099754
iteration 7322 : model1 loss : 0.180805 model2 loss : 0.128210
iteration 7323 : model1 loss : 0.148521 model2 loss : 0.093330
iteration 7324 : model1 loss : 0.132528 model2 loss : 0.086024
iteration 7325 : model1 loss : 0.101862 model2 loss : 0.069900
iteration 7326 : model1 loss : 0.126485 model2 loss : 0.104315
iteration 7327 : model1 loss : 0.140449 model2 loss : 0.121820
 73%|███████████████████▊       | 431/589 [2:55:40<1:01:18, 23.28s/it]iteration 7328 : model1 loss : 0.119930 model2 loss : 0.103546
iteration 7329 : model1 loss : 0.143628 model2 loss : 0.088046
iteration 7330 : model1 loss : 0.113031 model2 loss : 0.079988
iteration 7331 : model1 loss : 0.196872 model2 loss : 0.121976
iteration 7332 : model1 loss : 0.104491 model2 loss : 0.091938
iteration 7333 : model1 loss : 0.114059 model2 loss : 0.086349
iteration 7334 : model1 loss : 0.125962 model2 loss : 0.080598
iteration 7335 : model1 loss : 0.113878 model2 loss : 0.102573
iteration 7336 : model1 loss : 0.134737 model2 loss : 0.089461
iteration 7337 : model1 loss : 0.097120 model2 loss : 0.096395
iteration 7338 : model1 loss : 0.123154 model2 loss : 0.102207
iteration 7339 : model1 loss : 0.155228 model2 loss : 0.122799
iteration 7340 : model1 loss : 0.139592 model2 loss : 0.099460
iteration 7341 : model1 loss : 0.147585 model2 loss : 0.140659
iteration 7342 : model1 loss : 0.138571 model2 loss : 0.084938
iteration 7343 : model1 loss : 0.120157 model2 loss : 0.089292
iteration 7344 : model1 loss : 0.175607 model2 loss : 0.113578
 73%|███████████████████▊       | 432/589 [2:56:03<1:00:31, 23.13s/it]iteration 7345 : model1 loss : 0.150817 model2 loss : 0.114510
iteration 7346 : model1 loss : 0.108597 model2 loss : 0.091468
iteration 7347 : model1 loss : 0.147092 model2 loss : 0.095651
iteration 7348 : model1 loss : 0.139429 model2 loss : 0.101876
iteration 7349 : model1 loss : 0.092558 model2 loss : 0.116977
iteration 7350 : model1 loss : 0.131374 model2 loss : 0.087156
iteration 7351 : model1 loss : 0.108559 model2 loss : 0.092930
iteration 7352 : model1 loss : 0.160485 model2 loss : 0.128797
iteration 7353 : model1 loss : 0.121116 model2 loss : 0.075745
iteration 7354 : model1 loss : 0.146870 model2 loss : 0.113015
iteration 7355 : model1 loss : 0.149380 model2 loss : 0.135217
iteration 7356 : model1 loss : 0.105254 model2 loss : 0.135638
iteration 7357 : model1 loss : 0.162298 model2 loss : 0.126360
iteration 7358 : model1 loss : 0.141279 model2 loss : 0.099044
iteration 7359 : model1 loss : 0.137681 model2 loss : 0.096012
iteration 7360 : model1 loss : 0.113863 model2 loss : 0.107908
iteration 7361 : model1 loss : 0.097564 model2 loss : 0.100698
 74%|█████████████████████▎       | 433/589 [2:56:26<59:57, 23.06s/it]iteration 7362 : model1 loss : 0.175655 model2 loss : 0.112681
iteration 7363 : model1 loss : 0.131399 model2 loss : 0.127936
iteration 7364 : model1 loss : 0.236217 model2 loss : 0.119072
iteration 7365 : model1 loss : 0.121850 model2 loss : 0.113503
iteration 7366 : model1 loss : 0.126304 model2 loss : 0.122204
iteration 7367 : model1 loss : 0.126937 model2 loss : 0.112346
iteration 7368 : model1 loss : 0.140676 model2 loss : 0.114124
iteration 7369 : model1 loss : 0.085648 model2 loss : 0.084447
iteration 7370 : model1 loss : 0.126944 model2 loss : 0.105630
iteration 7371 : model1 loss : 0.115411 model2 loss : 0.082268
iteration 7372 : model1 loss : 0.134889 model2 loss : 0.099351
iteration 7373 : model1 loss : 0.120096 model2 loss : 0.098022
iteration 7374 : model1 loss : 0.137092 model2 loss : 0.100566
iteration 7375 : model1 loss : 0.113110 model2 loss : 0.145964
iteration 7376 : model1 loss : 0.127950 model2 loss : 0.066943
iteration 7377 : model1 loss : 0.116394 model2 loss : 0.109127
iteration 7378 : model1 loss : 0.116761 model2 loss : 0.089206
 74%|█████████████████████▎       | 434/589 [2:56:48<59:21, 22.98s/it]iteration 7379 : model1 loss : 0.126874 model2 loss : 0.087344
iteration 7380 : model1 loss : 0.136956 model2 loss : 0.100181
iteration 7381 : model1 loss : 0.107216 model2 loss : 0.096500
iteration 7382 : model1 loss : 0.139575 model2 loss : 0.107370
iteration 7383 : model1 loss : 0.128268 model2 loss : 0.115409
iteration 7384 : model1 loss : 0.200208 model2 loss : 0.121763
iteration 7385 : model1 loss : 0.143733 model2 loss : 0.114418
iteration 7386 : model1 loss : 0.162097 model2 loss : 0.082642
iteration 7387 : model1 loss : 0.079936 model2 loss : 0.084968
iteration 7388 : model1 loss : 0.139588 model2 loss : 0.111544
iteration 7389 : model1 loss : 0.094615 model2 loss : 0.081669
iteration 7390 : model1 loss : 0.139345 model2 loss : 0.109536
iteration 7391 : model1 loss : 0.160092 model2 loss : 0.086992
iteration 7392 : model1 loss : 0.209263 model2 loss : 0.111964
iteration 7393 : model1 loss : 0.132002 model2 loss : 0.100112
iteration 7394 : model1 loss : 0.122772 model2 loss : 0.102992
iteration 7395 : model1 loss : 0.130364 model2 loss : 0.079101
 74%|█████████████████████▍       | 435/589 [2:57:11<58:50, 22.93s/it]iteration 7396 : model1 loss : 0.123861 model2 loss : 0.086837
iteration 7397 : model1 loss : 0.161631 model2 loss : 0.104907
iteration 7398 : model1 loss : 0.156482 model2 loss : 0.101579
iteration 7399 : model1 loss : 0.173943 model2 loss : 0.133083
iteration 7400 : model1 loss : 0.116435 model2 loss : 0.112632
iteration 7400 : model1_mean_dice : 0.633628 model1_mean_hd95 : 93.104873 model1_mean_iou : 0.511237
iteration 7400 : model2_mean_dice : 0.747764 model2_mean_hd95 : 73.362296 model2_mean_iou : 0.634796
iteration 7401 : model1 loss : 0.141134 model2 loss : 0.119998
iteration 7402 : model1 loss : 0.099472 model2 loss : 0.066063
iteration 7403 : model1 loss : 0.104249 model2 loss : 0.095313
iteration 7404 : model1 loss : 0.125030 model2 loss : 0.089110
iteration 7405 : model1 loss : 0.121235 model2 loss : 0.064472
iteration 7406 : model1 loss : 0.190797 model2 loss : 0.153273
iteration 7407 : model1 loss : 0.145924 model2 loss : 0.104333
iteration 7408 : model1 loss : 0.121172 model2 loss : 0.092531
iteration 7409 : model1 loss : 0.135428 model2 loss : 0.108395
iteration 7410 : model1 loss : 0.149935 model2 loss : 0.091372
iteration 7411 : model1 loss : 0.138494 model2 loss : 0.113470
iteration 7412 : model1 loss : 0.158788 model2 loss : 0.089659
 74%|███████████████████▉       | 436/589 [2:57:54<1:13:30, 28.83s/it]iteration 7413 : model1 loss : 0.113806 model2 loss : 0.097511
iteration 7414 : model1 loss : 0.110933 model2 loss : 0.105181
iteration 7415 : model1 loss : 0.145589 model2 loss : 0.117636
iteration 7416 : model1 loss : 0.140710 model2 loss : 0.106583
iteration 7417 : model1 loss : 0.151288 model2 loss : 0.110127
iteration 7418 : model1 loss : 0.129026 model2 loss : 0.096465
iteration 7419 : model1 loss : 0.163028 model2 loss : 0.097256
iteration 7420 : model1 loss : 0.134685 model2 loss : 0.077318
iteration 7421 : model1 loss : 0.142274 model2 loss : 0.109751
iteration 7422 : model1 loss : 0.128444 model2 loss : 0.089713
iteration 7423 : model1 loss : 0.112680 model2 loss : 0.080213
iteration 7424 : model1 loss : 0.160086 model2 loss : 0.096582
iteration 7425 : model1 loss : 0.155436 model2 loss : 0.091101
iteration 7426 : model1 loss : 0.133865 model2 loss : 0.093469
iteration 7427 : model1 loss : 0.129733 model2 loss : 0.089135
iteration 7428 : model1 loss : 0.101954 model2 loss : 0.081157
iteration 7429 : model1 loss : 0.153086 model2 loss : 0.132883
 74%|████████████████████       | 437/589 [2:58:17<1:08:23, 27.00s/it]iteration 7430 : model1 loss : 0.147910 model2 loss : 0.098592
iteration 7431 : model1 loss : 0.133454 model2 loss : 0.106296
iteration 7432 : model1 loss : 0.152833 model2 loss : 0.086431
iteration 7433 : model1 loss : 0.151246 model2 loss : 0.072602
iteration 7434 : model1 loss : 0.102486 model2 loss : 0.106520
iteration 7435 : model1 loss : 0.122456 model2 loss : 0.105132
iteration 7436 : model1 loss : 0.137776 model2 loss : 0.103720
iteration 7437 : model1 loss : 0.146195 model2 loss : 0.106209
iteration 7438 : model1 loss : 0.154279 model2 loss : 0.067126
iteration 7439 : model1 loss : 0.145128 model2 loss : 0.092492
iteration 7440 : model1 loss : 0.181823 model2 loss : 0.114938
iteration 7441 : model1 loss : 0.122674 model2 loss : 0.089627
iteration 7442 : model1 loss : 0.110401 model2 loss : 0.086144
iteration 7443 : model1 loss : 0.147150 model2 loss : 0.093215
iteration 7444 : model1 loss : 0.202805 model2 loss : 0.108360
iteration 7445 : model1 loss : 0.126940 model2 loss : 0.066876
iteration 7446 : model1 loss : 0.155918 model2 loss : 0.095227
 74%|████████████████████       | 438/589 [2:58:39<1:04:44, 25.73s/it]iteration 7447 : model1 loss : 0.178038 model2 loss : 0.118520
iteration 7448 : model1 loss : 0.128020 model2 loss : 0.084215
iteration 7449 : model1 loss : 0.171988 model2 loss : 0.132526
iteration 7450 : model1 loss : 0.154685 model2 loss : 0.091419
iteration 7451 : model1 loss : 0.156696 model2 loss : 0.100036
iteration 7452 : model1 loss : 0.133478 model2 loss : 0.123003
iteration 7453 : model1 loss : 0.120339 model2 loss : 0.109666
iteration 7454 : model1 loss : 0.123464 model2 loss : 0.119536
iteration 7455 : model1 loss : 0.112883 model2 loss : 0.090854
iteration 7456 : model1 loss : 0.107150 model2 loss : 0.081334
iteration 7457 : model1 loss : 0.155667 model2 loss : 0.144667
iteration 7458 : model1 loss : 0.114024 model2 loss : 0.104190
iteration 7459 : model1 loss : 0.104311 model2 loss : 0.103876
iteration 7460 : model1 loss : 0.128151 model2 loss : 0.120840
iteration 7461 : model1 loss : 0.117963 model2 loss : 0.078943
iteration 7462 : model1 loss : 0.142663 model2 loss : 0.100194
iteration 7463 : model1 loss : 0.142935 model2 loss : 0.097222
 75%|████████████████████       | 439/589 [2:59:02<1:02:08, 24.86s/it]iteration 7464 : model1 loss : 0.115847 model2 loss : 0.088520
iteration 7465 : model1 loss : 0.123968 model2 loss : 0.113921
iteration 7466 : model1 loss : 0.168113 model2 loss : 0.139433
iteration 7467 : model1 loss : 0.157892 model2 loss : 0.127719
iteration 7468 : model1 loss : 0.149306 model2 loss : 0.096329
iteration 7469 : model1 loss : 0.096178 model2 loss : 0.073778
iteration 7470 : model1 loss : 0.136608 model2 loss : 0.122797
iteration 7471 : model1 loss : 0.091771 model2 loss : 0.093353
iteration 7472 : model1 loss : 0.122901 model2 loss : 0.100625
iteration 7473 : model1 loss : 0.137764 model2 loss : 0.132457
iteration 7474 : model1 loss : 0.173822 model2 loss : 0.140828
iteration 7475 : model1 loss : 0.235000 model2 loss : 0.123591
iteration 7476 : model1 loss : 0.120912 model2 loss : 0.117152
iteration 7477 : model1 loss : 0.137959 model2 loss : 0.106099
iteration 7478 : model1 loss : 0.096455 model2 loss : 0.065748
iteration 7479 : model1 loss : 0.116191 model2 loss : 0.077352
iteration 7480 : model1 loss : 0.110740 model2 loss : 0.116186
 75%|████████████████████▏      | 440/589 [2:59:25<1:00:09, 24.22s/it]iteration 7481 : model1 loss : 0.127467 model2 loss : 0.092877
iteration 7482 : model1 loss : 0.114188 model2 loss : 0.092691
iteration 7483 : model1 loss : 0.138171 model2 loss : 0.111272
iteration 7484 : model1 loss : 0.185518 model2 loss : 0.112818
iteration 7485 : model1 loss : 0.124598 model2 loss : 0.128683
iteration 7486 : model1 loss : 0.122584 model2 loss : 0.099779
iteration 7487 : model1 loss : 0.143523 model2 loss : 0.100199
iteration 7488 : model1 loss : 0.163185 model2 loss : 0.114424
iteration 7489 : model1 loss : 0.198582 model2 loss : 0.122360
iteration 7490 : model1 loss : 0.119434 model2 loss : 0.095132
iteration 7491 : model1 loss : 0.129843 model2 loss : 0.078720
iteration 7492 : model1 loss : 0.125509 model2 loss : 0.082958
iteration 7493 : model1 loss : 0.112393 model2 loss : 0.094213
iteration 7494 : model1 loss : 0.151116 model2 loss : 0.113663
iteration 7495 : model1 loss : 0.118580 model2 loss : 0.106468
iteration 7496 : model1 loss : 0.127336 model2 loss : 0.085478
iteration 7497 : model1 loss : 0.119819 model2 loss : 0.085518
 75%|█████████████████████▋       | 441/589 [2:59:48<58:40, 23.78s/it]iteration 7498 : model1 loss : 0.126571 model2 loss : 0.093712
iteration 7499 : model1 loss : 0.161490 model2 loss : 0.113740
iteration 7500 : model1 loss : 0.129351 model2 loss : 0.112556
iteration 7501 : model1 loss : 0.122724 model2 loss : 0.081463
iteration 7502 : model1 loss : 0.145584 model2 loss : 0.112422
iteration 7503 : model1 loss : 0.167799 model2 loss : 0.152667
iteration 7504 : model1 loss : 0.146738 model2 loss : 0.116547
iteration 7505 : model1 loss : 0.130062 model2 loss : 0.104297
iteration 7506 : model1 loss : 0.128075 model2 loss : 0.136503
iteration 7507 : model1 loss : 0.134525 model2 loss : 0.080692
iteration 7508 : model1 loss : 0.158416 model2 loss : 0.077064
iteration 7509 : model1 loss : 0.126597 model2 loss : 0.099884
iteration 7510 : model1 loss : 0.079787 model2 loss : 0.057120
iteration 7511 : model1 loss : 0.150751 model2 loss : 0.137841
iteration 7512 : model1 loss : 0.157326 model2 loss : 0.126099
iteration 7513 : model1 loss : 0.099139 model2 loss : 0.091522
iteration 7514 : model1 loss : 0.142249 model2 loss : 0.084137
 75%|█████████████████████▊       | 442/589 [3:00:11<57:38, 23.53s/it]iteration 7515 : model1 loss : 0.160914 model2 loss : 0.091406
iteration 7516 : model1 loss : 0.152751 model2 loss : 0.104481
iteration 7517 : model1 loss : 0.150464 model2 loss : 0.122189
iteration 7518 : model1 loss : 0.136244 model2 loss : 0.095907
iteration 7519 : model1 loss : 0.123713 model2 loss : 0.097103
iteration 7520 : model1 loss : 0.118517 model2 loss : 0.081488
iteration 7521 : model1 loss : 0.142120 model2 loss : 0.080792
iteration 7522 : model1 loss : 0.120915 model2 loss : 0.092901
iteration 7523 : model1 loss : 0.134032 model2 loss : 0.102089
iteration 7524 : model1 loss : 0.098689 model2 loss : 0.133455
iteration 7525 : model1 loss : 0.092650 model2 loss : 0.068348
iteration 7526 : model1 loss : 0.170519 model2 loss : 0.129981
iteration 7527 : model1 loss : 0.152613 model2 loss : 0.126802
iteration 7528 : model1 loss : 0.113170 model2 loss : 0.078233
iteration 7529 : model1 loss : 0.126683 model2 loss : 0.087550
iteration 7530 : model1 loss : 0.099058 model2 loss : 0.096245
iteration 7531 : model1 loss : 0.167622 model2 loss : 0.107811
 75%|█████████████████████▊       | 443/589 [3:00:33<56:41, 23.30s/it]iteration 7532 : model1 loss : 0.127338 model2 loss : 0.106812
iteration 7533 : model1 loss : 0.155366 model2 loss : 0.125143
iteration 7534 : model1 loss : 0.160333 model2 loss : 0.123605
iteration 7535 : model1 loss : 0.118897 model2 loss : 0.073346
iteration 7536 : model1 loss : 0.117144 model2 loss : 0.100600
iteration 7537 : model1 loss : 0.120621 model2 loss : 0.083410
iteration 7538 : model1 loss : 0.105758 model2 loss : 0.070350
iteration 7539 : model1 loss : 0.124416 model2 loss : 0.106577
iteration 7540 : model1 loss : 0.101729 model2 loss : 0.099827
iteration 7541 : model1 loss : 0.123344 model2 loss : 0.119996
iteration 7542 : model1 loss : 0.166750 model2 loss : 0.105008
iteration 7543 : model1 loss : 0.119456 model2 loss : 0.114459
iteration 7544 : model1 loss : 0.120424 model2 loss : 0.100759
iteration 7545 : model1 loss : 0.116961 model2 loss : 0.077379
iteration 7546 : model1 loss : 0.101370 model2 loss : 0.061614
iteration 7547 : model1 loss : 0.145041 model2 loss : 0.086664
iteration 7548 : model1 loss : 0.134318 model2 loss : 0.115613
 75%|█████████████████████▊       | 444/589 [3:00:56<55:56, 23.15s/it]iteration 7549 : model1 loss : 0.115117 model2 loss : 0.106353
iteration 7550 : model1 loss : 0.173490 model2 loss : 0.100216
iteration 7551 : model1 loss : 0.129169 model2 loss : 0.090013
iteration 7552 : model1 loss : 0.137765 model2 loss : 0.102051
iteration 7553 : model1 loss : 0.121959 model2 loss : 0.109272
iteration 7554 : model1 loss : 0.090075 model2 loss : 0.083329
iteration 7555 : model1 loss : 0.181894 model2 loss : 0.111965
iteration 7556 : model1 loss : 0.123635 model2 loss : 0.150439
iteration 7557 : model1 loss : 0.116080 model2 loss : 0.118026
iteration 7558 : model1 loss : 0.111501 model2 loss : 0.111663
iteration 7559 : model1 loss : 0.145995 model2 loss : 0.101416
iteration 7560 : model1 loss : 0.132542 model2 loss : 0.070056
iteration 7561 : model1 loss : 0.181881 model2 loss : 0.141046
iteration 7562 : model1 loss : 0.146291 model2 loss : 0.108528
iteration 7563 : model1 loss : 0.125819 model2 loss : 0.091730
iteration 7564 : model1 loss : 0.131941 model2 loss : 0.092071
iteration 7565 : model1 loss : 0.115763 model2 loss : 0.069999
 76%|█████████████████████▉       | 445/589 [3:01:19<55:20, 23.06s/it]iteration 7566 : model1 loss : 0.116835 model2 loss : 0.097426
iteration 7567 : model1 loss : 0.115659 model2 loss : 0.081776
iteration 7568 : model1 loss : 0.170718 model2 loss : 0.095794
iteration 7569 : model1 loss : 0.146864 model2 loss : 0.110393
iteration 7570 : model1 loss : 0.124925 model2 loss : 0.099566
iteration 7571 : model1 loss : 0.104170 model2 loss : 0.080194
iteration 7572 : model1 loss : 0.114581 model2 loss : 0.073177
iteration 7573 : model1 loss : 0.126821 model2 loss : 0.094620
iteration 7574 : model1 loss : 0.136290 model2 loss : 0.111742
iteration 7575 : model1 loss : 0.096293 model2 loss : 0.069840
iteration 7576 : model1 loss : 0.165682 model2 loss : 0.106773
iteration 7577 : model1 loss : 0.112131 model2 loss : 0.080768
iteration 7578 : model1 loss : 0.119078 model2 loss : 0.102802
iteration 7579 : model1 loss : 0.122334 model2 loss : 0.091243
iteration 7580 : model1 loss : 0.137387 model2 loss : 0.068867
iteration 7581 : model1 loss : 0.160569 model2 loss : 0.124219
iteration 7582 : model1 loss : 0.156250 model2 loss : 0.128115
 76%|█████████████████████▉       | 446/589 [3:01:42<54:45, 22.97s/it]iteration 7583 : model1 loss : 0.107763 model2 loss : 0.087894
iteration 7584 : model1 loss : 0.138124 model2 loss : 0.098206
iteration 7585 : model1 loss : 0.179402 model2 loss : 0.111636
iteration 7586 : model1 loss : 0.096178 model2 loss : 0.072495
iteration 7587 : model1 loss : 0.120202 model2 loss : 0.092086
iteration 7588 : model1 loss : 0.127173 model2 loss : 0.101551
iteration 7589 : model1 loss : 0.171257 model2 loss : 0.134526
iteration 7590 : model1 loss : 0.119233 model2 loss : 0.089738
iteration 7591 : model1 loss : 0.145008 model2 loss : 0.137404
iteration 7592 : model1 loss : 0.138274 model2 loss : 0.110761
iteration 7593 : model1 loss : 0.180544 model2 loss : 0.123708
iteration 7594 : model1 loss : 0.154032 model2 loss : 0.087975
iteration 7595 : model1 loss : 0.149331 model2 loss : 0.093673
iteration 7596 : model1 loss : 0.130337 model2 loss : 0.090842
iteration 7597 : model1 loss : 0.123338 model2 loss : 0.099561
iteration 7598 : model1 loss : 0.126674 model2 loss : 0.099513
iteration 7599 : model1 loss : 0.194237 model2 loss : 0.113265
 76%|██████████████████████       | 447/589 [3:02:04<54:12, 22.90s/it]iteration 7600 : model1 loss : 0.121810 model2 loss : 0.106057
iteration 7600 : model1_mean_dice : 0.713108 model1_mean_hd95 : 82.807688 model1_mean_iou : 0.584482
iteration 7600 : model2_mean_dice : 0.744159 model2_mean_hd95 : 70.579467 model2_mean_iou : 0.631450
iteration 7601 : model1 loss : 0.156944 model2 loss : 0.097936
iteration 7602 : model1 loss : 0.120228 model2 loss : 0.100263
iteration 7603 : model1 loss : 0.137121 model2 loss : 0.116145
iteration 7604 : model1 loss : 0.169934 model2 loss : 0.106248
iteration 7605 : model1 loss : 0.135484 model2 loss : 0.102891
iteration 7606 : model1 loss : 0.137253 model2 loss : 0.109306
iteration 7607 : model1 loss : 0.117468 model2 loss : 0.078833
iteration 7608 : model1 loss : 0.139069 model2 loss : 0.115292
iteration 7609 : model1 loss : 0.134750 model2 loss : 0.093338
iteration 7610 : model1 loss : 0.119971 model2 loss : 0.084707
iteration 7611 : model1 loss : 0.110308 model2 loss : 0.106580
iteration 7612 : model1 loss : 0.126723 model2 loss : 0.097046
iteration 7613 : model1 loss : 0.130542 model2 loss : 0.070954
iteration 7614 : model1 loss : 0.140024 model2 loss : 0.101196
iteration 7615 : model1 loss : 0.118612 model2 loss : 0.106833
iteration 7616 : model1 loss : 0.129647 model2 loss : 0.115522
 76%|████████████████████▌      | 448/589 [3:02:48<1:08:07, 28.99s/it]iteration 7617 : model1 loss : 0.098901 model2 loss : 0.092163
iteration 7618 : model1 loss : 0.141866 model2 loss : 0.079983
iteration 7619 : model1 loss : 0.132555 model2 loss : 0.101521
iteration 7620 : model1 loss : 0.165531 model2 loss : 0.114690
iteration 7621 : model1 loss : 0.135546 model2 loss : 0.122906
iteration 7622 : model1 loss : 0.137999 model2 loss : 0.143254
iteration 7623 : model1 loss : 0.142946 model2 loss : 0.083227
iteration 7624 : model1 loss : 0.157583 model2 loss : 0.127442
iteration 7625 : model1 loss : 0.133111 model2 loss : 0.131513
iteration 7626 : model1 loss : 0.126841 model2 loss : 0.081180
iteration 7627 : model1 loss : 0.131353 model2 loss : 0.100408
iteration 7628 : model1 loss : 0.141177 model2 loss : 0.099815
iteration 7629 : model1 loss : 0.133343 model2 loss : 0.090426
iteration 7630 : model1 loss : 0.085042 model2 loss : 0.084587
iteration 7631 : model1 loss : 0.121885 model2 loss : 0.146699
iteration 7632 : model1 loss : 0.108631 model2 loss : 0.078189
iteration 7633 : model1 loss : 0.101090 model2 loss : 0.093533
 76%|████████████████████▌      | 449/589 [3:03:10<1:03:15, 27.11s/it]iteration 7634 : model1 loss : 0.172979 model2 loss : 0.112736
iteration 7635 : model1 loss : 0.127167 model2 loss : 0.103257
iteration 7636 : model1 loss : 0.130328 model2 loss : 0.078869
iteration 7637 : model1 loss : 0.103635 model2 loss : 0.068264
iteration 7638 : model1 loss : 0.126530 model2 loss : 0.083312
iteration 7639 : model1 loss : 0.140895 model2 loss : 0.098516
iteration 7640 : model1 loss : 0.153857 model2 loss : 0.105385
iteration 7641 : model1 loss : 0.117635 model2 loss : 0.126212
iteration 7642 : model1 loss : 0.172753 model2 loss : 0.089162
iteration 7643 : model1 loss : 0.132415 model2 loss : 0.118957
iteration 7644 : model1 loss : 0.156823 model2 loss : 0.105454
iteration 7645 : model1 loss : 0.102195 model2 loss : 0.071285
iteration 7646 : model1 loss : 0.156826 model2 loss : 0.090765
iteration 7647 : model1 loss : 0.152207 model2 loss : 0.090824
iteration 7648 : model1 loss : 0.098232 model2 loss : 0.075214
iteration 7649 : model1 loss : 0.137757 model2 loss : 0.113760
iteration 7650 : model1 loss : 0.118678 model2 loss : 0.071850
 76%|██████████████████████▏      | 450/589 [3:03:33<59:51, 25.84s/it]iteration 7651 : model1 loss : 0.119706 model2 loss : 0.108547
iteration 7652 : model1 loss : 0.149138 model2 loss : 0.100020
iteration 7653 : model1 loss : 0.111527 model2 loss : 0.082423
iteration 7654 : model1 loss : 0.122834 model2 loss : 0.092654
iteration 7655 : model1 loss : 0.147311 model2 loss : 0.111813
iteration 7656 : model1 loss : 0.130567 model2 loss : 0.095358
iteration 7657 : model1 loss : 0.143533 model2 loss : 0.088508
iteration 7658 : model1 loss : 0.114824 model2 loss : 0.099534
iteration 7659 : model1 loss : 0.114427 model2 loss : 0.092106
iteration 7660 : model1 loss : 0.124338 model2 loss : 0.104173
iteration 7661 : model1 loss : 0.189600 model2 loss : 0.189828
iteration 7662 : model1 loss : 0.121078 model2 loss : 0.117288
iteration 7663 : model1 loss : 0.153176 model2 loss : 0.110628
iteration 7664 : model1 loss : 0.153466 model2 loss : 0.119579
iteration 7665 : model1 loss : 0.106095 model2 loss : 0.103525
iteration 7666 : model1 loss : 0.132405 model2 loss : 0.116706
iteration 7667 : model1 loss : 0.127964 model2 loss : 0.090001
 77%|██████████████████████▏      | 451/589 [3:03:56<57:19, 24.92s/it]iteration 7668 : model1 loss : 0.167336 model2 loss : 0.128572
iteration 7669 : model1 loss : 0.142001 model2 loss : 0.110516
iteration 7670 : model1 loss : 0.118626 model2 loss : 0.088692
iteration 7671 : model1 loss : 0.120592 model2 loss : 0.072368
iteration 7672 : model1 loss : 0.180591 model2 loss : 0.099493
iteration 7673 : model1 loss : 0.146054 model2 loss : 0.154298
iteration 7674 : model1 loss : 0.077515 model2 loss : 0.070344
iteration 7675 : model1 loss : 0.111052 model2 loss : 0.086099
iteration 7676 : model1 loss : 0.119005 model2 loss : 0.085126
iteration 7677 : model1 loss : 0.136385 model2 loss : 0.107167
iteration 7678 : model1 loss : 0.114178 model2 loss : 0.097671
iteration 7679 : model1 loss : 0.123221 model2 loss : 0.083876
iteration 7680 : model1 loss : 0.145841 model2 loss : 0.087750
iteration 7681 : model1 loss : 0.142741 model2 loss : 0.096530
iteration 7682 : model1 loss : 0.139962 model2 loss : 0.101201
iteration 7683 : model1 loss : 0.117075 model2 loss : 0.116824
iteration 7684 : model1 loss : 0.185355 model2 loss : 0.102081
 77%|██████████████████████▎      | 452/589 [3:04:19<55:24, 24.26s/it]iteration 7685 : model1 loss : 0.107661 model2 loss : 0.086255
iteration 7686 : model1 loss : 0.128517 model2 loss : 0.084826
iteration 7687 : model1 loss : 0.137744 model2 loss : 0.099004
iteration 7688 : model1 loss : 0.116453 model2 loss : 0.074251
iteration 7689 : model1 loss : 0.148646 model2 loss : 0.111574
iteration 7690 : model1 loss : 0.132633 model2 loss : 0.095680
iteration 7691 : model1 loss : 0.124701 model2 loss : 0.086913
iteration 7692 : model1 loss : 0.134225 model2 loss : 0.122641
iteration 7693 : model1 loss : 0.155760 model2 loss : 0.152879
iteration 7694 : model1 loss : 0.108912 model2 loss : 0.083647
iteration 7695 : model1 loss : 0.113021 model2 loss : 0.101451
iteration 7696 : model1 loss : 0.107872 model2 loss : 0.079447
iteration 7697 : model1 loss : 0.160401 model2 loss : 0.106554
iteration 7698 : model1 loss : 0.170622 model2 loss : 0.103567
iteration 7699 : model1 loss : 0.125390 model2 loss : 0.092745
iteration 7700 : model1 loss : 0.103792 model2 loss : 0.087742
iteration 7701 : model1 loss : 0.125788 model2 loss : 0.095301
 77%|██████████████████████▎      | 453/589 [3:04:42<54:03, 23.85s/it]iteration 7702 : model1 loss : 0.150426 model2 loss : 0.085118
iteration 7703 : model1 loss : 0.101314 model2 loss : 0.071373
iteration 7704 : model1 loss : 0.153387 model2 loss : 0.105083
iteration 7705 : model1 loss : 0.119140 model2 loss : 0.091553
iteration 7706 : model1 loss : 0.114996 model2 loss : 0.091935
iteration 7707 : model1 loss : 0.162595 model2 loss : 0.134984
iteration 7708 : model1 loss : 0.104495 model2 loss : 0.097747
iteration 7709 : model1 loss : 0.171024 model2 loss : 0.097356
iteration 7710 : model1 loss : 0.131322 model2 loss : 0.097634
iteration 7711 : model1 loss : 0.134949 model2 loss : 0.118484
iteration 7712 : model1 loss : 0.137687 model2 loss : 0.106220
iteration 7713 : model1 loss : 0.117025 model2 loss : 0.098829
iteration 7714 : model1 loss : 0.127489 model2 loss : 0.081120
iteration 7715 : model1 loss : 0.101407 model2 loss : 0.078056
iteration 7716 : model1 loss : 0.116427 model2 loss : 0.091191
iteration 7717 : model1 loss : 0.120495 model2 loss : 0.100347
iteration 7718 : model1 loss : 0.122653 model2 loss : 0.086364
 77%|██████████████████████▎      | 454/589 [3:05:04<52:55, 23.53s/it]iteration 7719 : model1 loss : 0.118014 model2 loss : 0.075883
iteration 7720 : model1 loss : 0.136513 model2 loss : 0.149244
iteration 7721 : model1 loss : 0.173198 model2 loss : 0.115295
iteration 7722 : model1 loss : 0.111802 model2 loss : 0.082023
iteration 7723 : model1 loss : 0.149601 model2 loss : 0.088317
iteration 7724 : model1 loss : 0.130979 model2 loss : 0.102273
iteration 7725 : model1 loss : 0.129973 model2 loss : 0.091287
iteration 7726 : model1 loss : 0.132567 model2 loss : 0.098246
iteration 7727 : model1 loss : 0.097102 model2 loss : 0.081447
iteration 7728 : model1 loss : 0.116925 model2 loss : 0.093493
iteration 7729 : model1 loss : 0.119587 model2 loss : 0.098959
iteration 7730 : model1 loss : 0.127712 model2 loss : 0.114536
iteration 7731 : model1 loss : 0.141599 model2 loss : 0.091763
iteration 7732 : model1 loss : 0.129422 model2 loss : 0.092039
iteration 7733 : model1 loss : 0.116642 model2 loss : 0.084721
iteration 7734 : model1 loss : 0.120566 model2 loss : 0.099345
iteration 7735 : model1 loss : 0.166852 model2 loss : 0.077546
 77%|██████████████████████▍      | 455/589 [3:05:27<52:02, 23.30s/it]iteration 7736 : model1 loss : 0.128167 model2 loss : 0.093959
iteration 7737 : model1 loss : 0.140450 model2 loss : 0.104839
iteration 7738 : model1 loss : 0.132993 model2 loss : 0.088697
iteration 7739 : model1 loss : 0.132370 model2 loss : 0.078934
iteration 7740 : model1 loss : 0.130928 model2 loss : 0.106253
iteration 7741 : model1 loss : 0.092964 model2 loss : 0.084762
iteration 7742 : model1 loss : 0.123522 model2 loss : 0.097816
iteration 7743 : model1 loss : 0.176711 model2 loss : 0.130354
iteration 7744 : model1 loss : 0.099414 model2 loss : 0.076270
iteration 7745 : model1 loss : 0.130845 model2 loss : 0.090432
iteration 7746 : model1 loss : 0.158931 model2 loss : 0.109482
iteration 7747 : model1 loss : 0.140233 model2 loss : 0.109877
iteration 7748 : model1 loss : 0.124227 model2 loss : 0.080390
iteration 7749 : model1 loss : 0.133644 model2 loss : 0.108143
iteration 7750 : model1 loss : 0.128999 model2 loss : 0.092211
iteration 7751 : model1 loss : 0.124052 model2 loss : 0.086295
iteration 7752 : model1 loss : 0.162348 model2 loss : 0.070337
 77%|██████████████████████▍      | 456/589 [3:05:50<51:20, 23.16s/it]iteration 7753 : model1 loss : 0.174493 model2 loss : 0.082081
iteration 7754 : model1 loss : 0.233357 model2 loss : 0.097876
iteration 7755 : model1 loss : 0.184481 model2 loss : 0.088461
iteration 7756 : model1 loss : 0.101046 model2 loss : 0.070218
iteration 7757 : model1 loss : 0.146685 model2 loss : 0.123624
iteration 7758 : model1 loss : 0.119402 model2 loss : 0.108157
iteration 7759 : model1 loss : 0.107360 model2 loss : 0.077545
iteration 7760 : model1 loss : 0.177615 model2 loss : 0.148595
iteration 7761 : model1 loss : 0.117091 model2 loss : 0.090961
iteration 7762 : model1 loss : 0.121146 model2 loss : 0.084016
iteration 7763 : model1 loss : 0.141250 model2 loss : 0.082889
iteration 7764 : model1 loss : 0.114356 model2 loss : 0.094318
iteration 7765 : model1 loss : 0.130833 model2 loss : 0.098506
iteration 7766 : model1 loss : 0.166637 model2 loss : 0.121800
iteration 7767 : model1 loss : 0.114818 model2 loss : 0.093683
iteration 7768 : model1 loss : 0.082120 model2 loss : 0.060372
iteration 7769 : model1 loss : 0.132860 model2 loss : 0.138084
 78%|██████████████████████▌      | 457/589 [3:06:13<50:42, 23.05s/it]iteration 7770 : model1 loss : 0.166229 model2 loss : 0.096644
iteration 7771 : model1 loss : 0.136121 model2 loss : 0.082682
iteration 7772 : model1 loss : 0.116263 model2 loss : 0.070759
iteration 7773 : model1 loss : 0.130913 model2 loss : 0.101622
iteration 7774 : model1 loss : 0.115735 model2 loss : 0.092202
iteration 7775 : model1 loss : 0.092475 model2 loss : 0.082583
iteration 7776 : model1 loss : 0.156698 model2 loss : 0.124582
iteration 7777 : model1 loss : 0.136905 model2 loss : 0.109550
iteration 7778 : model1 loss : 0.147117 model2 loss : 0.087197
iteration 7779 : model1 loss : 0.134190 model2 loss : 0.091252
iteration 7780 : model1 loss : 0.112834 model2 loss : 0.092164
iteration 7781 : model1 loss : 0.116111 model2 loss : 0.102701
iteration 7782 : model1 loss : 0.115262 model2 loss : 0.094227
iteration 7783 : model1 loss : 0.102703 model2 loss : 0.090441
iteration 7784 : model1 loss : 0.145354 model2 loss : 0.129166
iteration 7785 : model1 loss : 0.140952 model2 loss : 0.082912
iteration 7786 : model1 loss : 0.146619 model2 loss : 0.093706
 78%|██████████████████████▌      | 458/589 [3:06:36<50:08, 22.96s/it]iteration 7787 : model1 loss : 0.147729 model2 loss : 0.100239
iteration 7788 : model1 loss : 0.093347 model2 loss : 0.066154
iteration 7789 : model1 loss : 0.110303 model2 loss : 0.087014
iteration 7790 : model1 loss : 0.096935 model2 loss : 0.062919
iteration 7791 : model1 loss : 0.104862 model2 loss : 0.095582
iteration 7792 : model1 loss : 0.155155 model2 loss : 0.100854
iteration 7793 : model1 loss : 0.120716 model2 loss : 0.089167
iteration 7794 : model1 loss : 0.123356 model2 loss : 0.090903
iteration 7795 : model1 loss : 0.150451 model2 loss : 0.115354
iteration 7796 : model1 loss : 0.179284 model2 loss : 0.084190
iteration 7797 : model1 loss : 0.178706 model2 loss : 0.121192
iteration 7798 : model1 loss : 0.133333 model2 loss : 0.078537
iteration 7799 : model1 loss : 0.121637 model2 loss : 0.102092
iteration 7800 : model1 loss : 0.129692 model2 loss : 0.081080
iteration 7800 : model1_mean_dice : 0.689781 model1_mean_hd95 : 90.815795 model1_mean_iou : 0.559051
iteration 7800 : model2_mean_dice : 0.732471 model2_mean_hd95 : 69.264366 model2_mean_iou : 0.619525
iteration 7801 : model1 loss : 0.153413 model2 loss : 0.103039
iteration 7802 : model1 loss : 0.123336 model2 loss : 0.110145
iteration 7803 : model1 loss : 0.122934 model2 loss : 0.124124
 78%|█████████████████████      | 459/589 [3:07:18<1:02:34, 28.88s/it]iteration 7804 : model1 loss : 0.149121 model2 loss : 0.125574
iteration 7805 : model1 loss : 0.154983 model2 loss : 0.090938
iteration 7806 : model1 loss : 0.129833 model2 loss : 0.061210
iteration 7807 : model1 loss : 0.151846 model2 loss : 0.094618
iteration 7808 : model1 loss : 0.107901 model2 loss : 0.087694
iteration 7809 : model1 loss : 0.126850 model2 loss : 0.123133
iteration 7810 : model1 loss : 0.137835 model2 loss : 0.106769
iteration 7811 : model1 loss : 0.152208 model2 loss : 0.083448
iteration 7812 : model1 loss : 0.116208 model2 loss : 0.114722
iteration 7813 : model1 loss : 0.123731 model2 loss : 0.111588
iteration 7814 : model1 loss : 0.111555 model2 loss : 0.073891
iteration 7815 : model1 loss : 0.153186 model2 loss : 0.104102
iteration 7816 : model1 loss : 0.124334 model2 loss : 0.093798
iteration 7817 : model1 loss : 0.129317 model2 loss : 0.082843
iteration 7818 : model1 loss : 0.137535 model2 loss : 0.103939
iteration 7819 : model1 loss : 0.116192 model2 loss : 0.094957
iteration 7820 : model1 loss : 0.129121 model2 loss : 0.111407
 78%|██████████████████████▋      | 460/589 [3:07:41<58:08, 27.04s/it]iteration 7821 : model1 loss : 0.101363 model2 loss : 0.085007
iteration 7822 : model1 loss : 0.112252 model2 loss : 0.068042
iteration 7823 : model1 loss : 0.131292 model2 loss : 0.090564
iteration 7824 : model1 loss : 0.135959 model2 loss : 0.086773
iteration 7825 : model1 loss : 0.133446 model2 loss : 0.091402
iteration 7826 : model1 loss : 0.128743 model2 loss : 0.093970
iteration 7827 : model1 loss : 0.144400 model2 loss : 0.089449
iteration 7828 : model1 loss : 0.159843 model2 loss : 0.112161
iteration 7829 : model1 loss : 0.112665 model2 loss : 0.071882
iteration 7830 : model1 loss : 0.099205 model2 loss : 0.089462
iteration 7831 : model1 loss : 0.122841 model2 loss : 0.084002
iteration 7832 : model1 loss : 0.150171 model2 loss : 0.104958
iteration 7833 : model1 loss : 0.171921 model2 loss : 0.102826
iteration 7834 : model1 loss : 0.125119 model2 loss : 0.096740
iteration 7835 : model1 loss : 0.098048 model2 loss : 0.106347
iteration 7836 : model1 loss : 0.134073 model2 loss : 0.123831
iteration 7837 : model1 loss : 0.115708 model2 loss : 0.082590
 78%|██████████████████████▋      | 461/589 [3:08:04<54:57, 25.76s/it]iteration 7838 : model1 loss : 0.168898 model2 loss : 0.103179
iteration 7839 : model1 loss : 0.134395 model2 loss : 0.089840
iteration 7840 : model1 loss : 0.118575 model2 loss : 0.090754
iteration 7841 : model1 loss : 0.080214 model2 loss : 0.058519
iteration 7842 : model1 loss : 0.120488 model2 loss : 0.096529
iteration 7843 : model1 loss : 0.116130 model2 loss : 0.078884
iteration 7844 : model1 loss : 0.117543 model2 loss : 0.088345
iteration 7845 : model1 loss : 0.140155 model2 loss : 0.113120
iteration 7846 : model1 loss : 0.119467 model2 loss : 0.093266
iteration 7847 : model1 loss : 0.127835 model2 loss : 0.083151
iteration 7848 : model1 loss : 0.151101 model2 loss : 0.076091
iteration 7849 : model1 loss : 0.108808 model2 loss : 0.079112
iteration 7850 : model1 loss : 0.133567 model2 loss : 0.076182
iteration 7851 : model1 loss : 0.138356 model2 loss : 0.085343
iteration 7852 : model1 loss : 0.116562 model2 loss : 0.086627
iteration 7853 : model1 loss : 0.138328 model2 loss : 0.108313
iteration 7854 : model1 loss : 0.122639 model2 loss : 0.105729
 78%|██████████████████████▋      | 462/589 [3:08:27<52:40, 24.89s/it]iteration 7855 : model1 loss : 0.110518 model2 loss : 0.073979
iteration 7856 : model1 loss : 0.134943 model2 loss : 0.092232
iteration 7857 : model1 loss : 0.117363 model2 loss : 0.090636
iteration 7858 : model1 loss : 0.112699 model2 loss : 0.107319
iteration 7859 : model1 loss : 0.087430 model2 loss : 0.067025
iteration 7860 : model1 loss : 0.195382 model2 loss : 0.115502
iteration 7861 : model1 loss : 0.115684 model2 loss : 0.081319
iteration 7862 : model1 loss : 0.111162 model2 loss : 0.076806
iteration 7863 : model1 loss : 0.116641 model2 loss : 0.072325
iteration 7864 : model1 loss : 0.150227 model2 loss : 0.075936
iteration 7865 : model1 loss : 0.164582 model2 loss : 0.085344
iteration 7866 : model1 loss : 0.181454 model2 loss : 0.114235
iteration 7867 : model1 loss : 0.156849 model2 loss : 0.099987
iteration 7868 : model1 loss : 0.132989 model2 loss : 0.122563
iteration 7869 : model1 loss : 0.118744 model2 loss : 0.078859
iteration 7870 : model1 loss : 0.116798 model2 loss : 0.102623
iteration 7871 : model1 loss : 0.121491 model2 loss : 0.083709
 79%|██████████████████████▊      | 463/589 [3:08:53<53:09, 25.31s/it]iteration 7872 : model1 loss : 0.140480 model2 loss : 0.090740
iteration 7873 : model1 loss : 0.141830 model2 loss : 0.096683
iteration 7874 : model1 loss : 0.105509 model2 loss : 0.098004
iteration 7875 : model1 loss : 0.094506 model2 loss : 0.092065
iteration 7876 : model1 loss : 0.106376 model2 loss : 0.144097
iteration 7877 : model1 loss : 0.119905 model2 loss : 0.104698
iteration 7878 : model1 loss : 0.138788 model2 loss : 0.106035
iteration 7879 : model1 loss : 0.171422 model2 loss : 0.152133
iteration 7880 : model1 loss : 0.134057 model2 loss : 0.092373
iteration 7881 : model1 loss : 0.083474 model2 loss : 0.077534
iteration 7882 : model1 loss : 0.095873 model2 loss : 0.063804
iteration 7883 : model1 loss : 0.139454 model2 loss : 0.090272
iteration 7884 : model1 loss : 0.151398 model2 loss : 0.117154
iteration 7885 : model1 loss : 0.136932 model2 loss : 0.081837
iteration 7886 : model1 loss : 0.127930 model2 loss : 0.103047
iteration 7887 : model1 loss : 0.115110 model2 loss : 0.095951
iteration 7888 : model1 loss : 0.152966 model2 loss : 0.111020
 79%|██████████████████████▊      | 464/589 [3:09:19<53:00, 25.45s/it]iteration 7889 : model1 loss : 0.119446 model2 loss : 0.120874
iteration 7890 : model1 loss : 0.135107 model2 loss : 0.094151
iteration 7891 : model1 loss : 0.135949 model2 loss : 0.083315
iteration 7892 : model1 loss : 0.105711 model2 loss : 0.101363
iteration 7893 : model1 loss : 0.105871 model2 loss : 0.084907
iteration 7894 : model1 loss : 0.107859 model2 loss : 0.095666
iteration 7895 : model1 loss : 0.106980 model2 loss : 0.072595
iteration 7896 : model1 loss : 0.142172 model2 loss : 0.069281
iteration 7897 : model1 loss : 0.115480 model2 loss : 0.084428
iteration 7898 : model1 loss : 0.131210 model2 loss : 0.124575
iteration 7899 : model1 loss : 0.119353 model2 loss : 0.107226
iteration 7900 : model1 loss : 0.191382 model2 loss : 0.131483
iteration 7901 : model1 loss : 0.121282 model2 loss : 0.080188
iteration 7902 : model1 loss : 0.137123 model2 loss : 0.089101
iteration 7903 : model1 loss : 0.141473 model2 loss : 0.123256
iteration 7904 : model1 loss : 0.137079 model2 loss : 0.117132
iteration 7905 : model1 loss : 0.119348 model2 loss : 0.110731
 79%|██████████████████████▉      | 465/589 [3:09:47<54:39, 26.45s/it]iteration 7906 : model1 loss : 0.161541 model2 loss : 0.101036
iteration 7907 : model1 loss : 0.111273 model2 loss : 0.100119
iteration 7908 : model1 loss : 0.122598 model2 loss : 0.089106
iteration 7909 : model1 loss : 0.149965 model2 loss : 0.080273
iteration 7910 : model1 loss : 0.100485 model2 loss : 0.087629
iteration 7911 : model1 loss : 0.091202 model2 loss : 0.077233
iteration 7912 : model1 loss : 0.120230 model2 loss : 0.084902
iteration 7913 : model1 loss : 0.113010 model2 loss : 0.085545
iteration 7914 : model1 loss : 0.116502 model2 loss : 0.075882
iteration 7915 : model1 loss : 0.181268 model2 loss : 0.092349
iteration 7916 : model1 loss : 0.127970 model2 loss : 0.087924
iteration 7917 : model1 loss : 0.112309 model2 loss : 0.094477
iteration 7918 : model1 loss : 0.128293 model2 loss : 0.099657
iteration 7919 : model1 loss : 0.140896 model2 loss : 0.134637
iteration 7920 : model1 loss : 0.152538 model2 loss : 0.100721
iteration 7921 : model1 loss : 0.136641 model2 loss : 0.078434
iteration 7922 : model1 loss : 0.137375 model2 loss : 0.110662
 79%|██████████████████████▉      | 466/589 [3:10:10<51:59, 25.36s/it]iteration 7923 : model1 loss : 0.117239 model2 loss : 0.104767
iteration 7924 : model1 loss : 0.166678 model2 loss : 0.105794
iteration 7925 : model1 loss : 0.104183 model2 loss : 0.077244
iteration 7926 : model1 loss : 0.153327 model2 loss : 0.105977
iteration 7927 : model1 loss : 0.133199 model2 loss : 0.107472
iteration 7928 : model1 loss : 0.111515 model2 loss : 0.077444
iteration 7929 : model1 loss : 0.114144 model2 loss : 0.071215
iteration 7930 : model1 loss : 0.198521 model2 loss : 0.091167
iteration 7931 : model1 loss : 0.131245 model2 loss : 0.091407
iteration 7932 : model1 loss : 0.097449 model2 loss : 0.069747
iteration 7933 : model1 loss : 0.127480 model2 loss : 0.150268
iteration 7934 : model1 loss : 0.118233 model2 loss : 0.079473
iteration 7935 : model1 loss : 0.101511 model2 loss : 0.093911
iteration 7936 : model1 loss : 0.204026 model2 loss : 0.101966
iteration 7937 : model1 loss : 0.115243 model2 loss : 0.124026
iteration 7938 : model1 loss : 0.140727 model2 loss : 0.080794
iteration 7939 : model1 loss : 0.154216 model2 loss : 0.097264
 79%|██████████████████████▉      | 467/589 [3:10:33<50:02, 24.61s/it]iteration 7940 : model1 loss : 0.116163 model2 loss : 0.094999
iteration 7941 : model1 loss : 0.098741 model2 loss : 0.090396
iteration 7942 : model1 loss : 0.151102 model2 loss : 0.109972
iteration 7943 : model1 loss : 0.156623 model2 loss : 0.101132
iteration 7944 : model1 loss : 0.116440 model2 loss : 0.097158
iteration 7945 : model1 loss : 0.103954 model2 loss : 0.081248
iteration 7946 : model1 loss : 0.187702 model2 loss : 0.102626
iteration 7947 : model1 loss : 0.119800 model2 loss : 0.086954
iteration 7948 : model1 loss : 0.126653 model2 loss : 0.105116
iteration 7949 : model1 loss : 0.131580 model2 loss : 0.103221
iteration 7950 : model1 loss : 0.105658 model2 loss : 0.074296
iteration 7951 : model1 loss : 0.124336 model2 loss : 0.061681
iteration 7952 : model1 loss : 0.145381 model2 loss : 0.091744
iteration 7953 : model1 loss : 0.126971 model2 loss : 0.098527
iteration 7954 : model1 loss : 0.107027 model2 loss : 0.105005
iteration 7955 : model1 loss : 0.122852 model2 loss : 0.087805
iteration 7956 : model1 loss : 0.123498 model2 loss : 0.079104
 79%|███████████████████████      | 468/589 [3:10:56<48:36, 24.10s/it]iteration 7957 : model1 loss : 0.144614 model2 loss : 0.113871
iteration 7958 : model1 loss : 0.132759 model2 loss : 0.106558
iteration 7959 : model1 loss : 0.140288 model2 loss : 0.082700
iteration 7960 : model1 loss : 0.119302 model2 loss : 0.073566
iteration 7961 : model1 loss : 0.142306 model2 loss : 0.113921
iteration 7962 : model1 loss : 0.155371 model2 loss : 0.095548
iteration 7963 : model1 loss : 0.126594 model2 loss : 0.102824
iteration 7964 : model1 loss : 0.099173 model2 loss : 0.062930
iteration 7965 : model1 loss : 0.114123 model2 loss : 0.080477
iteration 7966 : model1 loss : 0.186548 model2 loss : 0.098837
iteration 7967 : model1 loss : 0.171917 model2 loss : 0.097079
iteration 7968 : model1 loss : 0.114141 model2 loss : 0.082468
iteration 7969 : model1 loss : 0.125406 model2 loss : 0.107008
iteration 7970 : model1 loss : 0.122306 model2 loss : 0.072161
iteration 7971 : model1 loss : 0.112224 model2 loss : 0.085987
iteration 7972 : model1 loss : 0.101927 model2 loss : 0.088828
iteration 7973 : model1 loss : 0.131486 model2 loss : 0.097534
 80%|███████████████████████      | 469/589 [3:11:19<47:24, 23.70s/it]iteration 7974 : model1 loss : 0.132282 model2 loss : 0.084287
iteration 7975 : model1 loss : 0.150207 model2 loss : 0.080697
iteration 7976 : model1 loss : 0.116208 model2 loss : 0.084824
iteration 7977 : model1 loss : 0.110620 model2 loss : 0.054769
iteration 7978 : model1 loss : 0.080713 model2 loss : 0.070214
iteration 7979 : model1 loss : 0.110319 model2 loss : 0.103160
iteration 7980 : model1 loss : 0.111212 model2 loss : 0.092468
iteration 7981 : model1 loss : 0.122096 model2 loss : 0.082223
iteration 7982 : model1 loss : 0.105165 model2 loss : 0.081660
iteration 7983 : model1 loss : 0.146509 model2 loss : 0.107806
iteration 7984 : model1 loss : 0.127299 model2 loss : 0.116250
iteration 7985 : model1 loss : 0.114109 model2 loss : 0.066010
iteration 7986 : model1 loss : 0.139564 model2 loss : 0.103018
iteration 7987 : model1 loss : 0.176497 model2 loss : 0.110306
iteration 7988 : model1 loss : 0.132499 model2 loss : 0.089610
iteration 7989 : model1 loss : 0.124128 model2 loss : 0.077308
iteration 7990 : model1 loss : 0.178207 model2 loss : 0.092589
 80%|███████████████████████▏     | 470/589 [3:11:42<46:26, 23.42s/it]iteration 7991 : model1 loss : 0.103285 model2 loss : 0.071507
iteration 7992 : model1 loss : 0.086529 model2 loss : 0.074107
iteration 7993 : model1 loss : 0.118566 model2 loss : 0.125211
iteration 7994 : model1 loss : 0.118119 model2 loss : 0.078198
iteration 7995 : model1 loss : 0.108184 model2 loss : 0.085361
iteration 7996 : model1 loss : 0.106851 model2 loss : 0.097809
iteration 7997 : model1 loss : 0.141556 model2 loss : 0.127317
iteration 7998 : model1 loss : 0.128463 model2 loss : 0.121241
iteration 7999 : model1 loss : 0.130440 model2 loss : 0.092190
iteration 8000 : model1 loss : 0.169825 model2 loss : 0.119384
iteration 8000 : model1_mean_dice : 0.699124 model1_mean_hd95 : 88.191254 model1_mean_iou : 0.572768
iteration 8000 : model2_mean_dice : 0.759237 model2_mean_hd95 : 69.359000 model2_mean_iou : 0.646831
iteration 8001 : model1 loss : 0.125232 model2 loss : 0.082235
iteration 8002 : model1 loss : 0.101836 model2 loss : 0.096089
iteration 8003 : model1 loss : 0.175378 model2 loss : 0.119739
iteration 8004 : model1 loss : 0.121119 model2 loss : 0.101408
iteration 8005 : model1 loss : 0.132591 model2 loss : 0.094520
iteration 8006 : model1 loss : 0.171966 model2 loss : 0.130568
iteration 8007 : model1 loss : 0.118074 model2 loss : 0.077488
 80%|███████████████████████▏     | 471/589 [3:12:24<57:23, 29.18s/it]iteration 8008 : model1 loss : 0.150392 model2 loss : 0.160342
iteration 8009 : model1 loss : 0.126290 model2 loss : 0.079419
iteration 8010 : model1 loss : 0.102491 model2 loss : 0.120089
iteration 8011 : model1 loss : 0.158466 model2 loss : 0.101453
iteration 8012 : model1 loss : 0.121492 model2 loss : 0.182772
iteration 8013 : model1 loss : 0.103135 model2 loss : 0.085936
iteration 8014 : model1 loss : 0.132363 model2 loss : 0.157823
iteration 8015 : model1 loss : 0.117634 model2 loss : 0.073068
iteration 8016 : model1 loss : 0.125298 model2 loss : 0.141980
iteration 8017 : model1 loss : 0.120279 model2 loss : 0.100917
iteration 8018 : model1 loss : 0.129959 model2 loss : 0.095934
iteration 8019 : model1 loss : 0.142848 model2 loss : 0.079632
iteration 8020 : model1 loss : 0.144221 model2 loss : 0.115350
iteration 8021 : model1 loss : 0.127334 model2 loss : 0.087003
iteration 8022 : model1 loss : 0.125294 model2 loss : 0.102935
iteration 8023 : model1 loss : 0.101612 model2 loss : 0.093740
iteration 8024 : model1 loss : 0.111411 model2 loss : 0.095838
 80%|███████████████████████▏     | 472/589 [3:12:47<53:07, 27.25s/it]iteration 8025 : model1 loss : 0.136625 model2 loss : 0.100014
iteration 8026 : model1 loss : 0.122694 model2 loss : 0.103409
iteration 8027 : model1 loss : 0.154094 model2 loss : 0.100366
iteration 8028 : model1 loss : 0.117871 model2 loss : 0.092072
iteration 8029 : model1 loss : 0.136967 model2 loss : 0.117240
iteration 8030 : model1 loss : 0.137397 model2 loss : 0.120571
iteration 8031 : model1 loss : 0.094818 model2 loss : 0.081293
iteration 8032 : model1 loss : 0.139785 model2 loss : 0.097864
iteration 8033 : model1 loss : 0.109961 model2 loss : 0.091535
iteration 8034 : model1 loss : 0.124259 model2 loss : 0.093822
iteration 8035 : model1 loss : 0.104700 model2 loss : 0.082122
iteration 8036 : model1 loss : 0.104719 model2 loss : 0.083056
iteration 8037 : model1 loss : 0.123823 model2 loss : 0.112906
iteration 8038 : model1 loss : 0.126781 model2 loss : 0.109254
iteration 8039 : model1 loss : 0.131988 model2 loss : 0.093128
iteration 8040 : model1 loss : 0.111616 model2 loss : 0.101413
iteration 8041 : model1 loss : 0.141721 model2 loss : 0.112448
 80%|███████████████████████▎     | 473/589 [3:13:10<50:04, 25.90s/it]iteration 8042 : model1 loss : 0.096081 model2 loss : 0.075554
iteration 8043 : model1 loss : 0.087769 model2 loss : 0.088613
iteration 8044 : model1 loss : 0.160454 model2 loss : 0.091684
iteration 8045 : model1 loss : 0.132865 model2 loss : 0.111963
iteration 8046 : model1 loss : 0.090666 model2 loss : 0.077031
iteration 8047 : model1 loss : 0.127374 model2 loss : 0.101540
iteration 8048 : model1 loss : 0.116760 model2 loss : 0.090826
iteration 8049 : model1 loss : 0.163665 model2 loss : 0.102723
iteration 8050 : model1 loss : 0.114574 model2 loss : 0.111919
iteration 8051 : model1 loss : 0.153328 model2 loss : 0.096991
iteration 8052 : model1 loss : 0.128389 model2 loss : 0.105546
iteration 8053 : model1 loss : 0.121136 model2 loss : 0.096835
iteration 8054 : model1 loss : 0.115062 model2 loss : 0.109274
iteration 8055 : model1 loss : 0.126643 model2 loss : 0.087666
iteration 8056 : model1 loss : 0.154420 model2 loss : 0.111629
iteration 8057 : model1 loss : 0.108031 model2 loss : 0.104970
iteration 8058 : model1 loss : 0.150809 model2 loss : 0.100134
 80%|███████████████████████▎     | 474/589 [3:13:33<47:54, 25.00s/it]iteration 8059 : model1 loss : 0.125722 model2 loss : 0.090256
iteration 8060 : model1 loss : 0.139263 model2 loss : 0.107121
iteration 8061 : model1 loss : 0.109674 model2 loss : 0.084202
iteration 8062 : model1 loss : 0.090213 model2 loss : 0.104911
iteration 8063 : model1 loss : 0.133908 model2 loss : 0.120684
iteration 8064 : model1 loss : 0.112441 model2 loss : 0.066864
iteration 8065 : model1 loss : 0.131547 model2 loss : 0.096385
iteration 8066 : model1 loss : 0.119765 model2 loss : 0.084557
iteration 8067 : model1 loss : 0.110242 model2 loss : 0.083205
iteration 8068 : model1 loss : 0.132619 model2 loss : 0.074739
iteration 8069 : model1 loss : 0.175353 model2 loss : 0.102952
iteration 8070 : model1 loss : 0.155900 model2 loss : 0.107965
iteration 8071 : model1 loss : 0.164380 model2 loss : 0.092299
iteration 8072 : model1 loss : 0.103090 model2 loss : 0.091252
iteration 8073 : model1 loss : 0.116664 model2 loss : 0.079297
iteration 8074 : model1 loss : 0.109584 model2 loss : 0.071406
iteration 8075 : model1 loss : 0.114686 model2 loss : 0.097294
 81%|███████████████████████▍     | 475/589 [3:13:55<46:13, 24.33s/it]iteration 8076 : model1 loss : 0.121013 model2 loss : 0.072373
iteration 8077 : model1 loss : 0.110797 model2 loss : 0.086654
iteration 8078 : model1 loss : 0.102683 model2 loss : 0.096945
iteration 8079 : model1 loss : 0.126469 model2 loss : 0.071202
iteration 8080 : model1 loss : 0.102864 model2 loss : 0.087524
iteration 8081 : model1 loss : 0.139480 model2 loss : 0.134785
iteration 8082 : model1 loss : 0.142715 model2 loss : 0.093074
iteration 8083 : model1 loss : 0.107490 model2 loss : 0.087680
iteration 8084 : model1 loss : 0.145192 model2 loss : 0.098918
iteration 8085 : model1 loss : 0.124204 model2 loss : 0.097911
iteration 8086 : model1 loss : 0.137708 model2 loss : 0.093904
iteration 8087 : model1 loss : 0.140040 model2 loss : 0.136041
iteration 8088 : model1 loss : 0.157293 model2 loss : 0.112880
iteration 8089 : model1 loss : 0.110965 model2 loss : 0.103288
iteration 8090 : model1 loss : 0.137079 model2 loss : 0.092286
iteration 8091 : model1 loss : 0.140348 model2 loss : 0.108699
iteration 8092 : model1 loss : 0.137129 model2 loss : 0.088932
 81%|███████████████████████▍     | 476/589 [3:14:18<44:57, 23.87s/it]iteration 8093 : model1 loss : 0.103892 model2 loss : 0.079343
iteration 8094 : model1 loss : 0.116181 model2 loss : 0.067811
iteration 8095 : model1 loss : 0.134345 model2 loss : 0.151748
iteration 8096 : model1 loss : 0.114406 model2 loss : 0.076527
iteration 8097 : model1 loss : 0.140702 model2 loss : 0.098249
iteration 8098 : model1 loss : 0.146343 model2 loss : 0.109390
iteration 8099 : model1 loss : 0.124273 model2 loss : 0.075528
iteration 8100 : model1 loss : 0.150097 model2 loss : 0.119352
iteration 8101 : model1 loss : 0.112670 model2 loss : 0.075111
iteration 8102 : model1 loss : 0.113992 model2 loss : 0.076032
iteration 8103 : model1 loss : 0.136833 model2 loss : 0.104287
iteration 8104 : model1 loss : 0.130368 model2 loss : 0.098297
iteration 8105 : model1 loss : 0.117544 model2 loss : 0.100333
iteration 8106 : model1 loss : 0.138172 model2 loss : 0.102539
iteration 8107 : model1 loss : 0.128130 model2 loss : 0.088044
iteration 8108 : model1 loss : 0.096329 model2 loss : 0.077475
iteration 8109 : model1 loss : 0.125531 model2 loss : 0.076735
 81%|███████████████████████▍     | 477/589 [3:14:41<43:58, 23.56s/it]iteration 8110 : model1 loss : 0.124866 model2 loss : 0.089300
iteration 8111 : model1 loss : 0.122230 model2 loss : 0.104032
iteration 8112 : model1 loss : 0.109145 model2 loss : 0.112213
iteration 8113 : model1 loss : 0.127350 model2 loss : 0.100894
iteration 8114 : model1 loss : 0.117606 model2 loss : 0.090692
iteration 8115 : model1 loss : 0.114870 model2 loss : 0.107897
iteration 8116 : model1 loss : 0.106450 model2 loss : 0.091279
iteration 8117 : model1 loss : 0.151723 model2 loss : 0.101515
iteration 8118 : model1 loss : 0.123679 model2 loss : 0.078082
iteration 8119 : model1 loss : 0.144416 model2 loss : 0.065861
iteration 8120 : model1 loss : 0.169871 model2 loss : 0.100971
iteration 8121 : model1 loss : 0.139050 model2 loss : 0.095310
iteration 8122 : model1 loss : 0.125589 model2 loss : 0.115456
iteration 8123 : model1 loss : 0.131345 model2 loss : 0.072729
iteration 8124 : model1 loss : 0.093723 model2 loss : 0.110285
iteration 8125 : model1 loss : 0.111319 model2 loss : 0.086650
iteration 8126 : model1 loss : 0.099467 model2 loss : 0.090351
 81%|███████████████████████▌     | 478/589 [3:15:04<43:09, 23.33s/it]iteration 8127 : model1 loss : 0.108298 model2 loss : 0.122083
iteration 8128 : model1 loss : 0.128946 model2 loss : 0.090439
iteration 8129 : model1 loss : 0.178497 model2 loss : 0.083508
iteration 8130 : model1 loss : 0.112261 model2 loss : 0.073141
iteration 8131 : model1 loss : 0.114519 model2 loss : 0.087812
iteration 8132 : model1 loss : 0.133733 model2 loss : 0.096480
iteration 8133 : model1 loss : 0.099594 model2 loss : 0.074302
iteration 8134 : model1 loss : 0.111866 model2 loss : 0.108140
iteration 8135 : model1 loss : 0.114777 model2 loss : 0.089232
iteration 8136 : model1 loss : 0.189934 model2 loss : 0.141630
iteration 8137 : model1 loss : 0.139131 model2 loss : 0.096086
iteration 8138 : model1 loss : 0.116872 model2 loss : 0.085678
iteration 8139 : model1 loss : 0.129000 model2 loss : 0.094728
iteration 8140 : model1 loss : 0.122581 model2 loss : 0.096367
iteration 8141 : model1 loss : 0.119251 model2 loss : 0.083921
iteration 8142 : model1 loss : 0.152748 model2 loss : 0.124656
iteration 8143 : model1 loss : 0.096115 model2 loss : 0.060572
 81%|███████████████████████▌     | 479/589 [3:15:27<42:28, 23.17s/it]iteration 8144 : model1 loss : 0.158703 model2 loss : 0.124887
iteration 8145 : model1 loss : 0.149273 model2 loss : 0.085492
iteration 8146 : model1 loss : 0.176309 model2 loss : 0.120253
iteration 8147 : model1 loss : 0.098800 model2 loss : 0.079711
iteration 8148 : model1 loss : 0.099716 model2 loss : 0.081751
iteration 8149 : model1 loss : 0.114077 model2 loss : 0.104903
iteration 8150 : model1 loss : 0.115143 model2 loss : 0.109203
iteration 8151 : model1 loss : 0.097079 model2 loss : 0.106675
iteration 8152 : model1 loss : 0.126100 model2 loss : 0.085645
iteration 8153 : model1 loss : 0.140853 model2 loss : 0.095946
iteration 8154 : model1 loss : 0.137557 model2 loss : 0.089277
iteration 8155 : model1 loss : 0.110216 model2 loss : 0.090592
iteration 8156 : model1 loss : 0.167253 model2 loss : 0.122168
iteration 8157 : model1 loss : 0.153714 model2 loss : 0.106681
iteration 8158 : model1 loss : 0.109108 model2 loss : 0.091992
iteration 8159 : model1 loss : 0.121739 model2 loss : 0.094266
iteration 8160 : model1 loss : 0.090524 model2 loss : 0.069674
 81%|███████████████████████▋     | 480/589 [3:15:49<41:55, 23.08s/it]iteration 8161 : model1 loss : 0.125935 model2 loss : 0.091422
iteration 8162 : model1 loss : 0.126851 model2 loss : 0.074922
iteration 8163 : model1 loss : 0.145671 model2 loss : 0.092421
iteration 8164 : model1 loss : 0.105461 model2 loss : 0.084272
iteration 8165 : model1 loss : 0.126017 model2 loss : 0.083951
iteration 8166 : model1 loss : 0.145392 model2 loss : 0.090430
iteration 8167 : model1 loss : 0.158347 model2 loss : 0.104124
iteration 8168 : model1 loss : 0.129082 model2 loss : 0.071480
iteration 8169 : model1 loss : 0.115668 model2 loss : 0.111175
iteration 8170 : model1 loss : 0.110951 model2 loss : 0.079192
iteration 8171 : model1 loss : 0.170332 model2 loss : 0.107300
iteration 8172 : model1 loss : 0.106317 model2 loss : 0.102780
iteration 8173 : model1 loss : 0.148569 model2 loss : 0.091992
iteration 8174 : model1 loss : 0.121901 model2 loss : 0.077409
iteration 8175 : model1 loss : 0.148989 model2 loss : 0.119341
iteration 8176 : model1 loss : 0.080151 model2 loss : 0.076897
iteration 8177 : model1 loss : 0.129626 model2 loss : 0.097259
 82%|███████████████████████▋     | 481/589 [3:16:12<41:24, 23.00s/it]iteration 8178 : model1 loss : 0.119260 model2 loss : 0.080136
iteration 8179 : model1 loss : 0.168714 model2 loss : 0.112694
iteration 8180 : model1 loss : 0.103323 model2 loss : 0.097582
iteration 8181 : model1 loss : 0.141774 model2 loss : 0.093053
iteration 8182 : model1 loss : 0.086395 model2 loss : 0.099467
iteration 8183 : model1 loss : 0.115223 model2 loss : 0.099988
iteration 8184 : model1 loss : 0.108417 model2 loss : 0.093415
iteration 8185 : model1 loss : 0.118799 model2 loss : 0.093620
iteration 8186 : model1 loss : 0.140532 model2 loss : 0.119170
iteration 8187 : model1 loss : 0.105166 model2 loss : 0.091805
iteration 8188 : model1 loss : 0.146123 model2 loss : 0.096367
iteration 8189 : model1 loss : 0.118560 model2 loss : 0.094484
iteration 8190 : model1 loss : 0.127228 model2 loss : 0.101198
iteration 8191 : model1 loss : 0.152807 model2 loss : 0.082636
iteration 8192 : model1 loss : 0.103188 model2 loss : 0.075079
iteration 8193 : model1 loss : 0.124109 model2 loss : 0.128893
iteration 8194 : model1 loss : 0.126193 model2 loss : 0.122016
 82%|███████████████████████▋     | 482/589 [3:16:35<40:54, 22.94s/it]iteration 8195 : model1 loss : 0.104583 model2 loss : 0.083549
iteration 8196 : model1 loss : 0.134685 model2 loss : 0.098992
iteration 8197 : model1 loss : 0.124106 model2 loss : 0.089591
iteration 8198 : model1 loss : 0.160732 model2 loss : 0.120356
iteration 8199 : model1 loss : 0.141972 model2 loss : 0.101782
iteration 8200 : model1 loss : 0.122642 model2 loss : 0.082968
iteration 8200 : model1_mean_dice : 0.690750 model1_mean_hd95 : 88.679778 model1_mean_iou : 0.564846
iteration 8200 : model2_mean_dice : 0.743966 model2_mean_hd95 : 71.251860 model2_mean_iou : 0.633081
iteration 8201 : model1 loss : 0.118971 model2 loss : 0.103651
iteration 8202 : model1 loss : 0.158588 model2 loss : 0.113695
iteration 8203 : model1 loss : 0.104507 model2 loss : 0.094884
iteration 8204 : model1 loss : 0.115948 model2 loss : 0.096922
iteration 8205 : model1 loss : 0.163258 model2 loss : 0.082727
iteration 8206 : model1 loss : 0.095738 model2 loss : 0.088104
iteration 8207 : model1 loss : 0.111999 model2 loss : 0.075827
iteration 8208 : model1 loss : 0.118233 model2 loss : 0.074138
iteration 8209 : model1 loss : 0.114315 model2 loss : 0.085652
iteration 8210 : model1 loss : 0.107443 model2 loss : 0.093737
iteration 8211 : model1 loss : 0.117984 model2 loss : 0.091043
 82%|███████████████████████▊     | 483/589 [3:17:18<50:58, 28.86s/it]iteration 8212 : model1 loss : 0.107355 model2 loss : 0.072281
iteration 8213 : model1 loss : 0.158312 model2 loss : 0.083238
iteration 8214 : model1 loss : 0.116625 model2 loss : 0.094308
iteration 8215 : model1 loss : 0.155527 model2 loss : 0.097737
iteration 8216 : model1 loss : 0.101130 model2 loss : 0.085697
iteration 8217 : model1 loss : 0.096800 model2 loss : 0.071194
iteration 8218 : model1 loss : 0.154989 model2 loss : 0.102179
iteration 8219 : model1 loss : 0.115890 model2 loss : 0.111564
iteration 8220 : model1 loss : 0.134205 model2 loss : 0.079368
iteration 8221 : model1 loss : 0.123708 model2 loss : 0.082304
iteration 8222 : model1 loss : 0.109885 model2 loss : 0.082521
iteration 8223 : model1 loss : 0.168123 model2 loss : 0.136116
iteration 8224 : model1 loss : 0.136333 model2 loss : 0.092367
iteration 8225 : model1 loss : 0.123045 model2 loss : 0.085233
iteration 8226 : model1 loss : 0.149669 model2 loss : 0.078832
iteration 8227 : model1 loss : 0.122687 model2 loss : 0.092968
iteration 8228 : model1 loss : 0.111661 model2 loss : 0.084711
 82%|███████████████████████▊     | 484/589 [3:17:40<47:14, 27.00s/it]iteration 8229 : model1 loss : 0.232948 model2 loss : 0.102443
iteration 8230 : model1 loss : 0.105039 model2 loss : 0.074522
iteration 8231 : model1 loss : 0.123993 model2 loss : 0.085063
iteration 8232 : model1 loss : 0.133987 model2 loss : 0.083481
iteration 8233 : model1 loss : 0.108672 model2 loss : 0.120079
iteration 8234 : model1 loss : 0.094761 model2 loss : 0.061188
iteration 8235 : model1 loss : 0.180338 model2 loss : 0.128026
iteration 8236 : model1 loss : 0.087319 model2 loss : 0.071829
iteration 8237 : model1 loss : 0.111056 model2 loss : 0.093112
iteration 8238 : model1 loss : 0.109574 model2 loss : 0.073803
iteration 8239 : model1 loss : 0.136715 model2 loss : 0.098864
iteration 8240 : model1 loss : 0.140160 model2 loss : 0.108898
iteration 8241 : model1 loss : 0.134165 model2 loss : 0.088917
iteration 8242 : model1 loss : 0.155516 model2 loss : 0.102355
iteration 8243 : model1 loss : 0.099496 model2 loss : 0.082203
iteration 8244 : model1 loss : 0.076237 model2 loss : 0.054610
iteration 8245 : model1 loss : 0.136200 model2 loss : 0.114227
 82%|███████████████████████▉     | 485/589 [3:18:03<44:36, 25.73s/it]iteration 8246 : model1 loss : 0.087983 model2 loss : 0.052409
iteration 8247 : model1 loss : 0.144411 model2 loss : 0.088954
iteration 8248 : model1 loss : 0.118350 model2 loss : 0.086559
iteration 8249 : model1 loss : 0.124295 model2 loss : 0.079720
iteration 8250 : model1 loss : 0.138038 model2 loss : 0.122146
iteration 8251 : model1 loss : 0.137019 model2 loss : 0.090506
iteration 8252 : model1 loss : 0.126342 model2 loss : 0.105417
iteration 8253 : model1 loss : 0.129591 model2 loss : 0.078593
iteration 8254 : model1 loss : 0.099256 model2 loss : 0.073491
iteration 8255 : model1 loss : 0.156638 model2 loss : 0.095841
iteration 8256 : model1 loss : 0.094073 model2 loss : 0.068862
iteration 8257 : model1 loss : 0.132371 model2 loss : 0.131091
iteration 8258 : model1 loss : 0.107371 model2 loss : 0.089889
iteration 8259 : model1 loss : 0.103439 model2 loss : 0.080231
iteration 8260 : model1 loss : 0.147452 model2 loss : 0.091657
iteration 8261 : model1 loss : 0.096756 model2 loss : 0.087298
iteration 8262 : model1 loss : 0.169120 model2 loss : 0.124348
 83%|███████████████████████▉     | 486/589 [3:18:26<42:41, 24.87s/it]iteration 8263 : model1 loss : 0.145688 model2 loss : 0.096765
iteration 8264 : model1 loss : 0.123276 model2 loss : 0.094072
iteration 8265 : model1 loss : 0.157779 model2 loss : 0.136083
iteration 8266 : model1 loss : 0.136245 model2 loss : 0.111656
iteration 8267 : model1 loss : 0.150159 model2 loss : 0.118326
iteration 8268 : model1 loss : 0.098401 model2 loss : 0.097763
iteration 8269 : model1 loss : 0.141506 model2 loss : 0.090819
iteration 8270 : model1 loss : 0.122636 model2 loss : 0.107280
iteration 8271 : model1 loss : 0.122047 model2 loss : 0.119230
iteration 8272 : model1 loss : 0.140159 model2 loss : 0.104840
iteration 8273 : model1 loss : 0.109828 model2 loss : 0.093209
iteration 8274 : model1 loss : 0.109466 model2 loss : 0.077836
iteration 8275 : model1 loss : 0.147844 model2 loss : 0.082404
iteration 8276 : model1 loss : 0.130195 model2 loss : 0.102896
iteration 8277 : model1 loss : 0.096609 model2 loss : 0.083676
iteration 8278 : model1 loss : 0.105113 model2 loss : 0.088908
iteration 8279 : model1 loss : 0.196229 model2 loss : 0.144431
 83%|███████████████████████▉     | 487/589 [3:18:49<41:13, 24.25s/it]iteration 8280 : model1 loss : 0.120605 model2 loss : 0.097886
iteration 8281 : model1 loss : 0.156790 model2 loss : 0.130963
iteration 8282 : model1 loss : 0.145156 model2 loss : 0.071615
iteration 8283 : model1 loss : 0.130104 model2 loss : 0.105525
iteration 8284 : model1 loss : 0.115487 model2 loss : 0.112024
iteration 8285 : model1 loss : 0.107753 model2 loss : 0.093964
iteration 8286 : model1 loss : 0.163008 model2 loss : 0.089941
iteration 8287 : model1 loss : 0.105775 model2 loss : 0.067457
iteration 8288 : model1 loss : 0.095481 model2 loss : 0.081137
iteration 8289 : model1 loss : 0.130333 model2 loss : 0.089416
iteration 8290 : model1 loss : 0.145658 model2 loss : 0.088258
iteration 8291 : model1 loss : 0.174033 model2 loss : 0.097925
iteration 8292 : model1 loss : 0.121040 model2 loss : 0.111409
iteration 8293 : model1 loss : 0.131884 model2 loss : 0.084263
iteration 8294 : model1 loss : 0.115084 model2 loss : 0.084082
iteration 8295 : model1 loss : 0.120926 model2 loss : 0.114429
iteration 8296 : model1 loss : 0.090757 model2 loss : 0.062965
 83%|████████████████████████     | 488/589 [3:19:12<40:05, 23.81s/it]iteration 8297 : model1 loss : 0.144405 model2 loss : 0.107003
iteration 8298 : model1 loss : 0.130784 model2 loss : 0.079835
iteration 8299 : model1 loss : 0.113556 model2 loss : 0.097105
iteration 8300 : model1 loss : 0.122454 model2 loss : 0.081100
iteration 8301 : model1 loss : 0.108134 model2 loss : 0.088762
iteration 8302 : model1 loss : 0.111228 model2 loss : 0.097080
iteration 8303 : model1 loss : 0.105312 model2 loss : 0.074803
iteration 8304 : model1 loss : 0.141331 model2 loss : 0.092935
iteration 8305 : model1 loss : 0.119465 model2 loss : 0.099982
iteration 8306 : model1 loss : 0.121383 model2 loss : 0.068841
iteration 8307 : model1 loss : 0.136997 model2 loss : 0.102136
iteration 8308 : model1 loss : 0.180439 model2 loss : 0.096182
iteration 8309 : model1 loss : 0.165223 model2 loss : 0.134185
iteration 8310 : model1 loss : 0.127921 model2 loss : 0.093754
iteration 8311 : model1 loss : 0.088410 model2 loss : 0.086444
iteration 8312 : model1 loss : 0.132391 model2 loss : 0.097135
iteration 8313 : model1 loss : 0.130929 model2 loss : 0.125968
 83%|████████████████████████     | 489/589 [3:19:35<39:13, 23.54s/it]iteration 8314 : model1 loss : 0.130204 model2 loss : 0.098719
iteration 8315 : model1 loss : 0.159737 model2 loss : 0.119098
iteration 8316 : model1 loss : 0.157512 model2 loss : 0.091710
iteration 8317 : model1 loss : 0.107431 model2 loss : 0.072148
iteration 8318 : model1 loss : 0.108160 model2 loss : 0.086318
iteration 8319 : model1 loss : 0.122693 model2 loss : 0.076507
iteration 8320 : model1 loss : 0.175446 model2 loss : 0.111309
iteration 8321 : model1 loss : 0.106877 model2 loss : 0.082459
iteration 8322 : model1 loss : 0.117758 model2 loss : 0.072063
iteration 8323 : model1 loss : 0.138203 model2 loss : 0.114762
iteration 8324 : model1 loss : 0.138254 model2 loss : 0.105621
iteration 8325 : model1 loss : 0.163333 model2 loss : 0.118594
iteration 8326 : model1 loss : 0.137715 model2 loss : 0.088112
iteration 8327 : model1 loss : 0.128716 model2 loss : 0.070803
iteration 8328 : model1 loss : 0.109097 model2 loss : 0.086686
iteration 8329 : model1 loss : 0.112527 model2 loss : 0.090145
iteration 8330 : model1 loss : 0.104053 model2 loss : 0.075407
 83%|████████████████████████▏    | 490/589 [3:19:57<38:27, 23.31s/it]iteration 8331 : model1 loss : 0.133839 model2 loss : 0.106946
iteration 8332 : model1 loss : 0.175185 model2 loss : 0.101143
iteration 8333 : model1 loss : 0.141516 model2 loss : 0.085127
iteration 8334 : model1 loss : 0.119896 model2 loss : 0.092868
iteration 8335 : model1 loss : 0.109616 model2 loss : 0.088254
iteration 8336 : model1 loss : 0.126849 model2 loss : 0.092157
iteration 8337 : model1 loss : 0.140101 model2 loss : 0.099578
iteration 8338 : model1 loss : 0.103921 model2 loss : 0.104787
iteration 8339 : model1 loss : 0.113995 model2 loss : 0.071707
iteration 8340 : model1 loss : 0.131599 model2 loss : 0.107308
iteration 8341 : model1 loss : 0.146054 model2 loss : 0.087290
iteration 8342 : model1 loss : 0.132535 model2 loss : 0.107429
iteration 8343 : model1 loss : 0.122428 model2 loss : 0.112683
iteration 8344 : model1 loss : 0.128137 model2 loss : 0.074496
iteration 8345 : model1 loss : 0.104184 model2 loss : 0.084380
iteration 8346 : model1 loss : 0.110911 model2 loss : 0.075766
iteration 8347 : model1 loss : 0.098384 model2 loss : 0.080661
 83%|████████████████████████▏    | 491/589 [3:20:20<37:48, 23.15s/it]iteration 8348 : model1 loss : 0.178843 model2 loss : 0.070312
iteration 8349 : model1 loss : 0.165838 model2 loss : 0.121992
iteration 8350 : model1 loss : 0.122759 model2 loss : 0.070586
iteration 8351 : model1 loss : 0.108563 model2 loss : 0.066907
iteration 8352 : model1 loss : 0.108720 model2 loss : 0.111378
iteration 8353 : model1 loss : 0.132872 model2 loss : 0.087992
iteration 8354 : model1 loss : 0.154903 model2 loss : 0.101505
iteration 8355 : model1 loss : 0.145163 model2 loss : 0.094218
iteration 8356 : model1 loss : 0.134254 model2 loss : 0.105885
iteration 8357 : model1 loss : 0.095794 model2 loss : 0.071238
iteration 8358 : model1 loss : 0.147123 model2 loss : 0.088760
iteration 8359 : model1 loss : 0.162681 model2 loss : 0.093512
iteration 8360 : model1 loss : 0.141445 model2 loss : 0.102347
iteration 8361 : model1 loss : 0.113153 model2 loss : 0.087018
iteration 8362 : model1 loss : 0.122002 model2 loss : 0.093339
iteration 8363 : model1 loss : 0.135832 model2 loss : 0.096161
iteration 8364 : model1 loss : 0.092368 model2 loss : 0.068524
 84%|████████████████████████▏    | 492/589 [3:20:43<37:16, 23.06s/it]iteration 8365 : model1 loss : 0.118740 model2 loss : 0.080526
iteration 8366 : model1 loss : 0.175239 model2 loss : 0.109494
iteration 8367 : model1 loss : 0.112633 model2 loss : 0.090320
iteration 8368 : model1 loss : 0.142790 model2 loss : 0.086528
iteration 8369 : model1 loss : 0.096406 model2 loss : 0.097665
iteration 8370 : model1 loss : 0.131516 model2 loss : 0.094520
iteration 8371 : model1 loss : 0.118422 model2 loss : 0.106493
iteration 8372 : model1 loss : 0.107144 model2 loss : 0.090514
iteration 8373 : model1 loss : 0.159101 model2 loss : 0.152364
iteration 8374 : model1 loss : 0.128038 model2 loss : 0.074477
iteration 8375 : model1 loss : 0.123256 model2 loss : 0.082026
iteration 8376 : model1 loss : 0.174548 model2 loss : 0.106149
iteration 8377 : model1 loss : 0.121865 model2 loss : 0.084844
iteration 8378 : model1 loss : 0.133254 model2 loss : 0.094986
iteration 8379 : model1 loss : 0.148617 model2 loss : 0.074733
iteration 8380 : model1 loss : 0.114205 model2 loss : 0.091583
iteration 8381 : model1 loss : 0.133180 model2 loss : 0.098343
 84%|████████████████████████▎    | 493/589 [3:21:06<36:47, 22.99s/it]iteration 8382 : model1 loss : 0.138706 model2 loss : 0.104540
iteration 8383 : model1 loss : 0.107382 model2 loss : 0.067353
iteration 8384 : model1 loss : 0.116463 model2 loss : 0.086458
iteration 8385 : model1 loss : 0.106518 model2 loss : 0.097822
iteration 8386 : model1 loss : 0.121925 model2 loss : 0.142196
iteration 8387 : model1 loss : 0.127861 model2 loss : 0.080077
iteration 8388 : model1 loss : 0.130797 model2 loss : 0.106951
iteration 8389 : model1 loss : 0.126316 model2 loss : 0.088860
iteration 8390 : model1 loss : 0.138636 model2 loss : 0.095972
iteration 8391 : model1 loss : 0.107616 model2 loss : 0.069034
iteration 8392 : model1 loss : 0.135697 model2 loss : 0.105057
iteration 8393 : model1 loss : 0.099239 model2 loss : 0.072345
iteration 8394 : model1 loss : 0.098650 model2 loss : 0.067136
iteration 8395 : model1 loss : 0.160781 model2 loss : 0.104807
iteration 8396 : model1 loss : 0.129085 model2 loss : 0.095423
iteration 8397 : model1 loss : 0.119460 model2 loss : 0.094610
iteration 8398 : model1 loss : 0.129396 model2 loss : 0.110756
 84%|████████████████████████▎    | 494/589 [3:21:29<36:20, 22.95s/it]iteration 8399 : model1 loss : 0.130757 model2 loss : 0.075351
iteration 8400 : model1 loss : 0.107590 model2 loss : 0.078107
iteration 8400 : model1_mean_dice : 0.676010 model1_mean_hd95 : 89.909855 model1_mean_iou : 0.549823
iteration 8400 : model2_mean_dice : 0.744131 model2_mean_hd95 : 70.183327 model2_mean_iou : 0.631238
iteration 8401 : model1 loss : 0.110060 model2 loss : 0.090193
iteration 8402 : model1 loss : 0.111617 model2 loss : 0.098928
iteration 8403 : model1 loss : 0.113662 model2 loss : 0.104852
iteration 8404 : model1 loss : 0.121264 model2 loss : 0.095620
iteration 8405 : model1 loss : 0.126177 model2 loss : 0.090824
iteration 8406 : model1 loss : 0.091512 model2 loss : 0.089250
iteration 8407 : model1 loss : 0.127966 model2 loss : 0.093088
iteration 8408 : model1 loss : 0.127791 model2 loss : 0.098923
iteration 8409 : model1 loss : 0.124412 model2 loss : 0.080043
iteration 8410 : model1 loss : 0.105202 model2 loss : 0.081082
iteration 8411 : model1 loss : 0.126777 model2 loss : 0.139803
iteration 8412 : model1 loss : 0.146517 model2 loss : 0.107108
iteration 8413 : model1 loss : 0.137029 model2 loss : 0.094867
iteration 8414 : model1 loss : 0.114103 model2 loss : 0.122455
iteration 8415 : model1 loss : 0.140447 model2 loss : 0.101764
 84%|████████████████████████▎    | 495/589 [3:22:11<45:14, 28.88s/it]iteration 8416 : model1 loss : 0.135943 model2 loss : 0.084532
iteration 8417 : model1 loss : 0.118621 model2 loss : 0.083846
iteration 8418 : model1 loss : 0.137622 model2 loss : 0.090671
iteration 8419 : model1 loss : 0.117965 model2 loss : 0.064403
iteration 8420 : model1 loss : 0.105828 model2 loss : 0.082286
iteration 8421 : model1 loss : 0.132688 model2 loss : 0.106293
iteration 8422 : model1 loss : 0.143249 model2 loss : 0.116359
iteration 8423 : model1 loss : 0.115694 model2 loss : 0.117592
iteration 8424 : model1 loss : 0.111136 model2 loss : 0.082909
iteration 8425 : model1 loss : 0.097015 model2 loss : 0.061919
iteration 8426 : model1 loss : 0.116573 model2 loss : 0.098229
iteration 8427 : model1 loss : 0.140222 model2 loss : 0.103635
iteration 8428 : model1 loss : 0.113134 model2 loss : 0.070883
iteration 8429 : model1 loss : 0.163526 model2 loss : 0.072406
iteration 8430 : model1 loss : 0.153180 model2 loss : 0.084315
iteration 8431 : model1 loss : 0.138558 model2 loss : 0.074590
iteration 8432 : model1 loss : 0.121072 model2 loss : 0.101452
 84%|████████████████████████▍    | 496/589 [3:22:34<41:55, 27.05s/it]iteration 8433 : model1 loss : 0.119978 model2 loss : 0.087957
iteration 8434 : model1 loss : 0.130561 model2 loss : 0.092235
iteration 8435 : model1 loss : 0.123170 model2 loss : 0.080910
iteration 8436 : model1 loss : 0.153119 model2 loss : 0.106478
iteration 8437 : model1 loss : 0.155236 model2 loss : 0.119984
iteration 8438 : model1 loss : 0.099218 model2 loss : 0.066443
iteration 8439 : model1 loss : 0.138645 model2 loss : 0.120235
iteration 8440 : model1 loss : 0.120077 model2 loss : 0.075753
iteration 8441 : model1 loss : 0.097907 model2 loss : 0.081426
iteration 8442 : model1 loss : 0.124859 model2 loss : 0.102044
iteration 8443 : model1 loss : 0.116256 model2 loss : 0.073365
iteration 8444 : model1 loss : 0.113127 model2 loss : 0.080513
iteration 8445 : model1 loss : 0.123862 model2 loss : 0.073277
iteration 8446 : model1 loss : 0.143731 model2 loss : 0.097575
iteration 8447 : model1 loss : 0.112331 model2 loss : 0.079536
iteration 8448 : model1 loss : 0.116107 model2 loss : 0.116714
iteration 8449 : model1 loss : 0.179447 model2 loss : 0.097377
 84%|████████████████████████▍    | 497/589 [3:22:57<39:30, 25.77s/it]iteration 8450 : model1 loss : 0.103343 model2 loss : 0.063590
iteration 8451 : model1 loss : 0.112810 model2 loss : 0.099920
iteration 8452 : model1 loss : 0.127525 model2 loss : 0.094156
iteration 8453 : model1 loss : 0.108716 model2 loss : 0.073385
iteration 8454 : model1 loss : 0.105534 model2 loss : 0.122447
iteration 8455 : model1 loss : 0.069587 model2 loss : 0.062159
iteration 8456 : model1 loss : 0.144117 model2 loss : 0.092593
iteration 8457 : model1 loss : 0.122100 model2 loss : 0.072926
iteration 8458 : model1 loss : 0.135473 model2 loss : 0.118412
iteration 8459 : model1 loss : 0.119760 model2 loss : 0.068422
iteration 8460 : model1 loss : 0.138843 model2 loss : 0.128499
iteration 8461 : model1 loss : 0.139617 model2 loss : 0.099122
iteration 8462 : model1 loss : 0.155331 model2 loss : 0.117497
iteration 8463 : model1 loss : 0.137283 model2 loss : 0.100322
iteration 8464 : model1 loss : 0.130391 model2 loss : 0.088187
iteration 8465 : model1 loss : 0.108711 model2 loss : 0.090519
iteration 8466 : model1 loss : 0.116018 model2 loss : 0.071162
 85%|████████████████████████▌    | 498/589 [3:23:20<37:49, 24.94s/it]iteration 8467 : model1 loss : 0.130470 model2 loss : 0.093418
iteration 8468 : model1 loss : 0.117498 model2 loss : 0.078565
iteration 8469 : model1 loss : 0.135972 model2 loss : 0.077256
iteration 8470 : model1 loss : 0.135438 model2 loss : 0.119427
iteration 8471 : model1 loss : 0.137202 model2 loss : 0.083448
iteration 8472 : model1 loss : 0.091840 model2 loss : 0.078569
iteration 8473 : model1 loss : 0.111714 model2 loss : 0.092743
iteration 8474 : model1 loss : 0.082344 model2 loss : 0.053562
iteration 8475 : model1 loss : 0.149192 model2 loss : 0.099580
iteration 8476 : model1 loss : 0.159307 model2 loss : 0.085363
iteration 8477 : model1 loss : 0.115505 model2 loss : 0.077728
iteration 8478 : model1 loss : 0.143524 model2 loss : 0.087306
iteration 8479 : model1 loss : 0.117200 model2 loss : 0.116688
iteration 8480 : model1 loss : 0.172615 model2 loss : 0.111628
iteration 8481 : model1 loss : 0.110454 model2 loss : 0.079936
iteration 8482 : model1 loss : 0.108578 model2 loss : 0.080414
iteration 8483 : model1 loss : 0.121104 model2 loss : 0.126469
 85%|████████████████████████▌    | 499/589 [3:23:43<36:26, 24.30s/it]iteration 8484 : model1 loss : 0.093263 model2 loss : 0.071726
iteration 8485 : model1 loss : 0.173111 model2 loss : 0.107613
iteration 8486 : model1 loss : 0.155624 model2 loss : 0.103754
iteration 8487 : model1 loss : 0.101472 model2 loss : 0.059213
iteration 8488 : model1 loss : 0.082011 model2 loss : 0.061548
iteration 8489 : model1 loss : 0.218549 model2 loss : 0.155769
iteration 8490 : model1 loss : 0.133131 model2 loss : 0.090843
iteration 8491 : model1 loss : 0.127871 model2 loss : 0.121002
iteration 8492 : model1 loss : 0.128058 model2 loss : 0.093972
iteration 8493 : model1 loss : 0.152645 model2 loss : 0.101737
iteration 8494 : model1 loss : 0.143676 model2 loss : 0.122176
iteration 8495 : model1 loss : 0.123627 model2 loss : 0.100077
iteration 8496 : model1 loss : 0.087220 model2 loss : 0.080911
iteration 8497 : model1 loss : 0.120806 model2 loss : 0.092511
iteration 8498 : model1 loss : 0.099251 model2 loss : 0.073807
iteration 8499 : model1 loss : 0.109284 model2 loss : 0.079138
iteration 8500 : model1 loss : 0.142403 model2 loss : 0.085847
 85%|████████████████████████▌    | 500/589 [3:24:06<35:25, 23.88s/it]iteration 8501 : model1 loss : 0.133136 model2 loss : 0.077481
iteration 8502 : model1 loss : 0.166186 model2 loss : 0.103236
iteration 8503 : model1 loss : 0.140920 model2 loss : 0.108019
iteration 8504 : model1 loss : 0.159606 model2 loss : 0.087287
iteration 8505 : model1 loss : 0.111996 model2 loss : 0.131817
iteration 8506 : model1 loss : 0.127171 model2 loss : 0.085034
iteration 8507 : model1 loss : 0.160280 model2 loss : 0.092749
iteration 8508 : model1 loss : 0.094528 model2 loss : 0.090353
iteration 8509 : model1 loss : 0.097594 model2 loss : 0.073830
iteration 8510 : model1 loss : 0.104680 model2 loss : 0.076766
iteration 8511 : model1 loss : 0.136481 model2 loss : 0.101933
iteration 8512 : model1 loss : 0.117023 model2 loss : 0.151542
iteration 8513 : model1 loss : 0.146545 model2 loss : 0.086108
iteration 8514 : model1 loss : 0.119340 model2 loss : 0.083162
iteration 8515 : model1 loss : 0.100888 model2 loss : 0.089141
iteration 8516 : model1 loss : 0.122338 model2 loss : 0.122872
iteration 8517 : model1 loss : 0.134251 model2 loss : 0.102048
 85%|████████████████████████▋    | 501/589 [3:24:28<34:31, 23.54s/it]iteration 8518 : model1 loss : 0.131282 model2 loss : 0.099995
iteration 8519 : model1 loss : 0.134883 model2 loss : 0.087294
iteration 8520 : model1 loss : 0.111822 model2 loss : 0.079352
iteration 8521 : model1 loss : 0.123354 model2 loss : 0.058785
iteration 8522 : model1 loss : 0.111110 model2 loss : 0.104552
iteration 8523 : model1 loss : 0.122166 model2 loss : 0.094998
iteration 8524 : model1 loss : 0.151494 model2 loss : 0.109343
iteration 8525 : model1 loss : 0.130365 model2 loss : 0.097771
iteration 8526 : model1 loss : 0.136129 model2 loss : 0.072609
iteration 8527 : model1 loss : 0.111198 model2 loss : 0.076529
iteration 8528 : model1 loss : 0.083278 model2 loss : 0.077508
iteration 8529 : model1 loss : 0.120831 model2 loss : 0.127204
iteration 8530 : model1 loss : 0.124571 model2 loss : 0.108000
iteration 8531 : model1 loss : 0.163113 model2 loss : 0.069232
iteration 8532 : model1 loss : 0.120013 model2 loss : 0.091987
iteration 8533 : model1 loss : 0.122532 model2 loss : 0.102668
iteration 8534 : model1 loss : 0.117798 model2 loss : 0.076648
 85%|████████████████████████▋    | 502/589 [3:24:51<33:48, 23.32s/it]iteration 8535 : model1 loss : 0.135544 model2 loss : 0.089537
iteration 8536 : model1 loss : 0.110346 model2 loss : 0.069978
iteration 8537 : model1 loss : 0.135844 model2 loss : 0.112070
iteration 8538 : model1 loss : 0.141097 model2 loss : 0.079067
iteration 8539 : model1 loss : 0.112620 model2 loss : 0.079756
iteration 8540 : model1 loss : 0.139188 model2 loss : 0.082116
iteration 8541 : model1 loss : 0.114993 model2 loss : 0.102930
iteration 8542 : model1 loss : 0.147210 model2 loss : 0.106779
iteration 8543 : model1 loss : 0.111849 model2 loss : 0.080663
iteration 8544 : model1 loss : 0.097008 model2 loss : 0.071027
iteration 8545 : model1 loss : 0.132982 model2 loss : 0.101251
iteration 8546 : model1 loss : 0.164211 model2 loss : 0.089452
iteration 8547 : model1 loss : 0.139305 model2 loss : 0.074628
iteration 8548 : model1 loss : 0.138050 model2 loss : 0.119379
iteration 8549 : model1 loss : 0.106681 model2 loss : 0.059537
iteration 8550 : model1 loss : 0.091680 model2 loss : 0.097517
iteration 8551 : model1 loss : 0.137432 model2 loss : 0.115899
 85%|████████████████████████▊    | 503/589 [3:25:14<33:15, 23.20s/it]iteration 8552 : model1 loss : 0.116249 model2 loss : 0.071788
iteration 8553 : model1 loss : 0.125681 model2 loss : 0.108070
iteration 8554 : model1 loss : 0.146150 model2 loss : 0.095984
iteration 8555 : model1 loss : 0.136234 model2 loss : 0.067288
iteration 8556 : model1 loss : 0.108667 model2 loss : 0.081432
iteration 8557 : model1 loss : 0.173329 model2 loss : 0.131420
iteration 8558 : model1 loss : 0.134178 model2 loss : 0.079833
iteration 8559 : model1 loss : 0.147343 model2 loss : 0.135634
iteration 8560 : model1 loss : 0.142526 model2 loss : 0.108224
iteration 8561 : model1 loss : 0.102716 model2 loss : 0.070409
iteration 8562 : model1 loss : 0.168963 model2 loss : 0.129033
iteration 8563 : model1 loss : 0.088623 model2 loss : 0.082272
iteration 8564 : model1 loss : 0.109518 model2 loss : 0.089326
iteration 8565 : model1 loss : 0.166528 model2 loss : 0.115708
iteration 8566 : model1 loss : 0.104779 model2 loss : 0.101156
iteration 8567 : model1 loss : 0.111936 model2 loss : 0.087898
iteration 8568 : model1 loss : 0.130765 model2 loss : 0.077957
 86%|████████████████████████▊    | 504/589 [3:25:37<32:42, 23.09s/it]iteration 8569 : model1 loss : 0.105994 model2 loss : 0.084670
iteration 8570 : model1 loss : 0.107546 model2 loss : 0.099361
iteration 8571 : model1 loss : 0.196542 model2 loss : 0.088092
iteration 8572 : model1 loss : 0.111362 model2 loss : 0.073307
iteration 8573 : model1 loss : 0.115984 model2 loss : 0.084417
iteration 8574 : model1 loss : 0.171381 model2 loss : 0.089020
iteration 8575 : model1 loss : 0.156880 model2 loss : 0.100037
iteration 8576 : model1 loss : 0.130391 model2 loss : 0.094105
iteration 8577 : model1 loss : 0.114018 model2 loss : 0.089318
iteration 8578 : model1 loss : 0.137759 model2 loss : 0.098259
iteration 8579 : model1 loss : 0.143268 model2 loss : 0.085608
iteration 8580 : model1 loss : 0.157612 model2 loss : 0.090111
iteration 8581 : model1 loss : 0.131034 model2 loss : 0.080061
iteration 8582 : model1 loss : 0.139901 model2 loss : 0.073674
iteration 8583 : model1 loss : 0.101278 model2 loss : 0.079378
iteration 8584 : model1 loss : 0.086591 model2 loss : 0.074443
iteration 8585 : model1 loss : 0.109490 model2 loss : 0.089489
 86%|████████████████████████▊    | 505/589 [3:26:00<32:12, 23.01s/it]iteration 8586 : model1 loss : 0.129740 model2 loss : 0.099217
iteration 8587 : model1 loss : 0.124982 model2 loss : 0.066635
iteration 8588 : model1 loss : 0.159280 model2 loss : 0.107671
iteration 8589 : model1 loss : 0.140822 model2 loss : 0.097436
iteration 8590 : model1 loss : 0.094107 model2 loss : 0.083087
iteration 8591 : model1 loss : 0.108755 model2 loss : 0.075082
iteration 8592 : model1 loss : 0.143389 model2 loss : 0.144754
iteration 8593 : model1 loss : 0.111338 model2 loss : 0.076258
iteration 8594 : model1 loss : 0.104679 model2 loss : 0.071846
iteration 8595 : model1 loss : 0.147405 model2 loss : 0.073232
iteration 8596 : model1 loss : 0.147286 model2 loss : 0.099742
iteration 8597 : model1 loss : 0.143840 model2 loss : 0.106665
iteration 8598 : model1 loss : 0.131834 model2 loss : 0.094408
iteration 8599 : model1 loss : 0.141162 model2 loss : 0.099010
iteration 8600 : model1 loss : 0.119648 model2 loss : 0.107828
iteration 8600 : model1_mean_dice : 0.668530 model1_mean_hd95 : 93.237100 model1_mean_iou : 0.544416
iteration 8600 : model2_mean_dice : 0.744681 model2_mean_hd95 : 70.197889 model2_mean_iou : 0.633336
iteration 8601 : model1 loss : 0.104899 model2 loss : 0.087003
iteration 8602 : model1 loss : 0.116640 model2 loss : 0.083480
 86%|████████████████████████▉    | 506/589 [3:26:42<39:57, 28.88s/it]iteration 8603 : model1 loss : 0.105661 model2 loss : 0.095854
iteration 8604 : model1 loss : 0.159375 model2 loss : 0.088687
iteration 8605 : model1 loss : 0.116114 model2 loss : 0.070564
iteration 8606 : model1 loss : 0.155353 model2 loss : 0.107078
iteration 8607 : model1 loss : 0.144510 model2 loss : 0.133316
iteration 8608 : model1 loss : 0.097447 model2 loss : 0.072236
iteration 8609 : model1 loss : 0.146341 model2 loss : 0.101071
iteration 8610 : model1 loss : 0.123443 model2 loss : 0.065546
iteration 8611 : model1 loss : 0.150264 model2 loss : 0.080940
iteration 8612 : model1 loss : 0.093178 model2 loss : 0.066421
iteration 8613 : model1 loss : 0.135038 model2 loss : 0.099765
iteration 8614 : model1 loss : 0.143983 model2 loss : 0.096196
iteration 8615 : model1 loss : 0.129802 model2 loss : 0.100608
iteration 8616 : model1 loss : 0.108524 model2 loss : 0.094101
iteration 8617 : model1 loss : 0.130638 model2 loss : 0.089155
iteration 8618 : model1 loss : 0.126976 model2 loss : 0.082609
iteration 8619 : model1 loss : 0.105146 model2 loss : 0.074689
 86%|████████████████████████▉    | 507/589 [3:27:05<36:55, 27.02s/it]iteration 8620 : model1 loss : 0.114324 model2 loss : 0.100916
iteration 8621 : model1 loss : 0.148037 model2 loss : 0.089142
iteration 8622 : model1 loss : 0.165825 model2 loss : 0.121380
iteration 8623 : model1 loss : 0.117975 model2 loss : 0.065017
iteration 8624 : model1 loss : 0.129727 model2 loss : 0.086734
iteration 8625 : model1 loss : 0.139471 model2 loss : 0.106994
iteration 8626 : model1 loss : 0.146628 model2 loss : 0.119578
iteration 8627 : model1 loss : 0.098733 model2 loss : 0.098377
iteration 8628 : model1 loss : 0.124342 model2 loss : 0.089596
iteration 8629 : model1 loss : 0.105959 model2 loss : 0.088842
iteration 8630 : model1 loss : 0.103313 model2 loss : 0.073236
iteration 8631 : model1 loss : 0.146527 model2 loss : 0.099793
iteration 8632 : model1 loss : 0.101110 model2 loss : 0.092047
iteration 8633 : model1 loss : 0.126387 model2 loss : 0.088973
iteration 8634 : model1 loss : 0.096765 model2 loss : 0.084530
iteration 8635 : model1 loss : 0.127723 model2 loss : 0.098163
iteration 8636 : model1 loss : 0.143218 model2 loss : 0.074679
 86%|█████████████████████████    | 508/589 [3:27:28<34:46, 25.75s/it]iteration 8637 : model1 loss : 0.159995 model2 loss : 0.099141
iteration 8638 : model1 loss : 0.155918 model2 loss : 0.114422
iteration 8639 : model1 loss : 0.103238 model2 loss : 0.081168
iteration 8640 : model1 loss : 0.143189 model2 loss : 0.067955
iteration 8641 : model1 loss : 0.109095 model2 loss : 0.105661
iteration 8642 : model1 loss : 0.123163 model2 loss : 0.087231
iteration 8643 : model1 loss : 0.146389 model2 loss : 0.090471
iteration 8644 : model1 loss : 0.083631 model2 loss : 0.085938
iteration 8645 : model1 loss : 0.107589 model2 loss : 0.080933
iteration 8646 : model1 loss : 0.119834 model2 loss : 0.118619
iteration 8647 : model1 loss : 0.119358 model2 loss : 0.109938
iteration 8648 : model1 loss : 0.093569 model2 loss : 0.058152
iteration 8649 : model1 loss : 0.127935 model2 loss : 0.078790
iteration 8650 : model1 loss : 0.145552 model2 loss : 0.093078
iteration 8651 : model1 loss : 0.106154 model2 loss : 0.077350
iteration 8652 : model1 loss : 0.115086 model2 loss : 0.100555
iteration 8653 : model1 loss : 0.080114 model2 loss : 0.068079
 86%|█████████████████████████    | 509/589 [3:27:51<33:12, 24.90s/it]iteration 8654 : model1 loss : 0.151904 model2 loss : 0.106557
iteration 8655 : model1 loss : 0.127841 model2 loss : 0.093708
iteration 8656 : model1 loss : 0.136914 model2 loss : 0.085554
iteration 8657 : model1 loss : 0.158782 model2 loss : 0.085390
iteration 8658 : model1 loss : 0.095617 model2 loss : 0.088607
iteration 8659 : model1 loss : 0.130903 model2 loss : 0.138234
iteration 8660 : model1 loss : 0.154114 model2 loss : 0.096585
iteration 8661 : model1 loss : 0.126988 model2 loss : 0.093462
iteration 8662 : model1 loss : 0.112567 model2 loss : 0.096912
iteration 8663 : model1 loss : 0.099549 model2 loss : 0.061511
iteration 8664 : model1 loss : 0.083352 model2 loss : 0.071734
iteration 8665 : model1 loss : 0.168781 model2 loss : 0.093889
iteration 8666 : model1 loss : 0.103135 model2 loss : 0.096480
iteration 8667 : model1 loss : 0.104963 model2 loss : 0.105982
iteration 8668 : model1 loss : 0.093866 model2 loss : 0.077306
iteration 8669 : model1 loss : 0.114683 model2 loss : 0.125614
iteration 8670 : model1 loss : 0.107468 model2 loss : 0.100664
 87%|█████████████████████████    | 510/589 [3:28:13<31:55, 24.24s/it]iteration 8671 : model1 loss : 0.120769 model2 loss : 0.100201
iteration 8672 : model1 loss : 0.158438 model2 loss : 0.129283
iteration 8673 : model1 loss : 0.132254 model2 loss : 0.089321
iteration 8674 : model1 loss : 0.113913 model2 loss : 0.076746
iteration 8675 : model1 loss : 0.119909 model2 loss : 0.079038
iteration 8676 : model1 loss : 0.109247 model2 loss : 0.080935
iteration 8677 : model1 loss : 0.094640 model2 loss : 0.067865
iteration 8678 : model1 loss : 0.112131 model2 loss : 0.086898
iteration 8679 : model1 loss : 0.151259 model2 loss : 0.121757
iteration 8680 : model1 loss : 0.115272 model2 loss : 0.088560
iteration 8681 : model1 loss : 0.124056 model2 loss : 0.080353
iteration 8682 : model1 loss : 0.138280 model2 loss : 0.091985
iteration 8683 : model1 loss : 0.127582 model2 loss : 0.115377
iteration 8684 : model1 loss : 0.148643 model2 loss : 0.146333
iteration 8685 : model1 loss : 0.145481 model2 loss : 0.106064
iteration 8686 : model1 loss : 0.093842 model2 loss : 0.089210
iteration 8687 : model1 loss : 0.095345 model2 loss : 0.084804
 87%|█████████████████████████▏   | 511/589 [3:28:36<30:56, 23.80s/it]iteration 8688 : model1 loss : 0.148271 model2 loss : 0.110057
iteration 8689 : model1 loss : 0.115282 model2 loss : 0.070550
iteration 8690 : model1 loss : 0.104145 model2 loss : 0.061493
iteration 8691 : model1 loss : 0.103830 model2 loss : 0.073816
iteration 8692 : model1 loss : 0.135162 model2 loss : 0.117466
iteration 8693 : model1 loss : 0.147105 model2 loss : 0.104299
iteration 8694 : model1 loss : 0.168108 model2 loss : 0.147867
iteration 8695 : model1 loss : 0.123282 model2 loss : 0.091580
iteration 8696 : model1 loss : 0.093721 model2 loss : 0.069245
iteration 8697 : model1 loss : 0.111846 model2 loss : 0.074404
iteration 8698 : model1 loss : 0.090126 model2 loss : 0.064854
iteration 8699 : model1 loss : 0.127083 model2 loss : 0.088246
iteration 8700 : model1 loss : 0.141467 model2 loss : 0.110830
iteration 8701 : model1 loss : 0.184845 model2 loss : 0.110084
iteration 8702 : model1 loss : 0.106476 model2 loss : 0.082796
iteration 8703 : model1 loss : 0.100981 model2 loss : 0.084093
iteration 8704 : model1 loss : 0.164463 model2 loss : 0.082889
 87%|█████████████████████████▏   | 512/589 [3:28:59<30:09, 23.49s/it]iteration 8705 : model1 loss : 0.101092 model2 loss : 0.106031
iteration 8706 : model1 loss : 0.107847 model2 loss : 0.099705
iteration 8707 : model1 loss : 0.134318 model2 loss : 0.095607
iteration 8708 : model1 loss : 0.141827 model2 loss : 0.093839
iteration 8709 : model1 loss : 0.138869 model2 loss : 0.083010
iteration 8710 : model1 loss : 0.139370 model2 loss : 0.110635
iteration 8711 : model1 loss : 0.106690 model2 loss : 0.086458
iteration 8712 : model1 loss : 0.094650 model2 loss : 0.089708
iteration 8713 : model1 loss : 0.159802 model2 loss : 0.073415
iteration 8714 : model1 loss : 0.120031 model2 loss : 0.114085
iteration 8715 : model1 loss : 0.114698 model2 loss : 0.086188
iteration 8716 : model1 loss : 0.138044 model2 loss : 0.087033
iteration 8717 : model1 loss : 0.126004 model2 loss : 0.081571
iteration 8718 : model1 loss : 0.152412 model2 loss : 0.086445
iteration 8719 : model1 loss : 0.180391 model2 loss : 0.118617
iteration 8720 : model1 loss : 0.109007 model2 loss : 0.069396
iteration 8721 : model1 loss : 0.123197 model2 loss : 0.083056
 87%|█████████████████████████▎   | 513/589 [3:29:22<29:28, 23.26s/it]iteration 8722 : model1 loss : 0.118185 model2 loss : 0.080825
iteration 8723 : model1 loss : 0.170528 model2 loss : 0.104941
iteration 8724 : model1 loss : 0.137518 model2 loss : 0.088431
iteration 8725 : model1 loss : 0.121497 model2 loss : 0.073461
iteration 8726 : model1 loss : 0.134645 model2 loss : 0.105260
iteration 8727 : model1 loss : 0.142724 model2 loss : 0.084303
iteration 8728 : model1 loss : 0.105558 model2 loss : 0.094269
iteration 8729 : model1 loss : 0.128748 model2 loss : 0.095998
iteration 8730 : model1 loss : 0.111294 model2 loss : 0.079675
iteration 8731 : model1 loss : 0.118437 model2 loss : 0.100780
iteration 8732 : model1 loss : 0.118149 model2 loss : 0.091581
iteration 8733 : model1 loss : 0.130198 model2 loss : 0.094767
iteration 8734 : model1 loss : 0.152300 model2 loss : 0.113323
iteration 8735 : model1 loss : 0.140843 model2 loss : 0.087521
iteration 8736 : model1 loss : 0.093232 model2 loss : 0.081312
iteration 8737 : model1 loss : 0.131134 model2 loss : 0.119886
iteration 8738 : model1 loss : 0.096506 model2 loss : 0.063456
 87%|█████████████████████████▎   | 514/589 [3:29:44<28:53, 23.11s/it]iteration 8739 : model1 loss : 0.085059 model2 loss : 0.078533
iteration 8740 : model1 loss : 0.139119 model2 loss : 0.088348
iteration 8741 : model1 loss : 0.117427 model2 loss : 0.078544
iteration 8742 : model1 loss : 0.131158 model2 loss : 0.078211
iteration 8743 : model1 loss : 0.122560 model2 loss : 0.120036
iteration 8744 : model1 loss : 0.108941 model2 loss : 0.078569
iteration 8745 : model1 loss : 0.131952 model2 loss : 0.071270
iteration 8746 : model1 loss : 0.158903 model2 loss : 0.103097
iteration 8747 : model1 loss : 0.132274 model2 loss : 0.086730
iteration 8748 : model1 loss : 0.116232 model2 loss : 0.101332
iteration 8749 : model1 loss : 0.132995 model2 loss : 0.109228
iteration 8750 : model1 loss : 0.157643 model2 loss : 0.124416
iteration 8751 : model1 loss : 0.140194 model2 loss : 0.092048
iteration 8752 : model1 loss : 0.139661 model2 loss : 0.095329
iteration 8753 : model1 loss : 0.108795 model2 loss : 0.090925
iteration 8754 : model1 loss : 0.099092 model2 loss : 0.090199
iteration 8755 : model1 loss : 0.103712 model2 loss : 0.073313
 87%|█████████████████████████▎   | 515/589 [3:30:07<28:24, 23.03s/it]iteration 8756 : model1 loss : 0.151779 model2 loss : 0.117017
iteration 8757 : model1 loss : 0.114583 model2 loss : 0.108426
iteration 8758 : model1 loss : 0.129860 model2 loss : 0.100908
iteration 8759 : model1 loss : 0.109242 model2 loss : 0.086153
iteration 8760 : model1 loss : 0.100814 model2 loss : 0.093069
iteration 8761 : model1 loss : 0.098941 model2 loss : 0.092550
iteration 8762 : model1 loss : 0.129956 model2 loss : 0.102877
iteration 8763 : model1 loss : 0.139575 model2 loss : 0.136599
iteration 8764 : model1 loss : 0.117838 model2 loss : 0.067724
iteration 8765 : model1 loss : 0.129591 model2 loss : 0.118980
iteration 8766 : model1 loss : 0.143622 model2 loss : 0.082039
iteration 8767 : model1 loss : 0.124799 model2 loss : 0.086853
iteration 8768 : model1 loss : 0.101650 model2 loss : 0.074556
iteration 8769 : model1 loss : 0.135647 model2 loss : 0.077037
iteration 8770 : model1 loss : 0.096668 model2 loss : 0.072151
iteration 8771 : model1 loss : 0.126727 model2 loss : 0.109722
iteration 8772 : model1 loss : 0.103520 model2 loss : 0.099278
 88%|█████████████████████████▍   | 516/589 [3:30:30<27:56, 22.96s/it]iteration 8773 : model1 loss : 0.131200 model2 loss : 0.083344
iteration 8774 : model1 loss : 0.111275 model2 loss : 0.110599
iteration 8775 : model1 loss : 0.147480 model2 loss : 0.069917
iteration 8776 : model1 loss : 0.148648 model2 loss : 0.112591
iteration 8777 : model1 loss : 0.122005 model2 loss : 0.074931
iteration 8778 : model1 loss : 0.100587 model2 loss : 0.084346
iteration 8779 : model1 loss : 0.116956 model2 loss : 0.107852
iteration 8780 : model1 loss : 0.129846 model2 loss : 0.100533
iteration 8781 : model1 loss : 0.130373 model2 loss : 0.108865
iteration 8782 : model1 loss : 0.135343 model2 loss : 0.086195
iteration 8783 : model1 loss : 0.144452 model2 loss : 0.093856
iteration 8784 : model1 loss : 0.139574 model2 loss : 0.109585
iteration 8785 : model1 loss : 0.098799 model2 loss : 0.073058
iteration 8786 : model1 loss : 0.093377 model2 loss : 0.073810
iteration 8787 : model1 loss : 0.107413 model2 loss : 0.091850
iteration 8788 : model1 loss : 0.114291 model2 loss : 0.082299
iteration 8789 : model1 loss : 0.121120 model2 loss : 0.092617
 88%|█████████████████████████▍   | 517/589 [3:30:53<27:28, 22.89s/it]iteration 8790 : model1 loss : 0.110142 model2 loss : 0.065737
iteration 8791 : model1 loss : 0.084687 model2 loss : 0.066342
iteration 8792 : model1 loss : 0.144503 model2 loss : 0.110289
iteration 8793 : model1 loss : 0.142833 model2 loss : 0.094649
iteration 8794 : model1 loss : 0.093818 model2 loss : 0.111088
iteration 8795 : model1 loss : 0.111610 model2 loss : 0.084118
iteration 8796 : model1 loss : 0.112531 model2 loss : 0.076781
iteration 8797 : model1 loss : 0.117781 model2 loss : 0.089114
iteration 8798 : model1 loss : 0.131041 model2 loss : 0.112213
iteration 8799 : model1 loss : 0.124738 model2 loss : 0.112906
iteration 8800 : model1 loss : 0.156437 model2 loss : 0.107892
iteration 8800 : model1_mean_dice : 0.683654 model1_mean_hd95 : 89.906145 model1_mean_iou : 0.558991
iteration 8800 : model2_mean_dice : 0.752201 model2_mean_hd95 : 72.172317 model2_mean_iou : 0.641072
iteration 8801 : model1 loss : 0.099339 model2 loss : 0.092343
iteration 8802 : model1 loss : 0.122099 model2 loss : 0.102496
iteration 8803 : model1 loss : 0.126612 model2 loss : 0.106309
iteration 8804 : model1 loss : 0.147788 model2 loss : 0.082881
iteration 8805 : model1 loss : 0.095613 model2 loss : 0.100031
iteration 8806 : model1 loss : 0.146178 model2 loss : 0.087227
 88%|█████████████████████████▌   | 518/589 [3:31:36<34:07, 28.84s/it]iteration 8807 : model1 loss : 0.137332 model2 loss : 0.120602
iteration 8808 : model1 loss : 0.084595 model2 loss : 0.085285
iteration 8809 : model1 loss : 0.133426 model2 loss : 0.077126
iteration 8810 : model1 loss : 0.087487 model2 loss : 0.063762
iteration 8811 : model1 loss : 0.110411 model2 loss : 0.101337
iteration 8812 : model1 loss : 0.147083 model2 loss : 0.100539
iteration 8813 : model1 loss : 0.110045 model2 loss : 0.087714
iteration 8814 : model1 loss : 0.101268 model2 loss : 0.073370
iteration 8815 : model1 loss : 0.134294 model2 loss : 0.096718
iteration 8816 : model1 loss : 0.114130 model2 loss : 0.092077
iteration 8817 : model1 loss : 0.161292 model2 loss : 0.096454
iteration 8818 : model1 loss : 0.187884 model2 loss : 0.126067
iteration 8819 : model1 loss : 0.154606 model2 loss : 0.078796
iteration 8820 : model1 loss : 0.120288 model2 loss : 0.077838
iteration 8821 : model1 loss : 0.134141 model2 loss : 0.114010
iteration 8822 : model1 loss : 0.147758 model2 loss : 0.091723
iteration 8823 : model1 loss : 0.126593 model2 loss : 0.110824
 88%|█████████████████████████▌   | 519/589 [3:31:58<31:30, 27.00s/it]iteration 8824 : model1 loss : 0.133664 model2 loss : 0.100672
iteration 8825 : model1 loss : 0.094540 model2 loss : 0.074887
iteration 8826 : model1 loss : 0.114351 model2 loss : 0.070324
iteration 8827 : model1 loss : 0.153456 model2 loss : 0.112878
iteration 8828 : model1 loss : 0.107222 model2 loss : 0.086758
iteration 8829 : model1 loss : 0.104995 model2 loss : 0.089923
iteration 8830 : model1 loss : 0.166656 model2 loss : 0.112628
iteration 8831 : model1 loss : 0.126836 model2 loss : 0.076322
iteration 8832 : model1 loss : 0.151425 model2 loss : 0.087776
iteration 8833 : model1 loss : 0.131354 model2 loss : 0.096269
iteration 8834 : model1 loss : 0.173943 model2 loss : 0.086916
iteration 8835 : model1 loss : 0.113185 model2 loss : 0.076558
iteration 8836 : model1 loss : 0.132212 model2 loss : 0.083170
iteration 8837 : model1 loss : 0.152913 model2 loss : 0.110349
iteration 8838 : model1 loss : 0.132460 model2 loss : 0.088266
iteration 8839 : model1 loss : 0.100530 model2 loss : 0.078725
iteration 8840 : model1 loss : 0.131385 model2 loss : 0.090775
 88%|█████████████████████████▌   | 520/589 [3:32:21<29:34, 25.72s/it]iteration 8841 : model1 loss : 0.090902 model2 loss : 0.067832
iteration 8842 : model1 loss : 0.118856 model2 loss : 0.068998
iteration 8843 : model1 loss : 0.129405 model2 loss : 0.115613
iteration 8844 : model1 loss : 0.131096 model2 loss : 0.072153
iteration 8845 : model1 loss : 0.092985 model2 loss : 0.078624
iteration 8846 : model1 loss : 0.117846 model2 loss : 0.115882
iteration 8847 : model1 loss : 0.123391 model2 loss : 0.091079
iteration 8848 : model1 loss : 0.124708 model2 loss : 0.091708
iteration 8849 : model1 loss : 0.119614 model2 loss : 0.076038
iteration 8850 : model1 loss : 0.119037 model2 loss : 0.094256
iteration 8851 : model1 loss : 0.160992 model2 loss : 0.116405
iteration 8852 : model1 loss : 0.099968 model2 loss : 0.077518
iteration 8853 : model1 loss : 0.132365 model2 loss : 0.100798
iteration 8854 : model1 loss : 0.116030 model2 loss : 0.077001
iteration 8855 : model1 loss : 0.120894 model2 loss : 0.088577
iteration 8856 : model1 loss : 0.105945 model2 loss : 0.105930
iteration 8857 : model1 loss : 0.137852 model2 loss : 0.074248
 88%|█████████████████████████▋   | 521/589 [3:32:44<28:10, 24.86s/it]iteration 8858 : model1 loss : 0.099241 model2 loss : 0.068731
iteration 8859 : model1 loss : 0.139011 model2 loss : 0.103079
iteration 8860 : model1 loss : 0.132441 model2 loss : 0.062436
iteration 8861 : model1 loss : 0.136488 model2 loss : 0.090725
iteration 8862 : model1 loss : 0.120590 model2 loss : 0.093608
iteration 8863 : model1 loss : 0.132258 model2 loss : 0.092423
iteration 8864 : model1 loss : 0.119413 model2 loss : 0.081891
iteration 8865 : model1 loss : 0.139672 model2 loss : 0.072695
iteration 8866 : model1 loss : 0.120391 model2 loss : 0.070672
iteration 8867 : model1 loss : 0.124531 model2 loss : 0.088955
iteration 8868 : model1 loss : 0.129933 model2 loss : 0.097810
iteration 8869 : model1 loss : 0.126544 model2 loss : 0.098231
iteration 8870 : model1 loss : 0.138788 model2 loss : 0.081714
iteration 8871 : model1 loss : 0.135602 model2 loss : 0.082903
iteration 8872 : model1 loss : 0.103428 model2 loss : 0.068914
iteration 8873 : model1 loss : 0.110841 model2 loss : 0.088339
iteration 8874 : model1 loss : 0.087297 model2 loss : 0.076566
 89%|█████████████████████████▋   | 522/589 [3:33:07<27:03, 24.23s/it]iteration 8875 : model1 loss : 0.150713 model2 loss : 0.100704
iteration 8876 : model1 loss : 0.120074 model2 loss : 0.076811
iteration 8877 : model1 loss : 0.120920 model2 loss : 0.101832
iteration 8878 : model1 loss : 0.106350 model2 loss : 0.070429
iteration 8879 : model1 loss : 0.107332 model2 loss : 0.105914
iteration 8880 : model1 loss : 0.165814 model2 loss : 0.110498
iteration 8881 : model1 loss : 0.117337 model2 loss : 0.088786
iteration 8882 : model1 loss : 0.140615 model2 loss : 0.099428
iteration 8883 : model1 loss : 0.126997 model2 loss : 0.101166
iteration 8884 : model1 loss : 0.156664 model2 loss : 0.137732
iteration 8885 : model1 loss : 0.116573 model2 loss : 0.076487
iteration 8886 : model1 loss : 0.119119 model2 loss : 0.072472
iteration 8887 : model1 loss : 0.143794 model2 loss : 0.083143
iteration 8888 : model1 loss : 0.119458 model2 loss : 0.074928
iteration 8889 : model1 loss : 0.106989 model2 loss : 0.098095
iteration 8890 : model1 loss : 0.092775 model2 loss : 0.099759
iteration 8891 : model1 loss : 0.107720 model2 loss : 0.065500
 89%|█████████████████████████▊   | 523/589 [3:33:29<26:11, 23.81s/it]iteration 8892 : model1 loss : 0.126129 model2 loss : 0.091936
iteration 8893 : model1 loss : 0.121406 model2 loss : 0.063699
iteration 8894 : model1 loss : 0.155452 model2 loss : 0.079338
iteration 8895 : model1 loss : 0.189425 model2 loss : 0.116890
iteration 8896 : model1 loss : 0.122385 model2 loss : 0.094722
iteration 8897 : model1 loss : 0.095166 model2 loss : 0.061658
iteration 8898 : model1 loss : 0.150806 model2 loss : 0.096429
iteration 8899 : model1 loss : 0.128892 model2 loss : 0.076853
iteration 8900 : model1 loss : 0.117898 model2 loss : 0.078035
iteration 8901 : model1 loss : 0.114193 model2 loss : 0.093654
iteration 8902 : model1 loss : 0.134033 model2 loss : 0.104575
iteration 8903 : model1 loss : 0.118920 model2 loss : 0.076903
iteration 8904 : model1 loss : 0.114511 model2 loss : 0.093669
iteration 8905 : model1 loss : 0.112532 model2 loss : 0.072755
iteration 8906 : model1 loss : 0.108363 model2 loss : 0.071274
iteration 8907 : model1 loss : 0.105699 model2 loss : 0.072058
iteration 8908 : model1 loss : 0.114907 model2 loss : 0.115182
 89%|█████████████████████████▊   | 524/589 [3:33:52<25:28, 23.52s/it]iteration 8909 : model1 loss : 0.153698 model2 loss : 0.077948
iteration 8910 : model1 loss : 0.126562 model2 loss : 0.079796
iteration 8911 : model1 loss : 0.159185 model2 loss : 0.091558
iteration 8912 : model1 loss : 0.130674 model2 loss : 0.079184
iteration 8913 : model1 loss : 0.105337 model2 loss : 0.085657
iteration 8914 : model1 loss : 0.120604 model2 loss : 0.098436
iteration 8915 : model1 loss : 0.098385 model2 loss : 0.071090
iteration 8916 : model1 loss : 0.121826 model2 loss : 0.077621
iteration 8917 : model1 loss : 0.139669 model2 loss : 0.100358
iteration 8918 : model1 loss : 0.141695 model2 loss : 0.093627
iteration 8919 : model1 loss : 0.135043 model2 loss : 0.079211
iteration 8920 : model1 loss : 0.088132 model2 loss : 0.071086
iteration 8921 : model1 loss : 0.131287 model2 loss : 0.084981
iteration 8922 : model1 loss : 0.124065 model2 loss : 0.133495
iteration 8923 : model1 loss : 0.135068 model2 loss : 0.074517
iteration 8924 : model1 loss : 0.124616 model2 loss : 0.090613
iteration 8925 : model1 loss : 0.148042 model2 loss : 0.095419
 89%|█████████████████████████▊   | 525/589 [3:34:15<24:51, 23.30s/it]iteration 8926 : model1 loss : 0.112012 model2 loss : 0.078297
iteration 8927 : model1 loss : 0.130658 model2 loss : 0.072570
iteration 8928 : model1 loss : 0.133636 model2 loss : 0.107319
iteration 8929 : model1 loss : 0.172512 model2 loss : 0.119107
iteration 8930 : model1 loss : 0.084797 model2 loss : 0.079511
iteration 8931 : model1 loss : 0.126172 model2 loss : 0.079357
iteration 8932 : model1 loss : 0.173416 model2 loss : 0.079241
iteration 8933 : model1 loss : 0.120359 model2 loss : 0.115703
iteration 8934 : model1 loss : 0.136950 model2 loss : 0.112461
iteration 8935 : model1 loss : 0.116536 model2 loss : 0.099165
iteration 8936 : model1 loss : 0.123236 model2 loss : 0.092303
iteration 8937 : model1 loss : 0.145427 model2 loss : 0.142625
iteration 8938 : model1 loss : 0.087694 model2 loss : 0.079039
iteration 8939 : model1 loss : 0.144457 model2 loss : 0.106551
iteration 8940 : model1 loss : 0.112729 model2 loss : 0.072931
iteration 8941 : model1 loss : 0.095519 model2 loss : 0.063462
iteration 8942 : model1 loss : 0.119792 model2 loss : 0.088778
 89%|█████████████████████████▉   | 526/589 [3:34:38<24:17, 23.14s/it]iteration 8943 : model1 loss : 0.127238 model2 loss : 0.077929
iteration 8944 : model1 loss : 0.122819 model2 loss : 0.087659
iteration 8945 : model1 loss : 0.138745 model2 loss : 0.079548
iteration 8946 : model1 loss : 0.103544 model2 loss : 0.087572
iteration 8947 : model1 loss : 0.094995 model2 loss : 0.083414
iteration 8948 : model1 loss : 0.106127 model2 loss : 0.087542
iteration 8949 : model1 loss : 0.129240 model2 loss : 0.114127
iteration 8950 : model1 loss : 0.109810 model2 loss : 0.084047
iteration 8951 : model1 loss : 0.227054 model2 loss : 0.144339
iteration 8952 : model1 loss : 0.144905 model2 loss : 0.102432
iteration 8953 : model1 loss : 0.166747 model2 loss : 0.087474
iteration 8954 : model1 loss : 0.117718 model2 loss : 0.083979
iteration 8955 : model1 loss : 0.139834 model2 loss : 0.100445
iteration 8956 : model1 loss : 0.132456 model2 loss : 0.096176
iteration 8957 : model1 loss : 0.114808 model2 loss : 0.078080
iteration 8958 : model1 loss : 0.119363 model2 loss : 0.124973
iteration 8959 : model1 loss : 0.130856 model2 loss : 0.083367
 89%|█████████████████████████▉   | 527/589 [3:35:01<23:50, 23.07s/it]iteration 8960 : model1 loss : 0.105191 model2 loss : 0.083650
iteration 8961 : model1 loss : 0.115553 model2 loss : 0.066036
iteration 8962 : model1 loss : 0.131007 model2 loss : 0.082072
iteration 8963 : model1 loss : 0.116367 model2 loss : 0.069117
iteration 8964 : model1 loss : 0.089930 model2 loss : 0.111801
iteration 8965 : model1 loss : 0.149334 model2 loss : 0.119013
iteration 8966 : model1 loss : 0.096960 model2 loss : 0.089888
iteration 8967 : model1 loss : 0.131860 model2 loss : 0.092587
iteration 8968 : model1 loss : 0.108256 model2 loss : 0.058549
iteration 8969 : model1 loss : 0.110314 model2 loss : 0.096242
iteration 8970 : model1 loss : 0.112597 model2 loss : 0.093202
iteration 8971 : model1 loss : 0.141231 model2 loss : 0.100741
iteration 8972 : model1 loss : 0.132932 model2 loss : 0.093396
iteration 8973 : model1 loss : 0.134045 model2 loss : 0.082738
iteration 8974 : model1 loss : 0.108586 model2 loss : 0.084017
iteration 8975 : model1 loss : 0.124723 model2 loss : 0.084352
iteration 8976 : model1 loss : 0.116890 model2 loss : 0.095776
 90%|█████████████████████████▉   | 528/589 [3:35:23<23:22, 22.98s/it]iteration 8977 : model1 loss : 0.157701 model2 loss : 0.122171
iteration 8978 : model1 loss : 0.117544 model2 loss : 0.071303
iteration 8979 : model1 loss : 0.104290 model2 loss : 0.065636
iteration 8980 : model1 loss : 0.120363 model2 loss : 0.091371
iteration 8981 : model1 loss : 0.125693 model2 loss : 0.095424
iteration 8982 : model1 loss : 0.142205 model2 loss : 0.116806
iteration 8983 : model1 loss : 0.115199 model2 loss : 0.093895
iteration 8984 : model1 loss : 0.113588 model2 loss : 0.074225
iteration 8985 : model1 loss : 0.115432 model2 loss : 0.099393
iteration 8986 : model1 loss : 0.117419 model2 loss : 0.094957
iteration 8987 : model1 loss : 0.115368 model2 loss : 0.086096
iteration 8988 : model1 loss : 0.123443 model2 loss : 0.093961
iteration 8989 : model1 loss : 0.126004 model2 loss : 0.094262
iteration 8990 : model1 loss : 0.117455 model2 loss : 0.064901
iteration 8991 : model1 loss : 0.128265 model2 loss : 0.084367
iteration 8992 : model1 loss : 0.119185 model2 loss : 0.090755
iteration 8993 : model1 loss : 0.094970 model2 loss : 0.090592
 90%|██████████████████████████   | 529/589 [3:35:46<22:55, 22.92s/it]iteration 8994 : model1 loss : 0.104352 model2 loss : 0.086293
iteration 8995 : model1 loss : 0.133439 model2 loss : 0.084925
iteration 8996 : model1 loss : 0.123772 model2 loss : 0.120018
iteration 8997 : model1 loss : 0.161247 model2 loss : 0.108395
iteration 8998 : model1 loss : 0.119707 model2 loss : 0.100431
iteration 8999 : model1 loss : 0.131622 model2 loss : 0.115298
iteration 9000 : model1 loss : 0.129376 model2 loss : 0.090950
iteration 9000 : model1_mean_dice : 0.674235 model1_mean_hd95 : 92.056041 model1_mean_iou : 0.545294
iteration 9000 : model2_mean_dice : 0.755935 model2_mean_hd95 : 68.614519 model2_mean_iou : 0.644397
save model1 to ../model/Dermofit/Cross_Teaching_Between_CNN_Transformer_7/unet/model1_iter_9000.pth
save model2 to ../model/Dermofit/Cross_Teaching_Between_CNN_Transformer_7/unet/model2_iter_9000.pth
iteration 9001 : model1 loss : 0.105754 model2 loss : 0.084500
iteration 9002 : model1 loss : 0.118801 model2 loss : 0.090183
iteration 9003 : model1 loss : 0.121964 model2 loss : 0.095714
iteration 9004 : model1 loss : 0.106419 model2 loss : 0.076918
iteration 9005 : model1 loss : 0.121415 model2 loss : 0.079411
iteration 9006 : model1 loss : 0.099315 model2 loss : 0.085730
iteration 9007 : model1 loss : 0.114237 model2 loss : 0.081640
iteration 9008 : model1 loss : 0.110598 model2 loss : 0.072734
iteration 9009 : model1 loss : 0.091090 model2 loss : 0.072967
iteration 9010 : model1 loss : 0.178046 model2 loss : 0.124310
 90%|██████████████████████████   | 530/589 [3:36:29<28:26, 28.92s/it]iteration 9011 : model1 loss : 0.141901 model2 loss : 0.085024
iteration 9012 : model1 loss : 0.117810 model2 loss : 0.071343
iteration 9013 : model1 loss : 0.125321 model2 loss : 0.072205
iteration 9014 : model1 loss : 0.149152 model2 loss : 0.095527
iteration 9015 : model1 loss : 0.094147 model2 loss : 0.081631
iteration 9016 : model1 loss : 0.103486 model2 loss : 0.086061
iteration 9017 : model1 loss : 0.133060 model2 loss : 0.092163
iteration 9018 : model1 loss : 0.130553 model2 loss : 0.090015
iteration 9019 : model1 loss : 0.121645 model2 loss : 0.088329
iteration 9020 : model1 loss : 0.092217 model2 loss : 0.082887
iteration 9021 : model1 loss : 0.127210 model2 loss : 0.114357
iteration 9022 : model1 loss : 0.097001 model2 loss : 0.095719
iteration 9023 : model1 loss : 0.130704 model2 loss : 0.112700
iteration 9024 : model1 loss : 0.140902 model2 loss : 0.070574
iteration 9025 : model1 loss : 0.106622 model2 loss : 0.062129
iteration 9026 : model1 loss : 0.122933 model2 loss : 0.094012
iteration 9027 : model1 loss : 0.139267 model2 loss : 0.128652
 90%|██████████████████████████▏  | 531/589 [3:36:52<26:10, 27.08s/it]iteration 9028 : model1 loss : 0.129485 model2 loss : 0.104199
iteration 9029 : model1 loss : 0.112794 model2 loss : 0.082527
iteration 9030 : model1 loss : 0.106227 model2 loss : 0.068110
iteration 9031 : model1 loss : 0.133438 model2 loss : 0.099058
iteration 9032 : model1 loss : 0.101616 model2 loss : 0.084117
iteration 9033 : model1 loss : 0.143711 model2 loss : 0.089851
iteration 9034 : model1 loss : 0.186004 model2 loss : 0.124471
iteration 9035 : model1 loss : 0.117389 model2 loss : 0.092094
iteration 9036 : model1 loss : 0.111836 model2 loss : 0.095012
iteration 9037 : model1 loss : 0.128307 model2 loss : 0.080118
iteration 9038 : model1 loss : 0.118909 model2 loss : 0.072354
iteration 9039 : model1 loss : 0.107580 model2 loss : 0.080761
iteration 9040 : model1 loss : 0.114443 model2 loss : 0.090933
iteration 9041 : model1 loss : 0.124289 model2 loss : 0.091461
iteration 9042 : model1 loss : 0.090265 model2 loss : 0.055097
iteration 9043 : model1 loss : 0.132967 model2 loss : 0.113394
iteration 9044 : model1 loss : 0.138950 model2 loss : 0.118160
 90%|██████████████████████████▏  | 532/589 [3:37:15<24:29, 25.78s/it]iteration 9045 : model1 loss : 0.127248 model2 loss : 0.099312
iteration 9046 : model1 loss : 0.132392 model2 loss : 0.103455
iteration 9047 : model1 loss : 0.154851 model2 loss : 0.087368
iteration 9048 : model1 loss : 0.113181 model2 loss : 0.067126
iteration 9049 : model1 loss : 0.130376 model2 loss : 0.084185
iteration 9050 : model1 loss : 0.096372 model2 loss : 0.089536
iteration 9051 : model1 loss : 0.123563 model2 loss : 0.083585
iteration 9052 : model1 loss : 0.126020 model2 loss : 0.083126
iteration 9053 : model1 loss : 0.132331 model2 loss : 0.099304
iteration 9054 : model1 loss : 0.134954 model2 loss : 0.080085
iteration 9055 : model1 loss : 0.115120 model2 loss : 0.092748
iteration 9056 : model1 loss : 0.104822 model2 loss : 0.095384
iteration 9057 : model1 loss : 0.108455 model2 loss : 0.070598
iteration 9058 : model1 loss : 0.109964 model2 loss : 0.086882
iteration 9059 : model1 loss : 0.093329 model2 loss : 0.076268
iteration 9060 : model1 loss : 0.151972 model2 loss : 0.132643
iteration 9061 : model1 loss : 0.138131 model2 loss : 0.079253
 90%|██████████████████████████▏  | 533/589 [3:37:38<23:14, 24.90s/it]iteration 9062 : model1 loss : 0.110758 model2 loss : 0.075346
iteration 9063 : model1 loss : 0.127479 model2 loss : 0.081453
iteration 9064 : model1 loss : 0.147985 model2 loss : 0.116478
iteration 9065 : model1 loss : 0.165305 model2 loss : 0.111323
iteration 9066 : model1 loss : 0.105193 model2 loss : 0.077306
iteration 9067 : model1 loss : 0.144738 model2 loss : 0.102970
iteration 9068 : model1 loss : 0.131376 model2 loss : 0.098685
iteration 9069 : model1 loss : 0.144102 model2 loss : 0.079027
iteration 9070 : model1 loss : 0.104560 model2 loss : 0.077238
iteration 9071 : model1 loss : 0.109504 model2 loss : 0.089677
iteration 9072 : model1 loss : 0.153260 model2 loss : 0.099617
iteration 9073 : model1 loss : 0.112826 model2 loss : 0.068263
iteration 9074 : model1 loss : 0.131161 model2 loss : 0.092598
iteration 9075 : model1 loss : 0.101940 model2 loss : 0.084510
iteration 9076 : model1 loss : 0.101821 model2 loss : 0.075036
iteration 9077 : model1 loss : 0.107360 model2 loss : 0.073592
iteration 9078 : model1 loss : 0.140503 model2 loss : 0.084663
 91%|██████████████████████████▎  | 534/589 [3:38:00<22:14, 24.26s/it]iteration 9079 : model1 loss : 0.118069 model2 loss : 0.086635
iteration 9080 : model1 loss : 0.110574 model2 loss : 0.089296
iteration 9081 : model1 loss : 0.144910 model2 loss : 0.080942
iteration 9082 : model1 loss : 0.095864 model2 loss : 0.074194
iteration 9083 : model1 loss : 0.098796 model2 loss : 0.091428
iteration 9084 : model1 loss : 0.145489 model2 loss : 0.096423
iteration 9085 : model1 loss : 0.115014 model2 loss : 0.102912
iteration 9086 : model1 loss : 0.094638 model2 loss : 0.080723
iteration 9087 : model1 loss : 0.078974 model2 loss : 0.055553
iteration 9088 : model1 loss : 0.123712 model2 loss : 0.089150
iteration 9089 : model1 loss : 0.138738 model2 loss : 0.085448
iteration 9090 : model1 loss : 0.141721 model2 loss : 0.086356
iteration 9091 : model1 loss : 0.124597 model2 loss : 0.086012
iteration 9092 : model1 loss : 0.105860 model2 loss : 0.079179
iteration 9093 : model1 loss : 0.125367 model2 loss : 0.088834
iteration 9094 : model1 loss : 0.097217 model2 loss : 0.100777
iteration 9095 : model1 loss : 0.134812 model2 loss : 0.080944
 91%|██████████████████████████▎  | 535/589 [3:38:23<21:26, 23.82s/it]iteration 9096 : model1 loss : 0.129191 model2 loss : 0.105313
iteration 9097 : model1 loss : 0.136516 model2 loss : 0.100217
iteration 9098 : model1 loss : 0.115126 model2 loss : 0.098848
iteration 9099 : model1 loss : 0.109111 model2 loss : 0.085545
iteration 9100 : model1 loss : 0.126155 model2 loss : 0.077876
iteration 9101 : model1 loss : 0.091077 model2 loss : 0.097877
iteration 9102 : model1 loss : 0.115584 model2 loss : 0.095340
iteration 9103 : model1 loss : 0.128554 model2 loss : 0.080304
iteration 9104 : model1 loss : 0.123246 model2 loss : 0.078975
iteration 9105 : model1 loss : 0.120415 model2 loss : 0.083230
iteration 9106 : model1 loss : 0.112532 model2 loss : 0.079912
iteration 9107 : model1 loss : 0.118982 model2 loss : 0.093259
iteration 9108 : model1 loss : 0.167938 model2 loss : 0.094791
iteration 9109 : model1 loss : 0.131908 model2 loss : 0.089115
iteration 9110 : model1 loss : 0.114598 model2 loss : 0.075403
iteration 9111 : model1 loss : 0.079905 model2 loss : 0.092892
iteration 9112 : model1 loss : 0.131480 model2 loss : 0.085329
 91%|██████████████████████████▍  | 536/589 [3:38:46<20:47, 23.54s/it]iteration 9113 : model1 loss : 0.117609 model2 loss : 0.079838
iteration 9114 : model1 loss : 0.128873 model2 loss : 0.112787
iteration 9115 : model1 loss : 0.104896 model2 loss : 0.078347
iteration 9116 : model1 loss : 0.144713 model2 loss : 0.086998
iteration 9117 : model1 loss : 0.105685 model2 loss : 0.078799
iteration 9118 : model1 loss : 0.108434 model2 loss : 0.096168
iteration 9119 : model1 loss : 0.119348 model2 loss : 0.082163
iteration 9120 : model1 loss : 0.161269 model2 loss : 0.095918
iteration 9121 : model1 loss : 0.142946 model2 loss : 0.096788
iteration 9122 : model1 loss : 0.091626 model2 loss : 0.063546
iteration 9123 : model1 loss : 0.110941 model2 loss : 0.067341
iteration 9124 : model1 loss : 0.083151 model2 loss : 0.076003
iteration 9125 : model1 loss : 0.151638 model2 loss : 0.106059
iteration 9126 : model1 loss : 0.128287 model2 loss : 0.109335
iteration 9127 : model1 loss : 0.126667 model2 loss : 0.088296
iteration 9128 : model1 loss : 0.069716 model2 loss : 0.078518
iteration 9129 : model1 loss : 0.092722 model2 loss : 0.084443
 91%|██████████████████████████▍  | 537/589 [3:39:09<20:11, 23.30s/it]iteration 9130 : model1 loss : 0.098787 model2 loss : 0.069235
iteration 9131 : model1 loss : 0.120367 model2 loss : 0.088912
iteration 9132 : model1 loss : 0.114190 model2 loss : 0.077010
iteration 9133 : model1 loss : 0.154116 model2 loss : 0.095655
iteration 9134 : model1 loss : 0.102933 model2 loss : 0.088583
iteration 9135 : model1 loss : 0.084507 model2 loss : 0.082113
iteration 9136 : model1 loss : 0.168282 model2 loss : 0.085523
iteration 9137 : model1 loss : 0.171327 model2 loss : 0.097777
iteration 9138 : model1 loss : 0.134099 model2 loss : 0.087779
iteration 9139 : model1 loss : 0.109529 model2 loss : 0.071408
iteration 9140 : model1 loss : 0.122712 model2 loss : 0.101555
iteration 9141 : model1 loss : 0.122333 model2 loss : 0.096188
iteration 9142 : model1 loss : 0.120652 model2 loss : 0.102357
iteration 9143 : model1 loss : 0.125956 model2 loss : 0.096407
iteration 9144 : model1 loss : 0.120077 model2 loss : 0.104093
iteration 9145 : model1 loss : 0.074424 model2 loss : 0.065474
iteration 9146 : model1 loss : 0.117894 model2 loss : 0.088313
 91%|██████████████████████████▍  | 538/589 [3:39:32<19:40, 23.15s/it]iteration 9147 : model1 loss : 0.117040 model2 loss : 0.078590
iteration 9148 : model1 loss : 0.113058 model2 loss : 0.078728
iteration 9149 : model1 loss : 0.149625 model2 loss : 0.083563
iteration 9150 : model1 loss : 0.109224 model2 loss : 0.053392
iteration 9151 : model1 loss : 0.093340 model2 loss : 0.081084
iteration 9152 : model1 loss : 0.128067 model2 loss : 0.108671
iteration 9153 : model1 loss : 0.145499 model2 loss : 0.071073
iteration 9154 : model1 loss : 0.119838 model2 loss : 0.085587
iteration 9155 : model1 loss : 0.118250 model2 loss : 0.088822
iteration 9156 : model1 loss : 0.100766 model2 loss : 0.132924
iteration 9157 : model1 loss : 0.116125 model2 loss : 0.076990
iteration 9158 : model1 loss : 0.113653 model2 loss : 0.090646
iteration 9159 : model1 loss : 0.133828 model2 loss : 0.093566
iteration 9160 : model1 loss : 0.125876 model2 loss : 0.104384
iteration 9161 : model1 loss : 0.113362 model2 loss : 0.115737
iteration 9162 : model1 loss : 0.098896 model2 loss : 0.068731
iteration 9163 : model1 loss : 0.113825 model2 loss : 0.093307
 92%|██████████████████████████▌  | 539/589 [3:39:54<19:13, 23.08s/it]iteration 9164 : model1 loss : 0.144936 model2 loss : 0.086906
iteration 9165 : model1 loss : 0.147482 model2 loss : 0.079349
iteration 9166 : model1 loss : 0.170181 model2 loss : 0.111847
iteration 9167 : model1 loss : 0.112172 model2 loss : 0.112873
iteration 9168 : model1 loss : 0.105570 model2 loss : 0.091769
iteration 9169 : model1 loss : 0.104565 model2 loss : 0.080484
iteration 9170 : model1 loss : 0.142626 model2 loss : 0.111042
iteration 9171 : model1 loss : 0.129922 model2 loss : 0.095866
iteration 9172 : model1 loss : 0.171842 model2 loss : 0.096273
iteration 9173 : model1 loss : 0.115915 model2 loss : 0.102677
iteration 9174 : model1 loss : 0.134362 model2 loss : 0.117573
iteration 9175 : model1 loss : 0.086050 model2 loss : 0.079785
iteration 9176 : model1 loss : 0.132137 model2 loss : 0.082782
iteration 9177 : model1 loss : 0.117181 model2 loss : 0.065065
iteration 9178 : model1 loss : 0.131277 model2 loss : 0.091241
iteration 9179 : model1 loss : 0.083780 model2 loss : 0.098917
iteration 9180 : model1 loss : 0.120443 model2 loss : 0.098721
 92%|██████████████████████████▌  | 540/589 [3:40:17<18:46, 23.00s/it]iteration 9181 : model1 loss : 0.149890 model2 loss : 0.093427
iteration 9182 : model1 loss : 0.164404 model2 loss : 0.058232
iteration 9183 : model1 loss : 0.111036 model2 loss : 0.063919
iteration 9184 : model1 loss : 0.150070 model2 loss : 0.088278
iteration 9185 : model1 loss : 0.121591 model2 loss : 0.093791
iteration 9186 : model1 loss : 0.087280 model2 loss : 0.071449
iteration 9187 : model1 loss : 0.120927 model2 loss : 0.080967
iteration 9188 : model1 loss : 0.106030 model2 loss : 0.072147
iteration 9189 : model1 loss : 0.123855 model2 loss : 0.091236
iteration 9190 : model1 loss : 0.099724 model2 loss : 0.086290
iteration 9191 : model1 loss : 0.146458 model2 loss : 0.085383
iteration 9192 : model1 loss : 0.116158 model2 loss : 0.118763
iteration 9193 : model1 loss : 0.167375 model2 loss : 0.078549
iteration 9194 : model1 loss : 0.129867 model2 loss : 0.088727
iteration 9195 : model1 loss : 0.083932 model2 loss : 0.057449
iteration 9196 : model1 loss : 0.119179 model2 loss : 0.089517
iteration 9197 : model1 loss : 0.125614 model2 loss : 0.100437
 92%|██████████████████████████▋  | 541/589 [3:40:40<18:21, 22.95s/it]iteration 9198 : model1 loss : 0.159840 model2 loss : 0.092401
iteration 9199 : model1 loss : 0.136174 model2 loss : 0.110447
iteration 9200 : model1 loss : 0.128472 model2 loss : 0.070340
iteration 9200 : model1_mean_dice : 0.678267 model1_mean_hd95 : 91.729089 model1_mean_iou : 0.551022
iteration 9200 : model2_mean_dice : 0.746945 model2_mean_hd95 : 71.646005 model2_mean_iou : 0.634837
iteration 9201 : model1 loss : 0.127906 model2 loss : 0.094472
iteration 9202 : model1 loss : 0.093978 model2 loss : 0.086598
iteration 9203 : model1 loss : 0.110349 model2 loss : 0.078956
iteration 9204 : model1 loss : 0.100646 model2 loss : 0.068721
iteration 9205 : model1 loss : 0.126545 model2 loss : 0.078647
iteration 9206 : model1 loss : 0.127803 model2 loss : 0.085217
iteration 9207 : model1 loss : 0.147101 model2 loss : 0.084988
iteration 9208 : model1 loss : 0.128707 model2 loss : 0.100364
iteration 9209 : model1 loss : 0.124785 model2 loss : 0.115214
iteration 9210 : model1 loss : 0.087564 model2 loss : 0.065628
iteration 9211 : model1 loss : 0.101496 model2 loss : 0.098165
iteration 9212 : model1 loss : 0.109835 model2 loss : 0.063306
iteration 9213 : model1 loss : 0.116272 model2 loss : 0.070605
iteration 9214 : model1 loss : 0.162666 model2 loss : 0.104322
 92%|██████████████████████████▋  | 542/589 [3:41:23<22:36, 28.86s/it]iteration 9215 : model1 loss : 0.127004 model2 loss : 0.071613
iteration 9216 : model1 loss : 0.100287 model2 loss : 0.086925
iteration 9217 : model1 loss : 0.175859 model2 loss : 0.106398
iteration 9218 : model1 loss : 0.092017 model2 loss : 0.082454
iteration 9219 : model1 loss : 0.148668 model2 loss : 0.088788
iteration 9220 : model1 loss : 0.166774 model2 loss : 0.084334
iteration 9221 : model1 loss : 0.087912 model2 loss : 0.071824
iteration 9222 : model1 loss : 0.105901 model2 loss : 0.059821
iteration 9223 : model1 loss : 0.139256 model2 loss : 0.130558
iteration 9224 : model1 loss : 0.110350 model2 loss : 0.063542
iteration 9225 : model1 loss : 0.122065 model2 loss : 0.081463
iteration 9226 : model1 loss : 0.095604 model2 loss : 0.062684
iteration 9227 : model1 loss : 0.124597 model2 loss : 0.108384
iteration 9228 : model1 loss : 0.108500 model2 loss : 0.091227
iteration 9229 : model1 loss : 0.103584 model2 loss : 0.068252
iteration 9230 : model1 loss : 0.138738 model2 loss : 0.104911
iteration 9231 : model1 loss : 0.121252 model2 loss : 0.077693
 92%|██████████████████████████▋  | 543/589 [3:41:46<20:43, 27.03s/it]iteration 9232 : model1 loss : 0.132275 model2 loss : 0.096763
iteration 9233 : model1 loss : 0.165710 model2 loss : 0.067833
iteration 9234 : model1 loss : 0.112654 model2 loss : 0.059666
iteration 9235 : model1 loss : 0.123828 model2 loss : 0.102962
iteration 9236 : model1 loss : 0.111251 model2 loss : 0.071429
iteration 9237 : model1 loss : 0.145715 model2 loss : 0.100788
iteration 9238 : model1 loss : 0.114057 model2 loss : 0.086913
iteration 9239 : model1 loss : 0.095094 model2 loss : 0.084615
iteration 9240 : model1 loss : 0.126657 model2 loss : 0.106941
iteration 9241 : model1 loss : 0.124639 model2 loss : 0.086313
iteration 9242 : model1 loss : 0.097099 model2 loss : 0.080778
iteration 9243 : model1 loss : 0.112248 model2 loss : 0.122472
iteration 9244 : model1 loss : 0.093911 model2 loss : 0.077743
iteration 9245 : model1 loss : 0.133787 model2 loss : 0.060905
iteration 9246 : model1 loss : 0.097844 model2 loss : 0.090637
iteration 9247 : model1 loss : 0.114784 model2 loss : 0.093196
iteration 9248 : model1 loss : 0.130308 model2 loss : 0.074360
 92%|██████████████████████████▊  | 544/589 [3:42:08<19:18, 25.75s/it]iteration 9249 : model1 loss : 0.115872 model2 loss : 0.084895
iteration 9250 : model1 loss : 0.112159 model2 loss : 0.093585
iteration 9251 : model1 loss : 0.137109 model2 loss : 0.072238
iteration 9252 : model1 loss : 0.141182 model2 loss : 0.084827
iteration 9253 : model1 loss : 0.120237 model2 loss : 0.080831
iteration 9254 : model1 loss : 0.092973 model2 loss : 0.081515
iteration 9255 : model1 loss : 0.167032 model2 loss : 0.060989
iteration 9256 : model1 loss : 0.109578 model2 loss : 0.071412
iteration 9257 : model1 loss : 0.111252 model2 loss : 0.101335
iteration 9258 : model1 loss : 0.108537 model2 loss : 0.084143
iteration 9259 : model1 loss : 0.144817 model2 loss : 0.118497
iteration 9260 : model1 loss : 0.129269 model2 loss : 0.065429
iteration 9261 : model1 loss : 0.127796 model2 loss : 0.074285
iteration 9262 : model1 loss : 0.120778 model2 loss : 0.096732
iteration 9263 : model1 loss : 0.103659 model2 loss : 0.094300
iteration 9264 : model1 loss : 0.155324 model2 loss : 0.127808
iteration 9265 : model1 loss : 0.108245 model2 loss : 0.073922
 93%|██████████████████████████▊  | 545/589 [3:42:31<18:15, 24.89s/it]iteration 9266 : model1 loss : 0.114273 model2 loss : 0.102730
iteration 9267 : model1 loss : 0.123208 model2 loss : 0.072154
iteration 9268 : model1 loss : 0.155185 model2 loss : 0.096904
iteration 9269 : model1 loss : 0.140697 model2 loss : 0.075432
iteration 9270 : model1 loss : 0.095134 model2 loss : 0.071602
iteration 9271 : model1 loss : 0.133405 model2 loss : 0.111667
iteration 9272 : model1 loss : 0.096831 model2 loss : 0.067560
iteration 9273 : model1 loss : 0.105512 model2 loss : 0.071769
iteration 9274 : model1 loss : 0.111359 model2 loss : 0.084227
iteration 9275 : model1 loss : 0.101406 model2 loss : 0.068271
iteration 9276 : model1 loss : 0.104039 model2 loss : 0.085284
iteration 9277 : model1 loss : 0.098321 model2 loss : 0.135834
iteration 9278 : model1 loss : 0.121505 model2 loss : 0.084694
iteration 9279 : model1 loss : 0.112121 model2 loss : 0.087088
iteration 9280 : model1 loss : 0.135069 model2 loss : 0.070076
iteration 9281 : model1 loss : 0.122256 model2 loss : 0.085656
iteration 9282 : model1 loss : 0.115427 model2 loss : 0.062679
 93%|██████████████████████████▉  | 546/589 [3:42:54<17:23, 24.26s/it]iteration 9283 : model1 loss : 0.156768 model2 loss : 0.076363
iteration 9284 : model1 loss : 0.109699 model2 loss : 0.079571
iteration 9285 : model1 loss : 0.151764 model2 loss : 0.089700
iteration 9286 : model1 loss : 0.121538 model2 loss : 0.097712
iteration 9287 : model1 loss : 0.123498 model2 loss : 0.088169
iteration 9288 : model1 loss : 0.099444 model2 loss : 0.074138
iteration 9289 : model1 loss : 0.124756 model2 loss : 0.069468
iteration 9290 : model1 loss : 0.130242 model2 loss : 0.093957
iteration 9291 : model1 loss : 0.157935 model2 loss : 0.099335
iteration 9292 : model1 loss : 0.094480 model2 loss : 0.064877
iteration 9293 : model1 loss : 0.122905 model2 loss : 0.089892
iteration 9294 : model1 loss : 0.117013 model2 loss : 0.094020
iteration 9295 : model1 loss : 0.121203 model2 loss : 0.073324
iteration 9296 : model1 loss : 0.118016 model2 loss : 0.077778
iteration 9297 : model1 loss : 0.110483 model2 loss : 0.085221
iteration 9298 : model1 loss : 0.144848 model2 loss : 0.109595
iteration 9299 : model1 loss : 0.123078 model2 loss : 0.078273
 93%|██████████████████████████▉  | 547/589 [3:43:17<16:40, 23.82s/it]iteration 9300 : model1 loss : 0.145227 model2 loss : 0.064725
iteration 9301 : model1 loss : 0.106101 model2 loss : 0.065824
iteration 9302 : model1 loss : 0.128513 model2 loss : 0.098400
iteration 9303 : model1 loss : 0.127803 model2 loss : 0.086590
iteration 9304 : model1 loss : 0.102548 model2 loss : 0.075283
iteration 9305 : model1 loss : 0.128003 model2 loss : 0.094046
iteration 9306 : model1 loss : 0.133299 model2 loss : 0.082840
iteration 9307 : model1 loss : 0.124454 model2 loss : 0.100568
iteration 9308 : model1 loss : 0.109918 model2 loss : 0.067342
iteration 9309 : model1 loss : 0.119292 model2 loss : 0.099726
iteration 9310 : model1 loss : 0.097614 model2 loss : 0.076527
iteration 9311 : model1 loss : 0.103224 model2 loss : 0.082929
iteration 9312 : model1 loss : 0.128240 model2 loss : 0.086059
iteration 9313 : model1 loss : 0.112018 model2 loss : 0.068972
iteration 9314 : model1 loss : 0.101759 model2 loss : 0.119053
iteration 9315 : model1 loss : 0.145562 model2 loss : 0.090808
iteration 9316 : model1 loss : 0.112596 model2 loss : 0.093181
 93%|██████████████████████████▉  | 548/589 [3:43:40<16:06, 23.57s/it]iteration 9317 : model1 loss : 0.112886 model2 loss : 0.093177
iteration 9318 : model1 loss : 0.131057 model2 loss : 0.075622
iteration 9319 : model1 loss : 0.104815 model2 loss : 0.072549
iteration 9320 : model1 loss : 0.121683 model2 loss : 0.073228
iteration 9321 : model1 loss : 0.103355 model2 loss : 0.076729
iteration 9322 : model1 loss : 0.119743 model2 loss : 0.078432
iteration 9323 : model1 loss : 0.212221 model2 loss : 0.107369
iteration 9324 : model1 loss : 0.109674 model2 loss : 0.066329
iteration 9325 : model1 loss : 0.118624 model2 loss : 0.087295
iteration 9326 : model1 loss : 0.103734 model2 loss : 0.080786
iteration 9327 : model1 loss : 0.120445 model2 loss : 0.093941
iteration 9328 : model1 loss : 0.112547 model2 loss : 0.066762
iteration 9329 : model1 loss : 0.135021 model2 loss : 0.071246
iteration 9330 : model1 loss : 0.113969 model2 loss : 0.071146
iteration 9331 : model1 loss : 0.094994 model2 loss : 0.067501
iteration 9332 : model1 loss : 0.141289 model2 loss : 0.124256
iteration 9333 : model1 loss : 0.107124 model2 loss : 0.090392
 93%|███████████████████████████  | 549/589 [3:44:03<15:33, 23.34s/it]iteration 9334 : model1 loss : 0.144358 model2 loss : 0.119783
iteration 9335 : model1 loss : 0.139485 model2 loss : 0.090437
iteration 9336 : model1 loss : 0.158499 model2 loss : 0.070795
iteration 9337 : model1 loss : 0.108124 model2 loss : 0.071088
iteration 9338 : model1 loss : 0.140659 model2 loss : 0.082910
iteration 9339 : model1 loss : 0.123235 model2 loss : 0.080156
iteration 9340 : model1 loss : 0.174907 model2 loss : 0.119658
iteration 9341 : model1 loss : 0.109469 model2 loss : 0.101263
iteration 9342 : model1 loss : 0.131206 model2 loss : 0.102167
iteration 9343 : model1 loss : 0.125539 model2 loss : 0.079955
iteration 9344 : model1 loss : 0.095187 model2 loss : 0.105054
iteration 9345 : model1 loss : 0.109259 model2 loss : 0.078651
iteration 9346 : model1 loss : 0.164531 model2 loss : 0.076205
iteration 9347 : model1 loss : 0.108084 model2 loss : 0.080035
iteration 9348 : model1 loss : 0.108770 model2 loss : 0.093178
iteration 9349 : model1 loss : 0.086978 model2 loss : 0.061831
iteration 9350 : model1 loss : 0.130013 model2 loss : 0.096979
 93%|███████████████████████████  | 550/589 [3:44:25<15:04, 23.19s/it]iteration 9351 : model1 loss : 0.139027 model2 loss : 0.103566
iteration 9352 : model1 loss : 0.091440 model2 loss : 0.062662
iteration 9353 : model1 loss : 0.140457 model2 loss : 0.113027
iteration 9354 : model1 loss : 0.112338 model2 loss : 0.081243
iteration 9355 : model1 loss : 0.111418 model2 loss : 0.109155
iteration 9356 : model1 loss : 0.107668 model2 loss : 0.072244
iteration 9357 : model1 loss : 0.135704 model2 loss : 0.095867
iteration 9358 : model1 loss : 0.124945 model2 loss : 0.093891
iteration 9359 : model1 loss : 0.142766 model2 loss : 0.106759
iteration 9360 : model1 loss : 0.119527 model2 loss : 0.088867
iteration 9361 : model1 loss : 0.096206 model2 loss : 0.077221
iteration 9362 : model1 loss : 0.101631 model2 loss : 0.090438
iteration 9363 : model1 loss : 0.106576 model2 loss : 0.112928
iteration 9364 : model1 loss : 0.090534 model2 loss : 0.063776
iteration 9365 : model1 loss : 0.097604 model2 loss : 0.074347
iteration 9366 : model1 loss : 0.094700 model2 loss : 0.092712
iteration 9367 : model1 loss : 0.138195 model2 loss : 0.120988
 94%|███████████████████████████▏ | 551/589 [3:44:48<14:36, 23.06s/it]iteration 9368 : model1 loss : 0.134311 model2 loss : 0.102246
iteration 9369 : model1 loss : 0.108504 model2 loss : 0.059005
iteration 9370 : model1 loss : 0.156522 model2 loss : 0.118869
iteration 9371 : model1 loss : 0.116361 model2 loss : 0.082441
iteration 9372 : model1 loss : 0.120634 model2 loss : 0.093148
iteration 9373 : model1 loss : 0.111135 model2 loss : 0.086247
iteration 9374 : model1 loss : 0.099175 model2 loss : 0.078776
iteration 9375 : model1 loss : 0.139431 model2 loss : 0.137083
iteration 9376 : model1 loss : 0.103097 model2 loss : 0.088186
iteration 9377 : model1 loss : 0.092944 model2 loss : 0.067466
iteration 9378 : model1 loss : 0.162747 model2 loss : 0.108892
iteration 9379 : model1 loss : 0.115963 model2 loss : 0.076045
iteration 9380 : model1 loss : 0.114246 model2 loss : 0.091287
iteration 9381 : model1 loss : 0.127820 model2 loss : 0.082116
iteration 9382 : model1 loss : 0.090528 model2 loss : 0.087635
iteration 9383 : model1 loss : 0.114170 model2 loss : 0.119336
iteration 9384 : model1 loss : 0.115472 model2 loss : 0.086214
 94%|███████████████████████████▏ | 552/589 [3:45:11<14:09, 22.97s/it]iteration 9385 : model1 loss : 0.089012 model2 loss : 0.090090
iteration 9386 : model1 loss : 0.117509 model2 loss : 0.084570
iteration 9387 : model1 loss : 0.131372 model2 loss : 0.090969
iteration 9388 : model1 loss : 0.098461 model2 loss : 0.085897
iteration 9389 : model1 loss : 0.124273 model2 loss : 0.099276
iteration 9390 : model1 loss : 0.121832 model2 loss : 0.083150
iteration 9391 : model1 loss : 0.094844 model2 loss : 0.064609
iteration 9392 : model1 loss : 0.108442 model2 loss : 0.096859
iteration 9393 : model1 loss : 0.100153 model2 loss : 0.073669
iteration 9394 : model1 loss : 0.136972 model2 loss : 0.092303
iteration 9395 : model1 loss : 0.110994 model2 loss : 0.075593
iteration 9396 : model1 loss : 0.151014 model2 loss : 0.090531
iteration 9397 : model1 loss : 0.094464 model2 loss : 0.062943
iteration 9398 : model1 loss : 0.114009 model2 loss : 0.068194
iteration 9399 : model1 loss : 0.118990 model2 loss : 0.120469
iteration 9400 : model1 loss : 0.139191 model2 loss : 0.118304
iteration 9400 : model1_mean_dice : 0.678592 model1_mean_hd95 : 92.616089 model1_mean_iou : 0.551131
iteration 9400 : model2_mean_dice : 0.746909 model2_mean_hd95 : 71.764387 model2_mean_iou : 0.634038
iteration 9401 : model1 loss : 0.101622 model2 loss : 0.070989
 94%|███████████████████████████▏ | 553/589 [3:45:54<17:21, 28.94s/it]iteration 9402 : model1 loss : 0.097042 model2 loss : 0.085577
iteration 9403 : model1 loss : 0.169046 model2 loss : 0.080703
iteration 9404 : model1 loss : 0.132446 model2 loss : 0.072080
iteration 9405 : model1 loss : 0.086861 model2 loss : 0.052881
iteration 9406 : model1 loss : 0.142792 model2 loss : 0.097645
iteration 9407 : model1 loss : 0.138045 model2 loss : 0.103260
iteration 9408 : model1 loss : 0.131541 model2 loss : 0.092393
iteration 9409 : model1 loss : 0.104832 model2 loss : 0.083039
iteration 9410 : model1 loss : 0.105624 model2 loss : 0.136581
iteration 9411 : model1 loss : 0.102855 model2 loss : 0.075020
iteration 9412 : model1 loss : 0.114962 model2 loss : 0.081352
iteration 9413 : model1 loss : 0.115408 model2 loss : 0.081325
iteration 9414 : model1 loss : 0.106727 model2 loss : 0.085062
iteration 9415 : model1 loss : 0.122882 model2 loss : 0.086618
iteration 9416 : model1 loss : 0.114275 model2 loss : 0.103839
iteration 9417 : model1 loss : 0.111645 model2 loss : 0.088800
iteration 9418 : model1 loss : 0.127370 model2 loss : 0.087874
 94%|███████████████████████████▎ | 554/589 [3:46:17<15:47, 27.07s/it]iteration 9419 : model1 loss : 0.094623 model2 loss : 0.080871
iteration 9420 : model1 loss : 0.149359 model2 loss : 0.106451
iteration 9421 : model1 loss : 0.142473 model2 loss : 0.080561
iteration 9422 : model1 loss : 0.121928 model2 loss : 0.079404
iteration 9423 : model1 loss : 0.096559 model2 loss : 0.068910
iteration 9424 : model1 loss : 0.102628 model2 loss : 0.077016
iteration 9425 : model1 loss : 0.118361 model2 loss : 0.089713
iteration 9426 : model1 loss : 0.107337 model2 loss : 0.090921
iteration 9427 : model1 loss : 0.113156 model2 loss : 0.087841
iteration 9428 : model1 loss : 0.122502 model2 loss : 0.093090
iteration 9429 : model1 loss : 0.161617 model2 loss : 0.083079
iteration 9430 : model1 loss : 0.123555 model2 loss : 0.090332
iteration 9431 : model1 loss : 0.159524 model2 loss : 0.088171
iteration 9432 : model1 loss : 0.111832 model2 loss : 0.144339
iteration 9433 : model1 loss : 0.107073 model2 loss : 0.058605
iteration 9434 : model1 loss : 0.123516 model2 loss : 0.081184
iteration 9435 : model1 loss : 0.152508 model2 loss : 0.076547
 94%|███████████████████████████▎ | 555/589 [3:46:39<14:36, 25.79s/it]iteration 9436 : model1 loss : 0.138547 model2 loss : 0.098353
iteration 9437 : model1 loss : 0.153822 model2 loss : 0.084694
iteration 9438 : model1 loss : 0.145241 model2 loss : 0.102243
iteration 9439 : model1 loss : 0.116524 model2 loss : 0.081321
iteration 9440 : model1 loss : 0.110994 model2 loss : 0.088068
iteration 9441 : model1 loss : 0.118104 model2 loss : 0.101791
iteration 9442 : model1 loss : 0.121344 model2 loss : 0.075897
iteration 9443 : model1 loss : 0.108492 model2 loss : 0.099903
iteration 9444 : model1 loss : 0.147123 model2 loss : 0.092037
iteration 9445 : model1 loss : 0.104944 model2 loss : 0.109232
iteration 9446 : model1 loss : 0.144705 model2 loss : 0.097958
iteration 9447 : model1 loss : 0.093150 model2 loss : 0.078791
iteration 9448 : model1 loss : 0.090851 model2 loss : 0.072126
iteration 9449 : model1 loss : 0.138490 model2 loss : 0.074118
iteration 9450 : model1 loss : 0.127316 model2 loss : 0.091573
iteration 9451 : model1 loss : 0.106367 model2 loss : 0.071916
iteration 9452 : model1 loss : 0.103401 model2 loss : 0.071031
 94%|███████████████████████████▍ | 556/589 [3:47:02<13:42, 24.92s/it]iteration 9453 : model1 loss : 0.116281 model2 loss : 0.076710
iteration 9454 : model1 loss : 0.116630 model2 loss : 0.064695
iteration 9455 : model1 loss : 0.120668 model2 loss : 0.098077
iteration 9456 : model1 loss : 0.126873 model2 loss : 0.073172
iteration 9457 : model1 loss : 0.098220 model2 loss : 0.101162
iteration 9458 : model1 loss : 0.113437 model2 loss : 0.081156
iteration 9459 : model1 loss : 0.129539 model2 loss : 0.089677
iteration 9460 : model1 loss : 0.133369 model2 loss : 0.121520
iteration 9461 : model1 loss : 0.097011 model2 loss : 0.060600
iteration 9462 : model1 loss : 0.090361 model2 loss : 0.070752
iteration 9463 : model1 loss : 0.110475 model2 loss : 0.075186
iteration 9464 : model1 loss : 0.123684 model2 loss : 0.082898
iteration 9465 : model1 loss : 0.123818 model2 loss : 0.069613
iteration 9466 : model1 loss : 0.128030 model2 loss : 0.091271
iteration 9467 : model1 loss : 0.134755 model2 loss : 0.106866
iteration 9468 : model1 loss : 0.100881 model2 loss : 0.059856
iteration 9469 : model1 loss : 0.141710 model2 loss : 0.106066
 95%|███████████████████████████▍ | 557/589 [3:47:25<12:56, 24.28s/it]iteration 9470 : model1 loss : 0.119462 model2 loss : 0.057568
iteration 9471 : model1 loss : 0.118686 model2 loss : 0.074421
iteration 9472 : model1 loss : 0.117628 model2 loss : 0.090563
iteration 9473 : model1 loss : 0.147080 model2 loss : 0.088492
iteration 9474 : model1 loss : 0.111044 model2 loss : 0.129790
iteration 9475 : model1 loss : 0.107594 model2 loss : 0.098644
iteration 9476 : model1 loss : 0.109292 model2 loss : 0.081899
iteration 9477 : model1 loss : 0.156432 model2 loss : 0.095209
iteration 9478 : model1 loss : 0.121991 model2 loss : 0.061005
iteration 9479 : model1 loss : 0.139489 model2 loss : 0.103275
iteration 9480 : model1 loss : 0.122731 model2 loss : 0.079568
iteration 9481 : model1 loss : 0.099172 model2 loss : 0.108564
iteration 9482 : model1 loss : 0.104973 model2 loss : 0.099717
iteration 9483 : model1 loss : 0.096329 model2 loss : 0.072431
iteration 9484 : model1 loss : 0.106742 model2 loss : 0.071681
iteration 9485 : model1 loss : 0.111951 model2 loss : 0.104427
iteration 9486 : model1 loss : 0.140678 model2 loss : 0.099600
 95%|███████████████████████████▍ | 558/589 [3:47:48<12:18, 23.82s/it]iteration 9487 : model1 loss : 0.099749 model2 loss : 0.080024
iteration 9488 : model1 loss : 0.125494 model2 loss : 0.094218
iteration 9489 : model1 loss : 0.117524 model2 loss : 0.063895
iteration 9490 : model1 loss : 0.127453 model2 loss : 0.082509
iteration 9491 : model1 loss : 0.112154 model2 loss : 0.090042
iteration 9492 : model1 loss : 0.109582 model2 loss : 0.087386
iteration 9493 : model1 loss : 0.148280 model2 loss : 0.089075
iteration 9494 : model1 loss : 0.145411 model2 loss : 0.118015
iteration 9495 : model1 loss : 0.121486 model2 loss : 0.080105
iteration 9496 : model1 loss : 0.088803 model2 loss : 0.077428
iteration 9497 : model1 loss : 0.125616 model2 loss : 0.079642
iteration 9498 : model1 loss : 0.181530 model2 loss : 0.115834
iteration 9499 : model1 loss : 0.106896 model2 loss : 0.082911
iteration 9500 : model1 loss : 0.139854 model2 loss : 0.080795
iteration 9501 : model1 loss : 0.127472 model2 loss : 0.085227
iteration 9502 : model1 loss : 0.106887 model2 loss : 0.072266
iteration 9503 : model1 loss : 0.116707 model2 loss : 0.096033
 95%|███████████████████████████▌ | 559/589 [3:48:11<11:46, 23.54s/it]iteration 9504 : model1 loss : 0.114238 model2 loss : 0.063702
iteration 9505 : model1 loss : 0.106269 model2 loss : 0.072440
iteration 9506 : model1 loss : 0.113558 model2 loss : 0.085809
iteration 9507 : model1 loss : 0.135819 model2 loss : 0.076430
iteration 9508 : model1 loss : 0.105290 model2 loss : 0.066838
iteration 9509 : model1 loss : 0.134021 model2 loss : 0.099792
iteration 9510 : model1 loss : 0.142140 model2 loss : 0.093470
iteration 9511 : model1 loss : 0.147387 model2 loss : 0.087355
iteration 9512 : model1 loss : 0.086547 model2 loss : 0.073490
iteration 9513 : model1 loss : 0.152341 model2 loss : 0.150309
iteration 9514 : model1 loss : 0.142001 model2 loss : 0.090045
iteration 9515 : model1 loss : 0.120151 model2 loss : 0.072111
iteration 9516 : model1 loss : 0.092598 model2 loss : 0.089965
iteration 9517 : model1 loss : 0.147753 model2 loss : 0.087788
iteration 9518 : model1 loss : 0.101059 model2 loss : 0.102647
iteration 9519 : model1 loss : 0.142432 model2 loss : 0.083949
iteration 9520 : model1 loss : 0.096814 model2 loss : 0.056877
 95%|███████████████████████████▌ | 560/589 [3:48:33<11:16, 23.32s/it]iteration 9521 : model1 loss : 0.127930 model2 loss : 0.071532
iteration 9522 : model1 loss : 0.136595 model2 loss : 0.074758
iteration 9523 : model1 loss : 0.123178 model2 loss : 0.082272
iteration 9524 : model1 loss : 0.140767 model2 loss : 0.080410
iteration 9525 : model1 loss : 0.097952 model2 loss : 0.093673
iteration 9526 : model1 loss : 0.130269 model2 loss : 0.085378
iteration 9527 : model1 loss : 0.104581 model2 loss : 0.064891
iteration 9528 : model1 loss : 0.088105 model2 loss : 0.062237
iteration 9529 : model1 loss : 0.105456 model2 loss : 0.084476
iteration 9530 : model1 loss : 0.125222 model2 loss : 0.083028
iteration 9531 : model1 loss : 0.162526 model2 loss : 0.128501
iteration 9532 : model1 loss : 0.088700 model2 loss : 0.091198
iteration 9533 : model1 loss : 0.099148 model2 loss : 0.086728
iteration 9534 : model1 loss : 0.136331 model2 loss : 0.079067
iteration 9535 : model1 loss : 0.111891 model2 loss : 0.112099
iteration 9536 : model1 loss : 0.133792 model2 loss : 0.106383
iteration 9537 : model1 loss : 0.116821 model2 loss : 0.081265
 95%|███████████████████████████▌ | 561/589 [3:48:56<10:48, 23.16s/it]iteration 9538 : model1 loss : 0.122142 model2 loss : 0.086575
iteration 9539 : model1 loss : 0.155047 model2 loss : 0.098210
iteration 9540 : model1 loss : 0.115613 model2 loss : 0.071536
iteration 9541 : model1 loss : 0.096835 model2 loss : 0.052741
iteration 9542 : model1 loss : 0.125017 model2 loss : 0.115502
iteration 9543 : model1 loss : 0.138095 model2 loss : 0.087073
iteration 9544 : model1 loss : 0.118710 model2 loss : 0.078032
iteration 9545 : model1 loss : 0.127597 model2 loss : 0.105794
iteration 9546 : model1 loss : 0.135712 model2 loss : 0.104719
iteration 9547 : model1 loss : 0.111716 model2 loss : 0.086790
iteration 9548 : model1 loss : 0.116984 model2 loss : 0.092933
iteration 9549 : model1 loss : 0.098974 model2 loss : 0.060422
iteration 9550 : model1 loss : 0.120744 model2 loss : 0.073848
iteration 9551 : model1 loss : 0.112848 model2 loss : 0.087221
iteration 9552 : model1 loss : 0.089604 model2 loss : 0.063265
iteration 9553 : model1 loss : 0.093045 model2 loss : 0.074077
iteration 9554 : model1 loss : 0.131599 model2 loss : 0.122873
 95%|███████████████████████████▋ | 562/589 [3:49:19<10:23, 23.09s/it]iteration 9555 : model1 loss : 0.102849 model2 loss : 0.071446
iteration 9556 : model1 loss : 0.117228 model2 loss : 0.098188
iteration 9557 : model1 loss : 0.151461 model2 loss : 0.108829
iteration 9558 : model1 loss : 0.155353 model2 loss : 0.077990
iteration 9559 : model1 loss : 0.079366 model2 loss : 0.067916
iteration 9560 : model1 loss : 0.101987 model2 loss : 0.081491
iteration 9561 : model1 loss : 0.110211 model2 loss : 0.079012
iteration 9562 : model1 loss : 0.095599 model2 loss : 0.059322
iteration 9563 : model1 loss : 0.102520 model2 loss : 0.069611
iteration 9564 : model1 loss : 0.108630 model2 loss : 0.078684
iteration 9565 : model1 loss : 0.133945 model2 loss : 0.127081
iteration 9566 : model1 loss : 0.147082 model2 loss : 0.109913
iteration 9567 : model1 loss : 0.105527 model2 loss : 0.095265
iteration 9568 : model1 loss : 0.117704 model2 loss : 0.076946
iteration 9569 : model1 loss : 0.098341 model2 loss : 0.079174
iteration 9570 : model1 loss : 0.135653 model2 loss : 0.108588
iteration 9571 : model1 loss : 0.101713 model2 loss : 0.094696
 96%|███████████████████████████▋ | 563/589 [3:49:42<09:57, 22.99s/it]iteration 9572 : model1 loss : 0.134993 model2 loss : 0.092065
iteration 9573 : model1 loss : 0.141980 model2 loss : 0.096840
iteration 9574 : model1 loss : 0.127044 model2 loss : 0.103034
iteration 9575 : model1 loss : 0.117750 model2 loss : 0.078111
iteration 9576 : model1 loss : 0.091919 model2 loss : 0.079106
iteration 9577 : model1 loss : 0.137087 model2 loss : 0.085608
iteration 9578 : model1 loss : 0.100711 model2 loss : 0.079556
iteration 9579 : model1 loss : 0.107097 model2 loss : 0.077831
iteration 9580 : model1 loss : 0.097897 model2 loss : 0.071528
iteration 9581 : model1 loss : 0.176596 model2 loss : 0.079692
iteration 9582 : model1 loss : 0.165176 model2 loss : 0.095137
iteration 9583 : model1 loss : 0.103072 model2 loss : 0.058731
iteration 9584 : model1 loss : 0.134968 model2 loss : 0.102794
iteration 9585 : model1 loss : 0.104264 model2 loss : 0.084084
iteration 9586 : model1 loss : 0.083989 model2 loss : 0.072157
iteration 9587 : model1 loss : 0.082202 model2 loss : 0.068813
iteration 9588 : model1 loss : 0.114004 model2 loss : 0.076032
 96%|███████████████████████████▊ | 564/589 [3:50:05<09:33, 22.92s/it]iteration 9589 : model1 loss : 0.105194 model2 loss : 0.111275
iteration 9590 : model1 loss : 0.107139 model2 loss : 0.075391
iteration 9591 : model1 loss : 0.109358 model2 loss : 0.078550
iteration 9592 : model1 loss : 0.083269 model2 loss : 0.073667
iteration 9593 : model1 loss : 0.113270 model2 loss : 0.100112
iteration 9594 : model1 loss : 0.114096 model2 loss : 0.087213
iteration 9595 : model1 loss : 0.107663 model2 loss : 0.086968
iteration 9596 : model1 loss : 0.148071 model2 loss : 0.079319
iteration 9597 : model1 loss : 0.129813 model2 loss : 0.090123
iteration 9598 : model1 loss : 0.087484 model2 loss : 0.078205
iteration 9599 : model1 loss : 0.139232 model2 loss : 0.138789
iteration 9600 : model1 loss : 0.137555 model2 loss : 0.104024
iteration 9600 : model1_mean_dice : 0.676809 model1_mean_hd95 : 92.426778 model1_mean_iou : 0.551472
iteration 9600 : model2_mean_dice : 0.749716 model2_mean_hd95 : 70.537155 model2_mean_iou : 0.637775
iteration 9601 : model1 loss : 0.128740 model2 loss : 0.103923
iteration 9602 : model1 loss : 0.079064 model2 loss : 0.064140
iteration 9603 : model1 loss : 0.106067 model2 loss : 0.072351
iteration 9604 : model1 loss : 0.128246 model2 loss : 0.113638
iteration 9605 : model1 loss : 0.133906 model2 loss : 0.106843
 96%|███████████████████████████▊ | 565/589 [3:50:47<11:32, 28.87s/it]iteration 9606 : model1 loss : 0.096342 model2 loss : 0.073986
iteration 9607 : model1 loss : 0.100857 model2 loss : 0.091349
iteration 9608 : model1 loss : 0.125475 model2 loss : 0.062529
iteration 9609 : model1 loss : 0.110519 model2 loss : 0.090394
iteration 9610 : model1 loss : 0.112008 model2 loss : 0.090995
iteration 9611 : model1 loss : 0.120556 model2 loss : 0.080274
iteration 9612 : model1 loss : 0.121906 model2 loss : 0.076630
iteration 9613 : model1 loss : 0.115014 model2 loss : 0.097343
iteration 9614 : model1 loss : 0.112234 model2 loss : 0.083765
iteration 9615 : model1 loss : 0.094145 model2 loss : 0.070152
iteration 9616 : model1 loss : 0.181304 model2 loss : 0.102997
iteration 9617 : model1 loss : 0.112533 model2 loss : 0.078614
iteration 9618 : model1 loss : 0.108719 model2 loss : 0.096916
iteration 9619 : model1 loss : 0.164670 model2 loss : 0.095783
iteration 9620 : model1 loss : 0.100078 model2 loss : 0.098732
iteration 9621 : model1 loss : 0.102355 model2 loss : 0.090660
iteration 9622 : model1 loss : 0.100211 model2 loss : 0.063446
 96%|███████████████████████████▊ | 566/589 [3:51:10<10:21, 27.01s/it]iteration 9623 : model1 loss : 0.074959 model2 loss : 0.068798
iteration 9624 : model1 loss : 0.107153 model2 loss : 0.085859
iteration 9625 : model1 loss : 0.102129 model2 loss : 0.065496
iteration 9626 : model1 loss : 0.091069 model2 loss : 0.060027
iteration 9627 : model1 loss : 0.132482 model2 loss : 0.113635
iteration 9628 : model1 loss : 0.101768 model2 loss : 0.078067
iteration 9629 : model1 loss : 0.126810 model2 loss : 0.079888
iteration 9630 : model1 loss : 0.100696 model2 loss : 0.063454
iteration 9631 : model1 loss : 0.141663 model2 loss : 0.119601
iteration 9632 : model1 loss : 0.110456 model2 loss : 0.125059
iteration 9633 : model1 loss : 0.153459 model2 loss : 0.080478
iteration 9634 : model1 loss : 0.140374 model2 loss : 0.101764
iteration 9635 : model1 loss : 0.127748 model2 loss : 0.094314
iteration 9636 : model1 loss : 0.135728 model2 loss : 0.094979
iteration 9637 : model1 loss : 0.108348 model2 loss : 0.087160
iteration 9638 : model1 loss : 0.107690 model2 loss : 0.087783
iteration 9639 : model1 loss : 0.121048 model2 loss : 0.061536
 96%|███████████████████████████▉ | 567/589 [3:51:33<09:26, 25.74s/it]iteration 9640 : model1 loss : 0.121193 model2 loss : 0.107256
iteration 9641 : model1 loss : 0.104505 model2 loss : 0.091395
iteration 9642 : model1 loss : 0.147680 model2 loss : 0.084651
iteration 9643 : model1 loss : 0.112242 model2 loss : 0.082638
iteration 9644 : model1 loss : 0.118829 model2 loss : 0.092970
iteration 9645 : model1 loss : 0.112651 model2 loss : 0.079595
iteration 9646 : model1 loss : 0.133719 model2 loss : 0.086568
iteration 9647 : model1 loss : 0.116768 model2 loss : 0.076417
iteration 9648 : model1 loss : 0.152639 model2 loss : 0.084023
iteration 9649 : model1 loss : 0.117232 model2 loss : 0.100709
iteration 9650 : model1 loss : 0.105064 model2 loss : 0.087288
iteration 9651 : model1 loss : 0.144768 model2 loss : 0.110356
iteration 9652 : model1 loss : 0.104581 model2 loss : 0.070687
iteration 9653 : model1 loss : 0.123472 model2 loss : 0.070078
iteration 9654 : model1 loss : 0.114319 model2 loss : 0.064985
iteration 9655 : model1 loss : 0.100273 model2 loss : 0.090294
iteration 9656 : model1 loss : 0.100986 model2 loss : 0.087728
 96%|███████████████████████████▉ | 568/589 [3:51:56<08:42, 24.88s/it]iteration 9657 : model1 loss : 0.165749 model2 loss : 0.102661
iteration 9658 : model1 loss : 0.114247 model2 loss : 0.067482
iteration 9659 : model1 loss : 0.087955 model2 loss : 0.059723
iteration 9660 : model1 loss : 0.118095 model2 loss : 0.081976
iteration 9661 : model1 loss : 0.119616 model2 loss : 0.095412
iteration 9662 : model1 loss : 0.135256 model2 loss : 0.103712
iteration 9663 : model1 loss : 0.233614 model2 loss : 0.102384
iteration 9664 : model1 loss : 0.102308 model2 loss : 0.103706
iteration 9665 : model1 loss : 0.111798 model2 loss : 0.078016
iteration 9666 : model1 loss : 0.120953 model2 loss : 0.080639
iteration 9667 : model1 loss : 0.150544 model2 loss : 0.077288
iteration 9668 : model1 loss : 0.102501 model2 loss : 0.088746
iteration 9669 : model1 loss : 0.116403 model2 loss : 0.092392
iteration 9670 : model1 loss : 0.132565 model2 loss : 0.097230
iteration 9671 : model1 loss : 0.121450 model2 loss : 0.082680
iteration 9672 : model1 loss : 0.080177 model2 loss : 0.071904
iteration 9673 : model1 loss : 0.094268 model2 loss : 0.074258
 97%|████████████████████████████ | 569/589 [3:52:18<08:04, 24.25s/it]iteration 9674 : model1 loss : 0.123964 model2 loss : 0.118097
iteration 9675 : model1 loss : 0.138389 model2 loss : 0.094041
iteration 9676 : model1 loss : 0.141157 model2 loss : 0.081452
iteration 9677 : model1 loss : 0.098766 model2 loss : 0.058173
iteration 9678 : model1 loss : 0.077657 model2 loss : 0.054931
iteration 9679 : model1 loss : 0.097723 model2 loss : 0.090520
iteration 9680 : model1 loss : 0.149669 model2 loss : 0.121579
iteration 9681 : model1 loss : 0.101569 model2 loss : 0.109492
iteration 9682 : model1 loss : 0.104073 model2 loss : 0.098351
iteration 9683 : model1 loss : 0.155777 model2 loss : 0.107762
iteration 9684 : model1 loss : 0.140339 model2 loss : 0.120904
iteration 9685 : model1 loss : 0.115253 model2 loss : 0.084873
iteration 9686 : model1 loss : 0.078914 model2 loss : 0.051775
iteration 9687 : model1 loss : 0.118878 model2 loss : 0.072502
iteration 9688 : model1 loss : 0.096969 model2 loss : 0.078282
iteration 9689 : model1 loss : 0.121414 model2 loss : 0.099602
iteration 9690 : model1 loss : 0.090923 model2 loss : 0.072153
 97%|████████████████████████████ | 570/589 [3:52:41<07:32, 23.81s/it]iteration 9691 : model1 loss : 0.126878 model2 loss : 0.078612
iteration 9692 : model1 loss : 0.153129 model2 loss : 0.104197
iteration 9693 : model1 loss : 0.139210 model2 loss : 0.091937
iteration 9694 : model1 loss : 0.098298 model2 loss : 0.061269
iteration 9695 : model1 loss : 0.099694 model2 loss : 0.078398
iteration 9696 : model1 loss : 0.145890 model2 loss : 0.102788
iteration 9697 : model1 loss : 0.107039 model2 loss : 0.084891
iteration 9698 : model1 loss : 0.155766 model2 loss : 0.098093
iteration 9699 : model1 loss : 0.088625 model2 loss : 0.093296
iteration 9700 : model1 loss : 0.102981 model2 loss : 0.126867
iteration 9701 : model1 loss : 0.159857 model2 loss : 0.087815
iteration 9702 : model1 loss : 0.121623 model2 loss : 0.100703
iteration 9703 : model1 loss : 0.083403 model2 loss : 0.062492
iteration 9704 : model1 loss : 0.099219 model2 loss : 0.084466
iteration 9705 : model1 loss : 0.099516 model2 loss : 0.078128
iteration 9706 : model1 loss : 0.096846 model2 loss : 0.061700
iteration 9707 : model1 loss : 0.145084 model2 loss : 0.087918
 97%|████████████████████████████ | 571/589 [3:53:04<07:03, 23.54s/it]iteration 9708 : model1 loss : 0.119811 model2 loss : 0.077052
iteration 9709 : model1 loss : 0.119399 model2 loss : 0.077124
iteration 9710 : model1 loss : 0.119420 model2 loss : 0.078681
iteration 9711 : model1 loss : 0.152722 model2 loss : 0.096938
iteration 9712 : model1 loss : 0.103676 model2 loss : 0.089176
iteration 9713 : model1 loss : 0.105268 model2 loss : 0.079790
iteration 9714 : model1 loss : 0.118633 model2 loss : 0.075544
iteration 9715 : model1 loss : 0.108604 model2 loss : 0.072969
iteration 9716 : model1 loss : 0.114143 model2 loss : 0.089616
iteration 9717 : model1 loss : 0.088974 model2 loss : 0.059660
iteration 9718 : model1 loss : 0.069560 model2 loss : 0.068920
iteration 9719 : model1 loss : 0.186548 model2 loss : 0.094531
iteration 9720 : model1 loss : 0.112173 model2 loss : 0.101633
iteration 9721 : model1 loss : 0.107987 model2 loss : 0.090773
iteration 9722 : model1 loss : 0.136473 model2 loss : 0.105003
iteration 9723 : model1 loss : 0.117838 model2 loss : 0.132444
iteration 9724 : model1 loss : 0.137293 model2 loss : 0.090958
 97%|████████████████████████████▏| 572/589 [3:53:27<06:36, 23.31s/it]iteration 9725 : model1 loss : 0.134151 model2 loss : 0.082864
iteration 9726 : model1 loss : 0.134776 model2 loss : 0.080503
iteration 9727 : model1 loss : 0.164028 model2 loss : 0.088846
iteration 9728 : model1 loss : 0.094299 model2 loss : 0.067820
iteration 9729 : model1 loss : 0.105686 model2 loss : 0.096322
iteration 9730 : model1 loss : 0.072578 model2 loss : 0.063246
iteration 9731 : model1 loss : 0.136258 model2 loss : 0.072201
iteration 9732 : model1 loss : 0.086861 model2 loss : 0.081516
iteration 9733 : model1 loss : 0.111535 model2 loss : 0.081578
iteration 9734 : model1 loss : 0.102407 model2 loss : 0.084415
iteration 9735 : model1 loss : 0.137320 model2 loss : 0.075973
iteration 9736 : model1 loss : 0.117986 model2 loss : 0.103806
iteration 9737 : model1 loss : 0.131442 model2 loss : 0.073751
iteration 9738 : model1 loss : 0.113059 model2 loss : 0.095542
iteration 9739 : model1 loss : 0.115743 model2 loss : 0.081276
iteration 9740 : model1 loss : 0.107504 model2 loss : 0.081490
iteration 9741 : model1 loss : 0.112731 model2 loss : 0.076166
 97%|████████████████████████████▏| 573/589 [3:53:50<06:10, 23.14s/it]iteration 9742 : model1 loss : 0.124947 model2 loss : 0.100905
iteration 9743 : model1 loss : 0.107362 model2 loss : 0.088743
iteration 9744 : model1 loss : 0.135503 model2 loss : 0.097050
iteration 9745 : model1 loss : 0.105893 model2 loss : 0.076936
iteration 9746 : model1 loss : 0.094568 model2 loss : 0.085446
iteration 9747 : model1 loss : 0.141858 model2 loss : 0.086071
iteration 9748 : model1 loss : 0.113442 model2 loss : 0.083697
iteration 9749 : model1 loss : 0.099522 model2 loss : 0.082170
iteration 9750 : model1 loss : 0.116940 model2 loss : 0.074430
iteration 9751 : model1 loss : 0.115954 model2 loss : 0.083501
iteration 9752 : model1 loss : 0.127999 model2 loss : 0.101492
iteration 9753 : model1 loss : 0.102663 model2 loss : 0.073964
iteration 9754 : model1 loss : 0.139600 model2 loss : 0.112511
iteration 9755 : model1 loss : 0.112426 model2 loss : 0.087614
iteration 9756 : model1 loss : 0.095348 model2 loss : 0.077359
iteration 9757 : model1 loss : 0.101032 model2 loss : 0.104517
iteration 9758 : model1 loss : 0.119375 model2 loss : 0.079752
 97%|████████████████████████████▎| 574/589 [3:54:13<05:46, 23.07s/it]iteration 9759 : model1 loss : 0.111416 model2 loss : 0.065811
iteration 9760 : model1 loss : 0.104007 model2 loss : 0.075916
iteration 9761 : model1 loss : 0.107166 model2 loss : 0.072287
iteration 9762 : model1 loss : 0.079237 model2 loss : 0.068412
iteration 9763 : model1 loss : 0.128766 model2 loss : 0.102603
iteration 9764 : model1 loss : 0.120472 model2 loss : 0.083093
iteration 9765 : model1 loss : 0.130777 model2 loss : 0.108544
iteration 9766 : model1 loss : 0.109006 model2 loss : 0.100583
iteration 9767 : model1 loss : 0.113049 model2 loss : 0.118298
iteration 9768 : model1 loss : 0.134618 model2 loss : 0.078467
iteration 9769 : model1 loss : 0.134515 model2 loss : 0.077036
iteration 9770 : model1 loss : 0.097971 model2 loss : 0.074542
iteration 9771 : model1 loss : 0.107228 model2 loss : 0.081374
iteration 9772 : model1 loss : 0.143817 model2 loss : 0.093083
iteration 9773 : model1 loss : 0.135622 model2 loss : 0.076325
iteration 9774 : model1 loss : 0.095446 model2 loss : 0.134421
iteration 9775 : model1 loss : 0.096017 model2 loss : 0.068721
 98%|████████████████████████████▎| 575/589 [3:54:35<05:21, 22.97s/it]iteration 9776 : model1 loss : 0.109819 model2 loss : 0.069649
iteration 9777 : model1 loss : 0.125293 model2 loss : 0.087002
iteration 9778 : model1 loss : 0.088058 model2 loss : 0.057044
iteration 9779 : model1 loss : 0.098622 model2 loss : 0.071716
iteration 9780 : model1 loss : 0.092001 model2 loss : 0.078845
iteration 9781 : model1 loss : 0.104857 model2 loss : 0.080270
iteration 9782 : model1 loss : 0.148804 model2 loss : 0.110585
iteration 9783 : model1 loss : 0.127611 model2 loss : 0.094532
iteration 9784 : model1 loss : 0.092540 model2 loss : 0.087780
iteration 9785 : model1 loss : 0.140148 model2 loss : 0.074954
iteration 9786 : model1 loss : 0.155086 model2 loss : 0.115908
iteration 9787 : model1 loss : 0.117568 model2 loss : 0.065024
iteration 9788 : model1 loss : 0.110500 model2 loss : 0.077491
iteration 9789 : model1 loss : 0.122504 model2 loss : 0.104340
iteration 9790 : model1 loss : 0.095601 model2 loss : 0.093393
iteration 9791 : model1 loss : 0.118010 model2 loss : 0.064565
iteration 9792 : model1 loss : 0.109705 model2 loss : 0.072387
 98%|████████████████████████████▎| 576/589 [3:54:58<04:57, 22.91s/it]iteration 9793 : model1 loss : 0.099316 model2 loss : 0.077398
iteration 9794 : model1 loss : 0.115236 model2 loss : 0.091513
iteration 9795 : model1 loss : 0.101034 model2 loss : 0.082375
iteration 9796 : model1 loss : 0.143076 model2 loss : 0.092469
iteration 9797 : model1 loss : 0.087387 model2 loss : 0.087862
iteration 9798 : model1 loss : 0.123834 model2 loss : 0.068150
iteration 9799 : model1 loss : 0.120976 model2 loss : 0.085198
iteration 9800 : model1 loss : 0.104218 model2 loss : 0.080443
iteration 9800 : model1_mean_dice : 0.667914 model1_mean_hd95 : 91.326692 model1_mean_iou : 0.541434
iteration 9800 : model2_mean_dice : 0.751545 model2_mean_hd95 : 69.132031 model2_mean_iou : 0.639832
iteration 9801 : model1 loss : 0.078099 model2 loss : 0.059692
iteration 9802 : model1 loss : 0.142172 model2 loss : 0.089981
iteration 9803 : model1 loss : 0.102860 model2 loss : 0.134525
iteration 9804 : model1 loss : 0.113516 model2 loss : 0.059625
iteration 9805 : model1 loss : 0.108711 model2 loss : 0.086220
iteration 9806 : model1 loss : 0.120613 model2 loss : 0.087758
iteration 9807 : model1 loss : 0.127400 model2 loss : 0.096390
iteration 9808 : model1 loss : 0.129314 model2 loss : 0.099950
iteration 9809 : model1 loss : 0.124888 model2 loss : 0.107733
 98%|████████████████████████████▍| 577/589 [3:55:41<05:46, 28.84s/it]iteration 9810 : model1 loss : 0.140738 model2 loss : 0.090157
iteration 9811 : model1 loss : 0.176490 model2 loss : 0.098062
iteration 9812 : model1 loss : 0.131715 model2 loss : 0.101219
iteration 9813 : model1 loss : 0.166058 model2 loss : 0.084308
iteration 9814 : model1 loss : 0.098297 model2 loss : 0.111999
iteration 9815 : model1 loss : 0.131333 model2 loss : 0.102804
iteration 9816 : model1 loss : 0.110604 model2 loss : 0.089660
iteration 9817 : model1 loss : 0.101805 model2 loss : 0.074674
iteration 9818 : model1 loss : 0.094726 model2 loss : 0.084106
iteration 9819 : model1 loss : 0.088290 model2 loss : 0.079471
iteration 9820 : model1 loss : 0.138756 model2 loss : 0.089716
iteration 9821 : model1 loss : 0.110891 model2 loss : 0.083186
iteration 9822 : model1 loss : 0.101784 model2 loss : 0.088456
iteration 9823 : model1 loss : 0.099973 model2 loss : 0.068433
iteration 9824 : model1 loss : 0.142467 model2 loss : 0.085824
iteration 9825 : model1 loss : 0.115185 model2 loss : 0.084810
iteration 9826 : model1 loss : 0.088977 model2 loss : 0.083553
 98%|████████████████████████████▍| 578/589 [3:56:04<04:57, 27.01s/it]iteration 9827 : model1 loss : 0.098101 model2 loss : 0.064032
iteration 9828 : model1 loss : 0.149236 model2 loss : 0.124988
iteration 9829 : model1 loss : 0.100114 model2 loss : 0.097555
iteration 9830 : model1 loss : 0.135596 model2 loss : 0.089093
iteration 9831 : model1 loss : 0.095065 model2 loss : 0.075965
iteration 9832 : model1 loss : 0.107341 model2 loss : 0.067650
iteration 9833 : model1 loss : 0.140015 model2 loss : 0.122401
iteration 9834 : model1 loss : 0.114890 model2 loss : 0.112867
iteration 9835 : model1 loss : 0.094564 model2 loss : 0.088622
iteration 9836 : model1 loss : 0.119481 model2 loss : 0.062910
iteration 9837 : model1 loss : 0.132834 model2 loss : 0.076642
iteration 9838 : model1 loss : 0.108416 model2 loss : 0.091575
iteration 9839 : model1 loss : 0.120283 model2 loss : 0.077676
iteration 9840 : model1 loss : 0.100310 model2 loss : 0.080686
iteration 9841 : model1 loss : 0.105813 model2 loss : 0.060233
iteration 9842 : model1 loss : 0.201490 model2 loss : 0.082233
iteration 9843 : model1 loss : 0.142271 model2 loss : 0.066338
 98%|████████████████████████████▌| 579/589 [3:56:26<04:17, 25.72s/it]iteration 9844 : model1 loss : 0.122618 model2 loss : 0.063359
iteration 9845 : model1 loss : 0.125460 model2 loss : 0.082558
iteration 9846 : model1 loss : 0.130944 model2 loss : 0.088448
iteration 9847 : model1 loss : 0.141925 model2 loss : 0.071720
iteration 9848 : model1 loss : 0.106138 model2 loss : 0.077452
iteration 9849 : model1 loss : 0.122858 model2 loss : 0.082405
iteration 9850 : model1 loss : 0.135911 model2 loss : 0.090621
iteration 9851 : model1 loss : 0.126236 model2 loss : 0.097859
iteration 9852 : model1 loss : 0.113701 model2 loss : 0.070413
iteration 9853 : model1 loss : 0.109466 model2 loss : 0.083377
iteration 9854 : model1 loss : 0.126809 model2 loss : 0.081269
iteration 9855 : model1 loss : 0.100351 model2 loss : 0.089249
iteration 9856 : model1 loss : 0.115951 model2 loss : 0.085950
iteration 9857 : model1 loss : 0.101224 model2 loss : 0.068510
iteration 9858 : model1 loss : 0.082284 model2 loss : 0.072016
iteration 9859 : model1 loss : 0.127001 model2 loss : 0.112812
iteration 9860 : model1 loss : 0.145411 model2 loss : 0.083377
 98%|████████████████████████████▌| 580/589 [3:56:49<03:43, 24.87s/it]iteration 9861 : model1 loss : 0.140536 model2 loss : 0.080857
iteration 9862 : model1 loss : 0.149951 model2 loss : 0.080602
iteration 9863 : model1 loss : 0.130728 model2 loss : 0.072219
iteration 9864 : model1 loss : 0.137892 model2 loss : 0.074237
iteration 9865 : model1 loss : 0.104513 model2 loss : 0.095808
iteration 9866 : model1 loss : 0.130775 model2 loss : 0.092505
iteration 9867 : model1 loss : 0.156558 model2 loss : 0.103628
iteration 9868 : model1 loss : 0.099632 model2 loss : 0.080358
iteration 9869 : model1 loss : 0.077282 model2 loss : 0.068566
iteration 9870 : model1 loss : 0.113959 model2 loss : 0.070655
iteration 9871 : model1 loss : 0.148594 model2 loss : 0.098780
iteration 9872 : model1 loss : 0.101953 model2 loss : 0.072702
iteration 9873 : model1 loss : 0.098518 model2 loss : 0.068036
iteration 9874 : model1 loss : 0.109420 model2 loss : 0.064184
iteration 9875 : model1 loss : 0.128346 model2 loss : 0.100866
iteration 9876 : model1 loss : 0.101053 model2 loss : 0.081959
iteration 9877 : model1 loss : 0.082234 model2 loss : 0.052224
 99%|████████████████████████████▌| 581/589 [3:57:12<03:13, 24.22s/it]iteration 9878 : model1 loss : 0.141484 model2 loss : 0.082659
iteration 9879 : model1 loss : 0.134029 model2 loss : 0.084813
iteration 9880 : model1 loss : 0.116954 model2 loss : 0.094155
iteration 9881 : model1 loss : 0.121017 model2 loss : 0.073164
iteration 9882 : model1 loss : 0.134015 model2 loss : 0.128397
iteration 9883 : model1 loss : 0.130487 model2 loss : 0.092786
iteration 9884 : model1 loss : 0.133356 model2 loss : 0.074726
iteration 9885 : model1 loss : 0.127039 model2 loss : 0.078311
iteration 9886 : model1 loss : 0.100556 model2 loss : 0.086030
iteration 9887 : model1 loss : 0.080936 model2 loss : 0.057102
iteration 9888 : model1 loss : 0.131576 model2 loss : 0.068744
iteration 9889 : model1 loss : 0.078356 model2 loss : 0.069698
iteration 9890 : model1 loss : 0.112561 model2 loss : 0.072010
iteration 9891 : model1 loss : 0.138951 model2 loss : 0.073541
iteration 9892 : model1 loss : 0.101409 model2 loss : 0.068170
iteration 9893 : model1 loss : 0.092041 model2 loss : 0.062973
iteration 9894 : model1 loss : 0.116189 model2 loss : 0.096422
 99%|████████████████████████████▋| 582/589 [3:57:35<02:46, 23.79s/it]iteration 9895 : model1 loss : 0.129527 model2 loss : 0.060773
iteration 9896 : model1 loss : 0.116894 model2 loss : 0.088466
iteration 9897 : model1 loss : 0.163537 model2 loss : 0.094219
iteration 9898 : model1 loss : 0.134273 model2 loss : 0.106890
iteration 9899 : model1 loss : 0.119210 model2 loss : 0.097134
iteration 9900 : model1 loss : 0.117583 model2 loss : 0.066078
iteration 9901 : model1 loss : 0.130924 model2 loss : 0.090817
iteration 9902 : model1 loss : 0.129087 model2 loss : 0.089961
iteration 9903 : model1 loss : 0.091340 model2 loss : 0.090058
iteration 9904 : model1 loss : 0.085835 model2 loss : 0.060533
iteration 9905 : model1 loss : 0.139660 model2 loss : 0.104303
iteration 9906 : model1 loss : 0.141773 model2 loss : 0.082477
iteration 9907 : model1 loss : 0.080974 model2 loss : 0.073093
iteration 9908 : model1 loss : 0.121052 model2 loss : 0.081890
iteration 9909 : model1 loss : 0.101545 model2 loss : 0.082791
iteration 9910 : model1 loss : 0.091164 model2 loss : 0.075401
iteration 9911 : model1 loss : 0.120544 model2 loss : 0.096962
 99%|████████████████████████████▋| 583/589 [3:57:58<02:21, 23.53s/it]iteration 9912 : model1 loss : 0.109154 model2 loss : 0.085188
iteration 9913 : model1 loss : 0.170063 model2 loss : 0.095596
iteration 9914 : model1 loss : 0.134548 model2 loss : 0.073694
iteration 9915 : model1 loss : 0.104760 model2 loss : 0.058107
iteration 9916 : model1 loss : 0.122320 model2 loss : 0.106577
iteration 9917 : model1 loss : 0.092446 model2 loss : 0.086717
iteration 9918 : model1 loss : 0.087484 model2 loss : 0.050953
iteration 9919 : model1 loss : 0.144117 model2 loss : 0.088044
iteration 9920 : model1 loss : 0.115814 model2 loss : 0.075660
iteration 9921 : model1 loss : 0.120562 model2 loss : 0.093917
iteration 9922 : model1 loss : 0.188236 model2 loss : 0.088868
iteration 9923 : model1 loss : 0.104393 model2 loss : 0.100428
iteration 9924 : model1 loss : 0.102687 model2 loss : 0.085753
iteration 9925 : model1 loss : 0.133045 model2 loss : 0.092036
iteration 9926 : model1 loss : 0.077413 model2 loss : 0.067021
iteration 9927 : model1 loss : 0.088639 model2 loss : 0.085422
iteration 9928 : model1 loss : 0.094527 model2 loss : 0.075236
 99%|████████████████████████████▊| 584/589 [3:58:20<01:56, 23.32s/it]iteration 9929 : model1 loss : 0.102101 model2 loss : 0.070989
iteration 9930 : model1 loss : 0.103015 model2 loss : 0.059894
iteration 9931 : model1 loss : 0.134108 model2 loss : 0.104417
iteration 9932 : model1 loss : 0.150190 model2 loss : 0.077040
iteration 9933 : model1 loss : 0.084699 model2 loss : 0.073177
iteration 9934 : model1 loss : 0.136016 model2 loss : 0.080135
iteration 9935 : model1 loss : 0.125022 model2 loss : 0.074502
iteration 9936 : model1 loss : 0.132785 model2 loss : 0.099452
iteration 9937 : model1 loss : 0.113209 model2 loss : 0.058559
iteration 9938 : model1 loss : 0.106170 model2 loss : 0.118608
iteration 9939 : model1 loss : 0.129837 model2 loss : 0.131080
iteration 9940 : model1 loss : 0.099271 model2 loss : 0.070009
iteration 9941 : model1 loss : 0.131984 model2 loss : 0.086021
iteration 9942 : model1 loss : 0.147717 model2 loss : 0.112167
iteration 9943 : model1 loss : 0.123499 model2 loss : 0.100287
iteration 9944 : model1 loss : 0.084860 model2 loss : 0.067921
iteration 9945 : model1 loss : 0.100377 model2 loss : 0.091152
 99%|████████████████████████████▊| 585/589 [3:58:43<01:32, 23.17s/it]iteration 9946 : model1 loss : 0.124509 model2 loss : 0.072922
iteration 9947 : model1 loss : 0.138788 model2 loss : 0.094346
iteration 9948 : model1 loss : 0.122267 model2 loss : 0.084687
iteration 9949 : model1 loss : 0.161060 model2 loss : 0.093413
iteration 9950 : model1 loss : 0.115066 model2 loss : 0.097554
iteration 9951 : model1 loss : 0.116399 model2 loss : 0.081125
iteration 9952 : model1 loss : 0.090880 model2 loss : 0.074475
iteration 9953 : model1 loss : 0.106266 model2 loss : 0.086035
iteration 9954 : model1 loss : 0.094273 model2 loss : 0.068901
iteration 9955 : model1 loss : 0.125042 model2 loss : 0.098554
iteration 9956 : model1 loss : 0.098559 model2 loss : 0.072770
iteration 9957 : model1 loss : 0.096798 model2 loss : 0.073711
iteration 9958 : model1 loss : 0.123401 model2 loss : 0.090186
iteration 9959 : model1 loss : 0.124037 model2 loss : 0.069729
iteration 9960 : model1 loss : 0.104471 model2 loss : 0.093132
iteration 9961 : model1 loss : 0.100319 model2 loss : 0.078980
iteration 9962 : model1 loss : 0.108140 model2 loss : 0.071077
 99%|████████████████████████████▊| 586/589 [3:59:06<01:09, 23.09s/it]iteration 9963 : model1 loss : 0.119317 model2 loss : 0.078167
iteration 9964 : model1 loss : 0.107708 model2 loss : 0.067685
iteration 9965 : model1 loss : 0.103709 model2 loss : 0.053612
iteration 9966 : model1 loss : 0.109909 model2 loss : 0.069586
iteration 9967 : model1 loss : 0.110326 model2 loss : 0.101657
iteration 9968 : model1 loss : 0.153968 model2 loss : 0.103565
iteration 9969 : model1 loss : 0.136717 model2 loss : 0.083109
iteration 9970 : model1 loss : 0.105238 model2 loss : 0.072209
iteration 9971 : model1 loss : 0.114846 model2 loss : 0.110090
iteration 9972 : model1 loss : 0.113141 model2 loss : 0.091701
iteration 9973 : model1 loss : 0.154119 model2 loss : 0.085812
iteration 9974 : model1 loss : 0.100279 model2 loss : 0.075377
iteration 9975 : model1 loss : 0.105365 model2 loss : 0.085231
iteration 9976 : model1 loss : 0.109549 model2 loss : 0.072741
iteration 9977 : model1 loss : 0.125587 model2 loss : 0.084422
iteration 9978 : model1 loss : 0.093783 model2 loss : 0.081995
iteration 9979 : model1 loss : 0.113934 model2 loss : 0.086661
100%|████████████████████████████▉| 587/589 [3:59:29<00:46, 23.01s/it]iteration 9980 : model1 loss : 0.105702 model2 loss : 0.078657
iteration 9981 : model1 loss : 0.122355 model2 loss : 0.096625
iteration 9982 : model1 loss : 0.089393 model2 loss : 0.073517
slurmstepd: error: *** JOB 25995652 ON gr028 CANCELLED AT 2022-10-18T02:21:26 DUE TO TIME LIMIT ***
